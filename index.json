[{"categories":null,"content":"介绍 hugo 是由go实现，一个快速建立静态网站的工具。详细教程可参考hugo中文文档 正式官网 ","date":"2024-01-30","objectID":"/2024/01/hugo/hugo/:1:0","tags":["hugo"],"title":"Hugo","uri":"/2024/01/hugo/hugo/"},{"categories":null,"content":"安装 直接下载对应操作系统的二进制 Hugo Releases ，放到$GOPATH/bin/ 源码安装 cd $GOPATH/src/github.com git clone git@github.com:gohugoio/hugo.git go install ","date":"2024-01-30","objectID":"/2024/01/hugo/hugo/:2:0","tags":["hugo"],"title":"Hugo","uri":"/2024/01/hugo/hugo/"},{"categories":null,"content":"hugo命令 # 生成站点 hugo new site ~/blog/Joker_null # 在根目录(Joker_null)下执行，生成Markdown文件 hugo new posts/hugo_guide.md # 在根目录(Joker_null)下执行,运行本地服务用于预览,-D参数是指草稿文件也加入预览文件中，localhost:1313 hugo server -D # 打包生成静态网页，用于部署 hugo -d ./release #详细命令参考 hugo help #启用正式环境变量 hugo server --environment production ","date":"2024-01-30","objectID":"/2024/01/hugo/hugo/:3:0","tags":["hugo"],"title":"Hugo","uri":"/2024/01/hugo/hugo/"},{"categories":null,"content":"docker compose version: '3.8' services: cache: image: redis restart: always ports: - '6379:6379' command: redis-server --save 20 1 --loglevel warning --requirepass facai888 volumes: - cache:/data volumes: cache: driver: local ","date":"2024-01-30","objectID":"/2024/01/redis/install/:0:1","tags":["redis"],"title":"redis install","uri":"/2024/01/redis/install/"},{"categories":null,"content":"docker compose services: mysql: image: mysql container_name: mysql hostname: mysqldb ports: - 3306:3306 restart: always volumes: - mysqldata:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=12345678 - TZ=Asia/Shanghai networks: - \"mysqlnet\" volumes: mysqldata: networks: mysqlnet: docker run --name mysql -v ~/docker/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=12345678 -d -p 3307:3306 --restart=always mysql ","date":"2024-01-30","objectID":"/2024/01/mysql/install/:1:0","tags":["mysql"],"title":"mysql install","uri":"/2024/01/mysql/install/"},{"categories":null,"content":"2024年似乎要做一些改变。 改变对人的态度，不能太热情，如果没有必要请冷淡处理 改变作息时间，十二点钟之前必须上床睡觉，就是天王老子来了也不行 深入学习java，golang。c# 虽说优雅，但精力有限 做一款网络游戏 谈一场恋爱，但不要认真 忘掉过去，禁止看已经看过的电影，禁止重复性做某件事情，要让自己明白过去的事情已经过去 ","date":"2024-01-24","objectID":"/2024/01/1/1/:0:1","tags":["杂谈"],"title":"1","uri":"/2024/01/1/1/"},{"categories":null,"content":"复制粘贴 yy 复制一行 p 粘贴 ","date":"2024-01-22","objectID":"/2024/01/vim/vim/:0:1","tags":["vim"],"title":"Vim","uri":"/2024/01/vim/vim/"},{"categories":null,"content":"跳转定义 在 VsCode 的 Vim 插件中，要跳转到定义，你可以在一般对应变量或函数的位置上按下 “gd”（Go to definition）。 ","date":"2024-01-22","objectID":"/2024/01/vim/vim/:0:2","tags":["vim"],"title":"Vim","uri":"/2024/01/vim/vim/"},{"categories":null,"content":"返回上一个光标的位置 ctrl+o ","date":"2024-01-22","objectID":"/2024/01/vim/vim/:0:3","tags":["vim"],"title":"Vim","uri":"/2024/01/vim/vim/"},{"categories":[""],"content":" 相互吸引的本质是提供对方认为有用的价值， ","date":"2022-12-30","objectID":"/2022/12/cs-target/cs-tartget/:0:0","tags":null,"title":"Cs Tartget","uri":"/2022/12/cs-target/cs-tartget/"},{"categories":null,"content":"需求 当本地docker 容器需要迁移，数据卷（volumes）的备份是必要的 原始备份 脚本 docker-vackup #!/bin/bash # Docker Volume File Backup and Restore Tool # Easily tar up a volume on a local (or remote) engine # Inspired by CLIP from Lukasz Lach set -Eeo pipefail handle_error() { exit_code=$? if [ -n \"${VACKUP_FAILURE_SCRIPT}\" ]; then /bin/bash \"${VACKUP_FAILURE_SCRIPT}\" \"$1\" $exit_code fi exit $exit_code } trap 'handle_error $LINENO' ERR usage() { cat \u003c\u003cEOF \"Docker Volume Backup\". Replicates image management commands for volumes. export/import copies files between a host tarball and a volume. For making volume backups and restores. save/load copies files between an image and a volume. For when you want to use image registries as a way to push/pull volume data. Usage: vackup export VOLUME FILE Creates a gzip'ed tarball in current directory from a volume vackup import FILE VOLUME Extracts a gzip'ed tarball into a volume vackup save VOLUME IMAGE Copies the volume contents to a busybox image in the /volume-data directory vackup load IMAGE VOLUME Copies /volume-data contents from an image to a volume EOF } if [ -z \"$1\" ] || [ \"$1\" == \"-h\" ] || [ \"$1\" == \"--help\" ]; then usage exit 0 fi cmd_export() { VOLUME_NAME=\"$2\" FILE_NAME=\"$3\" if [ -z \"$VOLUME_NAME\" ] || [ -z \"$FILE_NAME\" ]; then echo \"Error: Not enough arguments\" usage exit 1 fi if ! docker volume inspect --format '{{.Name}}' \"$VOLUME_NAME\"; then echo \"Error: Volume $VOLUME_NAME does not exist\" exit 1 fi # TODO: check if file exists on host, if it does # create a option for overwrite and check if that's set # TODO: if FILE_NAME starts with / we need to error out # unless we can translate full file paths if ! docker run --rm \\ -v \"$VOLUME_NAME\":/vackup-volume \\ -v \"$(pwd)\":/vackup \\ busybox \\ tar -zcvf /vackup/\"$FILE_NAME\" /vackup-volume; then echo \"Error: Failed to start busybox backup container\" exit 1 fi echo \"Successfully tar'ed volume $VOLUME_NAME into file $FILE_NAME\" } cmd_import() { FILE_NAME=\"$2\" VOLUME_NAME=\"$3\" if [ -z \"$VOLUME_NAME\" ] || [ -z \"$FILE_NAME\" ]; then echo \"Error: Not enough arguments\" usage exit 1 fi if ! docker volume inspect --format '{{.Name}}' \"$VOLUME_NAME\"; then echo \"Error: Volume $VOLUME_NAME does not exist\" docker volume create \"$VOLUME_NAME\" fi # TODO: check if file exists on host, if it does # create a option for overwrite and check if that's set # TODO: if FILE_NAME starts with / we need to error out # unless we can translate full file paths if ! docker run --rm \\ -v \"$VOLUME_NAME\":/vackup-volume \\ -v \"$(pwd)\":/vackup \\ busybox \\ tar -xvzf /vackup/\"$FILE_NAME\" -C /; then echo \"Error: Failed to start busybox container\" exit 1 fi echo \"Successfully unpacked $FILE_NAME into volume $VOLUME_NAME\" } cmd_save() { VOLUME_NAME=\"$2\" IMAGE_NAME=\"$3\" if [ -z \"$VOLUME_NAME\" ] || [ -z \"$IMAGE_NAME\" ]; then echo \"Error: Not enough arguments\" usage exit 1 fi if ! docker volume inspect --format '{{.Name}}' \"$VOLUME_NAME\"; then echo \"Error: Volume $VOLUME_NAME does not exist\" exit 1 fi if ! docker run \\ -v \"$VOLUME_NAME\":/mount-volume \\ busybox \\ cp -Rp /mount-volume/. /volume-data/; then echo \"Error: Failed to start busybox container\" exit 1 fi CONTAINER_ID=$(docker ps -lq) docker commit -m \"saving volume $VOLUME_NAME to /volume-data\" \"$CONTAINER_ID\" \"$IMAGE_NAME\" docker container rm \"$CONTAINER_ID\" echo \"Successfully copied volume $VOLUME_NAME into image $IMAGE_NAME, under /volume-data\" } cmd_load() { IMAGE_NAME=\"$2\" VOLUME_NAME=\"$3\" if [ -z \"$VOLUME_NAME\" ] || [ -z \"$IMAGE_NAME\" ]; then echo \"Error: Not enough arguments\" usage exit 1 fi if ! docker volume inspect --format '{{.Name}}' \"$VOLUME_NAME\"; then echo \"Volume $VOLUME_NAME does not exist, creating...\" docker volume create \"$VOLUME_NAME\" fi if ! docker run --rm \\ -v \"$VOLUME_NAME\":/mount-volume \\ \"$IMAGE_NAME\" \\ cp -Rp /volume-data/. /mount-volume/; then echo \"Error: Failed to start container from $IMAGE_NAME\" exit 1 fi echo \"Successfully copied /volume-data from $IMAGE_NAME into volume $VOLUME_NAME\" } COMMAND=\"$1\" case \"$COMMAND\" in","date":"2022-11-18","objectID":"/2022/11/docker/volumes-backup/:0:0","tags":["docker"],"title":"Volumes Backup","uri":"/2022/11/docker/volumes-backup/"},{"categories":null,"content":"用法 ▶ ./vackup \"Docker Volume Backup\". Replicates image management commands for volumes. export/import copies files between a host tarball and a volume. For making volume backups and restores. save/load copies files between an image and a volume. For when you want to use image registries as a way to push/pull volume data. Usage: vackup export VOLUME FILE Creates a gzip'ed tarball in current directory from a volume vackup import FILE VOLUME Extracts a gzip'ed tarball into a volume vackup save VOLUME IMAGE Copies the volume contents to a busybox image in the /volume-data directory vackup load IMAGE VOLUME Copies /volume-data contents from an image to a volume 导出 .tar.gz，mysql_mysqldata 是 VOLUME NAME ▶ ./vackup export mysql_mysqldata backup.tar.gz 导入.tar.gz,mysql_mysqldata1是 新的 VOLUME NAME /vackup import 1.tar.gz mysql_mysqldata1 将volume做成镜像 ▶ ./vackup save mysql_mysqldata1 busybox mysql_mysqldata1 sha256:5b54490e2044723e308f0f9f5a9ce91b698062107b9bef310a725edca8dab5ca cb0a8fbc5ad8 Successfully copied volume mysql_mysqldata1 into image busybox, under /volume-data 导出从镜像中导出volume ./vackup load busybox:latest mysql_mysqldata1 Error: No such volume: mysql_mysqldata1 Volume mysql_mysqldata1 does not exist, creating... mysql_mysqldata1 Successfully copied /volume-data from busybox:latest into volume mysql_mysqldata1 docker desktop插件 GitHub ","date":"2022-11-18","objectID":"/2022/11/docker/volumes-backup/:1:0","tags":["docker"],"title":"Volumes Backup","uri":"/2022/11/docker/volumes-backup/"},{"categories":null,"content":"测试代码 package main import ( \"fmt\" \"net/http\" _ \"net/http/pprof\" \"runtime\" ) func main() { fmt.Println(runtime.NumCPU()) go http.ListenAndServe(\"0.0.0.0:6060\", nil) fmt.Println(fib(100)) select {} } func fib(n int) int { if n \u003c= 1 { return 1 } return fib(n-1) + fib(n-2) } ","date":"2022-11-15","objectID":"/2022/11/golang/pprof/:0:1","tags":["golang"],"title":"go pprof","uri":"/2022/11/golang/pprof/"},{"categories":null,"content":"go tool pprof 火焰图 go tool pprof -http :8080 profile ","date":"2022-11-15","objectID":"/2022/11/golang/pprof/:0:2","tags":["golang"],"title":"go pprof","uri":"/2022/11/golang/pprof/"},{"categories":null,"content":"go test 采集内存 go test -bench=. -memprofile=mem.prof 采集cpu go test -bench=. -cpuprofile=cpu.prof ","date":"2022-11-15","objectID":"/2022/11/golang/pprof/:0:3","tags":["golang"],"title":"go pprof","uri":"/2022/11/golang/pprof/"},{"categories":null,"content":"golang 框架和库 ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:0:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"web 框架 gin ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:1:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"orm gorm ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:2:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"显示进度条 progressbar ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:3:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"下载文件 grab ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:4:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"日志 zap ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:5:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"RPC gRPC rpcx ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:6:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"爬虫 colly ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:7:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"工具函数库 lancet ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:8:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"分布式存储 minio ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:9:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"redis cli ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:10:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"终端UI 非常漂亮的终端ui tview ","date":"2022-10-27","objectID":"/2022/10/golang/go-pkg/:11:0","tags":["golang"],"title":"go pkg","uri":"/2022/10/golang/go-pkg/"},{"categories":null,"content":"缘由 本地的docker的 k8s 经常启动失败，谷歌检索问题后，stack overflow 上的帖子说到一种解决方案docker-desktop-kubernetes-failed-to-start，没错，这个解决方案，要把本地的所有镜像容器给干掉包括docker volume，我想着本地的容器也没啥，唯独mysql的数据是挂载外部的 /Users/joker/docker/mysql/lib，于是我初始化了docker 的所有镜像和容器。但是当我重新启动mysql容器，绑定这个路径发现容器根本起不起来，内部错误 Unknown redo log format。完了，里面的数据读不出来。不过文件还在.ibd文件还在，应该可以恢复 ","date":"2022-10-22","objectID":"/2022/10/mysql/mysql-fix-problem/:1:0","tags":["mysql"],"title":"docker mysql 数据恢复","uri":"/2022/10/mysql/mysql-fix-problem/"},{"categories":null,"content":"恢复过程 启动一个纯净mysql容器 docker-compose.yml version: '3.9' services: mysql: image: mysql ports: - 3307:3306 restart: always volumes: - mysqldata:/var/lib/mysql - /Users/joker/docker/mysql/lib/pwd/my_pwd.ibd:/my_pwd.ibd environment: - MYSQL_ROOT_PASSWORD=12345678 - TZ=Asia/Shanghai volumes: mysqldata: 把/Users/joker/docker/mysql/lib/pwd/my_pwd.ibd映射到容器里面 把表结构同步到新的mysql中，并执行 ALTER TABLE my_pwd DISCARD TABLESPACE; /my_pwd.ibd 将拷贝到/var/lib/mysql/pwd中，并且修改权限 root@03dd4294e1cb:/var/lib/mysql/pwd# cp /my_pwd.ibd ./ root@03dd4294e1cb:/var/lib/mysql/pwd# chown mysql:mysql my_pwd.ibd ALTER TABLE my_pwd IMPORT TABLESPACE; 结果一看数据果然恢复回来了 ","date":"2022-10-22","objectID":"/2022/10/mysql/mysql-fix-problem/:2:0","tags":["mysql"],"title":"docker mysql 数据恢复","uri":"/2022/10/mysql/mysql-fix-problem/"},{"categories":null,"content":"docker nginx 部署静态网页 $ docker run -d --name=nginx -p 90:80 -v /Users/joker/blog/Joker_null/release:/usr/share/nginx/html nginx ","date":"2022-10-14","objectID":"/2022/10/nginx/nginx/:0:0","tags":["nginx"],"title":"Nginx","uri":"/2022/10/nginx/nginx/"},{"categories":null,"content":"dlv dlv 是golang 的调试工具 github 简易教程 简易例子 func main() { slice := make([]int, 0) for i := 1; i \u003c 10; i++ { slice = append(slice, i) } } cd 代码根目录 $ dlv deug Type 'help' for list of commands. (dlv) (dlv) help The following commands are available: Running the program: call ------------------------ Resumes process, injecting a function call (EXPERIMENTAL!!!) continue (alias: c) --------- Run until breakpoint or program termination. next (alias: n) ------------- Step over to next source line. rebuild --------------------- Rebuild the target executable and restarts it. It does not work if the executable was not built by delve. restart (alias: r) ---------- Restart process. step (alias: s) ------------- Single step through program. step-instruction (alias: si) Single step a single cpu instruction. stepout (alias: so) --------- Step out of the current function. Manipulating breakpoints: break (alias: b) ------- Sets a breakpoint. breakpoints (alias: bp) Print out info for active breakpoints. clear ------------------ Deletes breakpoint. clearall --------------- Deletes multiple breakpoints. condition (alias: cond) Set breakpoint condition. on --------------------- Executes a command when a breakpoint is hit. toggle ----------------- Toggles on or off a breakpoint. trace (alias: t) ------- Set tracepoint. watch ------------------ Set watchpoint. Viewing program variables and memory: args ----------------- Print function arguments. display -------------- Print value of an expression every time the program stops. examinemem (alias: x) Examine raw memory at the given address. locals --------------- Print local variables. print (alias: p) ----- Evaluate an expression. regs ----------------- Print contents of CPU registers. set ------------------ Changes the value of a variable. vars ----------------- Print package variables. whatis --------------- Prints type of an expression. Listing and switching between threads and goroutines: goroutine (alias: gr) -- Shows or changes current goroutine goroutines (alias: grs) List program goroutines. thread (alias: tr) ----- Switch to the specified thread. threads ---------------- Print out info for every traced thread. Viewing the call stack and selecting frames: deferred --------- Executes command in the context of a deferred call. down ------------- Move the current frame down. frame ------------ Set the current frame, or execute command on a different frame. stack (alias: bt) Print stack trace. up --------------- Move the current frame up. Other commands: config --------------------- Changes configuration parameters. disassemble (alias: disass) Disassembler. dump ----------------------- Creates a core dump from the current process state edit (alias: ed) ----------- Open where you are in $DELVE_EDITOR or $EDITOR exit (alias: quit | q) ----- Exit the debugger. funcs ---------------------- Print list of functions. help (alias: h) ------------ Prints the help message. libraries ------------------ List loaded dynamic libraries list (alias: ls | l) ------- Show source code. source --------------------- Executes a file containing a list of delve commands sources -------------------- Print list of source files. transcript ----------------- Appends command output to a file. types ---------------------- Print list of types Type help followed by a command for full documentation. ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:0:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"break 设置一个断点 (dlv) break main.main Breakpoint 1 set at 0x1058eea for main.main() ./main.go:3 ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:1:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"breakpoints 查看设置的断点 (dlv) breakpoints Breakpoint runtime-fatal-throw (enabled) at 0x102dae0 for runtime.throw() /usr/local/go/src/runtime/panic.go:1188 (0) Breakpoint unrecovered-panic (enabled) at 0x102de40 for runtime.fatalpanic() /usr/local/go/src/runtime/panic.go:1271 (0) print runtime.curg._panic.arg Breakpoint 1 (enabled) at 0x1058eea for main.main() ./main.go:3 (0) ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:2:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"vars 查看全部包级的变量。 因为最终的目标程序可能含有大量的全局变量，我们可以通过一个正则参数选择想查看的全局变量： (dlv) vars main runtime.main_init_done = chan bool nil runtime.mainStarted = false continue 执行下一个断点 (dlv) continue \u003e main.main() ./main.go:3 (hits goroutine(1):1 total:1) (PC: 0x1058eea) 1: package main 2: =\u003e 3: func main() { 4: slice := make([]int, 0) 5: for i := 1; i \u003c 10; i++ { 6: slice = append(slice, i) 7: } 8: } (dlv) ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:3:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"next 或者 n单步执行， =\u003e表示所运行的位置 (dlv) n \u003e main.main() ./main.go:5 (PC: 0x1058f0d) 1: package main 2: 3: func main() { 4: slice := make([]int, 0) =\u003e 5: for i := 1; i \u003c 10; i++ { 6: slice = append(slice, i) 7: } 8: } ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:4:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"locals 打印局部变量 (dlv) locals slice = []int len: 2, cap: 2, [...] i = 3 ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:5:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"break \u0026\u0026 condition 可以使用 break 和 condition组合命令，在循环内部条件断点，例如下面这个操作 (dlv) break main.go:6 Breakpoint 2 set at 0x1058f24 for main.main() ./main.go:6 (dlv) condition 2 i==7 (dlv) continue \u003e main.main() ./main.go:6 (hits goroutine(1):1 total:1) (PC: 0x1058f24) 1: package main 2: 3: func main() { 4: slice := make([]int, 0) 5: for i := 1; i \u003c 10; i++ { =\u003e 6: slice = append(slice, i) 7: } 8: } (dlv) locals slice = []int len: 6, cap: 8, [...] i = 7 (dlv) break main.go:6根据行数设置一个断点，并且在i==7时生效 condition 2 i==7 2的意思是设置断点的索引(第几个断点 根据\"Breakpoint 2 set at 0x1058f24 for main.main() ./main.go:6\" 可知是2) locals 一下显示 slice = []int len: 6, cap: 8, […] i = 7 ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:6:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"print 打印局部变量 (dlv) print slice []int len: 6, cap: 8, [1,2,3,4,5,6] ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:7:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"stack 查看当前执行函数的栈帧信息： (dlv) stack 0 0x0000000001058f24 in main.main at ./main.go:6 1 0x00000000010300b3 in runtime.main at /usr/local/go/src/runtime/proc.go:255 2 0x0000000001056261 in runtime.goexit at /usr/local/go/src/runtime/asm_amd64.s:1581 ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:8:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"goroutine or goroutines goroutine 信息 (dlv) goroutine Thread 4125012 at ./main.go:6 Goroutine 1: Runtime: ./main.go:6 main.main (0x1058f24) User: ./main.go:6 main.main (0x1058f24) Go: \u003cautogenerated\u003e:1 runtime.newproc (0x1058369) Start: /usr/local/go/src/runtime/proc.go:145 runtime.main (0x102fec0) * Goroutine 1 - User: ./main.go:6 main.main (0x1058f24) (thread 4125012) Goroutine 2 - User: /usr/local/go/src/runtime/proc.go:367 runtime.gopark (0x10304d2) [force gc (idle)] Goroutine 3 - User: /usr/local/go/src/runtime/proc.go:367 runtime.gopark (0x10304d2) [GC sweep wait] Goroutine 4 - User: /usr/local/go/src/runtime/proc.go:367 runtime.gopark (0x10304d2) [GC scavenge wait] [4 goroutines] 调试服务 已知一个服务已经在运行，可以用dlv attach pid来调试 ，例如 gameserver 是一个tcp服务，监听8840端口 ▶ ./gameserver -c conf/conf.json 2022-10-13 09:22:41.593 INFO engine/server.go:29 game Server is listening on:8840 查看gameserver的pid $ ps -ef | grep gameserver 501 49913 34703 0 9:22AM ttys012 0:00.03 ./gameserver -c conf/conf.json pid 是49913 ▶ dlv attach 49913 Type 'help' for list of commands. (dlv) 至此就可以愉快的调试网络服务了，以客户端发送一个鉴权包作为了例子 ","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:9:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"例子 (dlv) break OnTraffic Command failed: Location \"OnTraffic\" ambiguous: github.com/cat3306/gameserver/engine.(*Server).OnTraffic, github.com/panjf2000/gnet/v2.(*BuiltinEventEngine).OnTraffic… (dlv) break github.com/cat3306/gameserver/engine.(*Server).OnTraffic Breakpoint 1 set at 0x1282d2f for github.com/cat3306/gameserver/engine.(*Server).OnTraffic() /Users/joker/code/go/src/github.com/cat3306/gameserver/engine/server.go:32 OnTraffic 有网络数据时回调的函数，这时候运行客户端，并且发送一个鉴权的数据包，客户端阻塞住了 dlv这边执行continue，可以看到=\u003e指向OnTraffic,表示服务运行到这里了 (dlv) continue \u003e github.com/cat3306/gameserver/engine.(*Server).OnTraffic() /Users/joker/code/go/src/github.com/cat3306/gameserver/engine/server.go:32 (hits goroutine(25):1 total:1) (PC: 0x1282d2f) Warning: debugging optimized function 27: func (s *Server) OnBoot(e gnet.Engine) (action gnet.Action) { 28: s.eng = e 29: glog.Logger.Sugar().Infof(\"game Server is listening on:%d\", conf.GameConfig.Port) 30: return 31: } =\u003e 32: func (s *Server) OnTraffic(c gnet.Conn) gnet.Action { 33: defer func() { 34: err := recover() 35: if err != nil { 36: glog.Logger.Sugar().Errorf(\"OnTraffic panic %v\", err) 37: } (dlv) netx or n单步运行 \u003e github.com/cat3306/gameserver/engine.(*Server).OnTraffic() /Users/joker/code/go/src/github.com/cat3306/gameserver/engine/server.go:40 (PC: 0x1282d9b) Warning: debugging optimized function 35: if err != nil { 36: glog.Logger.Sugar().Errorf(\"OnTraffic panic %v\", err) 37: } 38: }() 39: s.eng.CountConnections() 40: context, err := protocol.Decode(c) =\u003e 41: if err != nil { 42: glog.Logger.Sugar().Warnf(\"OnTraffic err:%s\", err.Error()) 43: return gnet.None 44: } 45: if context == nil { 打印context print context (dlv) print context *github.com/cat3306/gameserver/protocol.Context { Payload: []uint8 len: 552, cap: 65526, [123,34,67,105,112,104,101,114,84,101,120,116,34,58,34,79,84,70,105,68,102,56,69,102,75,109,52,75,83,81,80,117,56,76,101,98,43,98,103,82,43,78,119,97,120,75,81,56,70,47,79,65,55,54,111,66,100,55,89,49,79,121,85,122,...+488 more], CodeType: Json (2), Proto: 2105448169, Conn: github.com/panjf2000/gnet/v2.Conn(*github.com/panjf2000/gnet/v2.conn) *{ ctx: interface {} nil, peer: golang.org/x/sys/unix.Sockaddr(*golang.org/x/sys/unix.SockaddrInet6) ..., localAddr: net.Addr(*net.TCPAddr) ..., remoteAddr: net.Addr(*net.TCPAddr) ..., loop: *(*\"github.com/panjf2000/gnet/v2.eventloop\")(0xc00015acf0), outboundBuffer: *(*\"github.com/panjf2000/gnet/v2/pkg/buffer/elastic.Buffer\")(0xc000242cd0), pollAttachment: *(*\"github.com/panjf2000/gnet/v2/internal/netpoll.PollAttachment\")(0xc000241d10), inboundBuffer: (*\"github.com/panjf2000/gnet/v2/pkg/buffer/elastic.RingBuffer\")(0xc0001b0418), buffer: []uint8 len: 0, cap: 64974, [], fd: 19, isDatagram: false, opened: true, id: \"TljczpztN\", property: map[string]string [], propertyLock: (*sync.RWMutex)(0xc0001b0460),}, connMgr: *github.com/cat3306/gameserver/protocol.ConnManager nil,} print这个命令非常强大,可以打印具体的字段，比如 (dlv) print context.Proto 2105448169 (dlv) print context.CodeType Json (2) (dlv) print context.Conn.id \"TljczpztN\" (dlv) print context.Conn.remoteAddr net.Addr(*net.TCPAddr) *{ IP: net.IP len: 16, cap: 16, [0,0,0,0,0,0,0,0,0,0,255,255,127,0,0,1], Port: 54783, Zone: \"\",} (dlv) 设置另一个断点，然后continue进入执行handler的函数 (dlv) break ExeHandler Breakpoint 3 set at 0x12829f2 for github.com/cat3306/gameserver/engine.(*HandlerManager).ExeHandler() /Users/joker/code/go/src/github.com/cat3306/gameserver/engine/handlermgr.go:190 (dlv) continue \u003e github.com/cat3306/gameserver/engine.(*HandlerManager).ExeHandler() /Users/joker/code/go/src/github.com/cat3306/gameserver/engine/handlermgr.go:190 (hits goroutine(25):1 total:1) (PC: 0x12829f2) Warning: debugging optimized function 185: f(ctx, nil) 186: return nil 187: } 188: return HandlerNotFound 189: } =\u003e 190: func (h *HandlerManager) ExeHandler(auth bool, ctx *protocol.Context) { 191: err := h.exeSyncHandler(auth, ctx) 192: if !errors.Is(err, HandlerNotFound) { 193: return 194: } 195: err = h.exeAsyncHandler(auth, ctx) 中间过程和前面一样，直接跳到Cl","date":"2022-10-12","objectID":"/2022/10/golang/go-dlv/:10:0","tags":["golang"],"title":"go dlv","uri":"/2022/10/golang/go-dlv/"},{"categories":null,"content":"条件查询 \"must\": [] wildcard:模糊查询 POST user_action_record/_search { \"size\": 1000, \"from\": 0, \"query\": { \"bool\": { \"must\": [ { \"range\": { \"create_date\": { \"gte\": \"2022-07-30 17:23:55\", \"lte\": \"2022-07-31 17:23:55\" } } }, { \"match\": { \"public.appver\": \"1.5.1\" } }, { \"wildcard\": { \"context.ex1\": \"*没有获取*\" } } ] } } } 对应的sql SELECT * FROM user_action_record WHERE create_date \u003e= '2022-07-30 17:23:55' AND create_date \u003c= '2022-07-31 17:23:55' AND appver = '1.5.1' AND ex1 LIKE '%没有获取%' LIMIT 0, 1000 ","date":"2022-08-04","objectID":"/2022/08/es/es-application-scene/:1:0","tags":["es"],"title":"es 的应用场景","uri":"/2022/08/es/es-application-scene/"},{"categories":null,"content":"IN 查询 对多个uid查询 { \"size\": 20000, \"query\": { \"bool\": { \"must\": [ { \"range\": { \"create_date\": { \"gte\": \"2022-08-01 00:00:00\", \"lte\": \"2022-08-01 23:59:59\" } } }, { \"match\": { \"context.action\": \"boost_click\" } }, { \"terms\": { \"public.uid\": [ \"8686520496128\", \"3151070081024\", \"7021632212992\", \"4221968166912\", \"9486033596416\", \"5187451686912\", \"7987553398784\" ] } } ] } }, \"_source\": \"context.ex1\" } ","date":"2022-08-04","objectID":"/2022/08/es/es-application-scene/:2:0","tags":["es"],"title":"es 的应用场景","uri":"/2022/08/es/es-application-scene/"},{"categories":null,"content":"根据时间分组查询 并且根据分组的时间倒序排列 { \"size\": 0, \"query\": { \"bool\": { \"must\": [ { \"range\": { \"create_date\": { \"gte\": \"2022-08-03 00:00:00\", \"lte\": \"2022-08-04 23:59:59\" } } }, { \"match\": { \"context.action\": \"app_new_install\" } }, { \"match\": { \"type\": \"app_info\" } } ] } }, \"aggs\": { \"group\": { \"date_histogram\": { \"field\": \"create_date\", \"calendar_interval\": \"day\", \"format\": \"yyyy-MM-dd\", \"order\": { \"_key\": \"desc\" } } } } } ","date":"2022-08-04","objectID":"/2022/08/es/es-application-scene/:3:0","tags":["es"],"title":"es 的应用场景","uri":"/2022/08/es/es-application-scene/"},{"categories":null,"content":"mapping 删除 DELETE user_action_record_2022-10-18 获取mapping信息 GET user_action_record_2022-10-18/_mapping ","date":"2022-08-04","objectID":"/2022/08/es/es-application-scene/:4:0","tags":["es"],"title":"es 的应用场景","uri":"/2022/08/es/es-application-scene/"},{"categories":null,"content":"往事如风 最近时常想起她来，细细想来已有很多年未曾打过交道，也不知道她最近怎么样了，当时在绵阳火车站分别的时候，就想着这辈子最好不要相见了，下了火车站就把她的联系方式删的一干二净，企图从心里抹去这个人存在，当时坚定的认为她再也不是原来那个她，不过现在后悔也没用，感觉早已消亡，留下一副空壳罢了，但我愿意回忆那段尘封已久的记忆，与其说我怀恋她，倒不如说怀恋那个时候的自己。也许每一个男子全都有过这样的两个女人，至少两个。娶了红玫瑰，久而久之，红的变了墙上的一抹蚊子血，白的还是\"床前明月光\"；娶了白玫瑰，白的便是衣服上沾的一粒饭黏子，红的却是心口上一颗朱砂痣-----张爱玲 ","date":"2022-08-03","objectID":"/2022/08/thoughts-flowing-like-water/pure-love/:1:0","tags":null,"title":"纯爱","uri":"/2022/08/thoughts-flowing-like-water/pure-love/"},{"categories":null,"content":"小学同学 年少的我喜欢的那个女孩儿，有一个双胞胎妹妹，她们是我的小学同学。她妹妹曾经是我的同桌，上完体育课会喝我从家里带来的白开水，小时候能有什么感触，只是觉的作业好多，每天都玩不够。小学的时候我对她的印象只有那么一点点，她的作文功底非常好，常常会被当做范文全班朗读，在班上声情并茂朗读自己的写的作文，是我这种差生梦寐以求的事情。那个时候我只记得她走上讲台，念起自己的作文，有那么一种骄傲的姿态。我对她的印象仅此而已，那时候我和她妹妹比较熟络，谁又能想到几年后她对我的影响会那么大 ","date":"2022-08-03","objectID":"/2022/08/thoughts-flowing-like-water/pure-love/:2:0","tags":null,"title":"纯爱","uri":"/2022/08/thoughts-flowing-like-water/pure-love/"},{"categories":null,"content":"初中 初中我和她并没有什么交集，虽说都在同一个学校里，总共碰见也没几次。唯一的一次是在学校的小卖部，偶遇俩姐妹，毕竟是小学同学简单的打了招呼，大家都进入了青春期，那个时候就觉得双胞胎挺让人羡慕的。后来听到一些零碎的言论，初二的时候她们妈妈好像得癌症走了。 ","date":"2022-08-03","objectID":"/2022/08/thoughts-flowing-like-water/pure-love/:3:0","tags":null,"title":"纯爱","uri":"/2022/08/thoughts-flowing-like-water/pure-love/"},{"categories":null,"content":"高中同学 这世间的事论巧合还真不少。我记得高中入学报到那天，看了看贴在墙上的花名单，我发现我和那对双胞胎在同一个班。当时并不觉得有什么，毕竟那个时候还没有刻骨铭心的事情。直到开学后不久，好巧不巧的是我和姐姐同桌，不久我就喜欢上了她，她身上有一种让我无法抗拒的力量，我并不知道我为什么如此迷恋她，后来我才知道应该和原生家庭有关系。如今二十六岁的我依旧记得高一的那个学期和她发生的事情，虽说相处的时间很短暂但足以让我怀恋一生。实际上她算不上特别漂亮，只是一个活泼好动的高中生罢了。每次下课总是吵嚷着要和男生玩打手掌游戏，两人划拳，输的一方手放在桌子上，另一方的手则距离桌子一定高度用力拍下去，如果对方反应快，赢家便打到桌子上，这是一个考验反应的游戏。作为同桌的我经常陪她玩这个游戏直到上课，一来二去也算是朋友了。夏天的余热终究散去，内心的情愫却似野草般疯长。我常常时不时偷看她一眼，满眼都是青春的摸样。我不知道我度过多少秋日，只觉得那段时间过得太快，太快。在某个深秋的晚上，我怀着忐忑不安的心情递出了第一封情书。我希望她知道我喜欢她，我想带她去骑自行车，正如我现在听的《简单爱》里面的歌词一样。 ","date":"2022-08-03","objectID":"/2022/08/thoughts-flowing-like-water/pure-love/:4:0","tags":null,"title":"纯爱","uri":"/2022/08/thoughts-flowing-like-water/pure-love/"},{"categories":null,"content":"陡峭的下坡，她紧张地揪着我的连衣帽 ","date":"2022-08-03","objectID":"/2022/08/thoughts-flowing-like-water/pure-love/:5:0","tags":null,"title":"纯爱","uri":"/2022/08/thoughts-flowing-like-water/pure-love/"},{"categories":null,"content":"统计重复字段的重复次数 SELECT game_id,count(1)as c FROM android_game_detail GROUP BY game_id HAVING c\u003e1 ","date":"2022-08-03","objectID":"/2022/08/mysql/mysql-application-scene/:1:0","tags":["mysql"],"title":"mysql application scene","uri":"/2022/08/mysql/mysql-application-scene/"},{"categories":null,"content":"LEFT JOIN LEFT JOIN 可将两张或两张以上的表以某种条件连接起来，最左边的表作为主表，故叫LEFT JOIN 基本语法 SELECT select_list FROM t1 LEFT JOIN t2 ON join_condition; 在上面的语法中，t1是左表(主表)，t2是右表 LEFT JOIN 从左表开始(t1)筛选数据，并且t1的每一条与t2的每一条基于join_condition做判断，如此一来会有两种情况 如果join_condition的值为true,则LEFT JOIN两个表中行合并成一个新的数据行 如果join_condition的值为false，即t1的数据与t2的任何一条都不匹配，依然生成新的一条数据，但是t2的所有列都是NULL 也就是说LEFT JOIN返回的是t1所有的数据条数，而不管是否与t2的数据匹配，能匹配则带上t2的数据，不能匹配用NULL占位 ","date":"2022-08-02","objectID":"/2022/08/mysql/mysql-joins/:0:0","tags":["mysql"],"title":"mysql joins","uri":"/2022/08/mysql/mysql-joins/"},{"categories":null,"content":"例子 Download MySQL Sample Database ","date":"2022-08-02","objectID":"/2022/08/mysql/mysql-joins/:1:0","tags":["mysql"],"title":"mysql joins","uri":"/2022/08/mysql/mysql-joins/"},{"categories":null,"content":"两张表 这是顾客表和订单表 SELECT c.`customerNumber`, c.`customerName`, o.`orderNumber`, o.`status` FROM customers as c LEFT JOIN orders as o ON c.customerNumber = o.customerNumber; 可以看见有些数据的右表字段是NULL 利用这个特性，很容知道哪些顾客没有订单 SELECT c.`customerNumber`, c.`customerName`, o.`orderNumber`, o.`status` FROM customers as c LEFT JOIN orders as o ON c.customerNumber = o.customerNumber WHERE o.`orderNumber` IS NULL ","date":"2022-08-02","objectID":"/2022/08/mysql/mysql-joins/:1:1","tags":["mysql"],"title":"mysql joins","uri":"/2022/08/mysql/mysql-joins/"},{"categories":null,"content":"三张表 如果是三张表，情况如下 SELECT e.lastName, e.firstName, c.customerName, p.checkNumber, p.amount FROM employees as e LEFT JOIN customers as c ON e.employeeNumber = c.salesRepEmployeeNumber LEFT JOIN payments as p ON p.customerNumber = c.customerNumber ORDER BY c.customerName, p.checkNumber; 第一次LEFT JOIN是employees和customers关联，当关联的条件为false时，customers部分字段用NULL表示 第二次LEFT JOIN是customers和payments关联，当关联的条件为false时，payments部分字段用NULL表示 可以看到第一次关联不到数据项，第二次也关联不上，即如果customerName为NULL则payments部分字段必为NULL ","date":"2022-08-02","objectID":"/2022/08/mysql/mysql-joins/:1:2","tags":["mysql"],"title":"mysql joins","uri":"/2022/08/mysql/mysql-joins/"},{"categories":null,"content":"过滤条件应该放哪儿 将过滤条件放在WHERE中 SELECT o.orderNumber, customerNumber, productCode FROM orders as o LEFT JOIN orderDetails as od ON o.orderNumber=od.orderNumber WHERE o.orderNumber = 10123; 将过滤条件放在ON中 SELECT o.orderNumber, customerNumber, productCode FROM orders AS o LEFT JOIN orderDetails AS od ON o.orderNumber = od.orderNumber AND o.orderNumber = 10123; 看出差别来了吗 过滤条件放在连接语句的ON里面，是作为一种连接条件，只会关联o.orderNumber = od.orderNumber 并且o.orderNumber = 10123,但是左表依然是全部查询，只是关联条件比原来(o.orderNumber = od.orderNumber)苛刻了 而放在WHERE里面是将连接表作为整体来看，只展示o.orderNumber = 10123的数据 RIGHT JOIN RIGHT JOIN 和 LEFT JOIN 非常类似，唯一的区别是哪个表作为主表 SELECT select_list FROM t1 RIGHT JOIN t2 ON join_condition; 在上面的sql中，t2作为主表 SELECT employeeNumber, customerNumber FROM customers RIGHT JOIN employees ON salesRepEmployeeNumber = employeeNumber ORDER BY employeeNumber; INNER JOIN INNER JOIN 取几个表的交集 SELECT select_list FROM t1 INNER JOIN t2 ON join_condition1 INNER JOIN t3 ON join_condition2 ...; ","date":"2022-08-02","objectID":"/2022/08/mysql/mysql-joins/:1:3","tags":["mysql"],"title":"mysql joins","uri":"/2022/08/mysql/mysql-joins/"},{"categories":null,"content":"例子 SELECT c.`customerNumber`, c.`customerName`, o.`orderNumber`, o.`status` FROM customers as c INNER JOIN orders as o ON c.customerNumber = o.customerNumber WHERE `status` IS NULL; 可以发现，WHERE status IS NULL 的结果集为空，这和LEFT JOIN不一样，这是因为INNER JOIN 只有join_condition为true时才会生成新的数据行 可以和LEFT JOIN (RIGHT JOIN)搭配起来使用 employees表(主表)和customers表内联，和payments左联 SELECT e.lastName, e.firstName, c.customerName, p.checkNumber, p.amount FROM employees as e INNER JOIN customers as c ON e.employeeNumber = c.salesRepEmployeeNumber LEFT JOIN payments as p ON p.customerNumber = c.customerNumber ORDER BY c.customerName, p.checkNumber; employees表(主表)和customers表左联，和payments内联 SELECT e.lastName, e.firstName, c.customerName, p.checkNumber, p.amount FROM employees as e LEFT JOIN customers as c ON e.employeeNumber = c.salesRepEmployeeNumber INNER JOIN payments as p ON p.customerNumber = c.customerNumber ORDER BY c.customerName, p.checkNumber; ","date":"2022-08-02","objectID":"/2022/08/mysql/mysql-joins/:1:4","tags":["mysql"],"title":"mysql joins","uri":"/2022/08/mysql/mysql-joins/"},{"categories":null,"content":"为什么要同步 操作系统中进程拥有自己的虚拟内存空间，彼此访问不了对方的内存区域，这是操作系统层面上的内存安全。但是由于线程是操作系统的调度单位，多个线程共享进程的内存空间，并发的访问(读写)同一块内存会出现一些诡异的问题。同步访问内存是多线程编程不可逾越的操作。对golang稍微熟悉一点就会知道常用的map并发并不安全，如果出现多个goroutine同时读写，程序会发生崩溃。 互斥锁 锁的基本思想是：同一个时刻只有一个线程处于一块代码段，这块代码段称为临界区，刚开始的时候thread0进入临界区前会检查一个变量state是否为0，如果是0代表可以进入临界区，并且将state置为1，处理完逻辑后，离开临界区把state置为0。当然对于state操作是原子性的，否则这是个无穷无尽的并发问题。具体思想如下 package main import ( \"fmt\" \"sync/atomic\" \"time\" ) var ( state int32 CriticalZone = map[int]int{} WriteCount int ReadCount int ) const ( LockState = 1 UnLockState = 0 ) func Lock() bool { return atomic.CompareAndSwapInt32(\u0026state, UnLockState, LockState) } func UnLock() bool { return atomic.CompareAndSwapInt32(\u0026state, LockState, UnLockState) } func write() { for { if Lock() { WriteCount++ CriticalZone[WriteCount] = WriteCount UnLock() } } } func read() { for { if Lock() { ReadCount++ s := CriticalZone[ReadCount] fmt.Println(s) time.Sleep(time.Millisecond*200) UnLock() } } } func main() { CriticalZone = make(map[int]int) go write() go read() select {} } 实现了非常简单的互斥锁，如果把Lock 和UnLock的部分去掉，则会panic fatal error: concurrent map read and map write func write() { for { WriteCount++ CriticalZone[WriteCount] = WriteCount } } func read() { for { ReadCount++ s := CriticalZone[ReadCount] fmt.Println(s) time.Sleep(time.Millisecond * 200) } } 基本原语 golang在sync包中提供了一些基本的同步原语 sync.Mutex,sync.RWMutex,sync.WaitGroup,sync.Once,sync.Cond ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:0:0","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Mutex sync.Mutex的数据结构长这个样子，其中state表示互斥锁的状态，sema表示控制锁状态的信号量 //usr/local/go/src/sync/mutex.go type Mutex struct { state int32 sema uint32 } state的枚举 ///usr/local/go/src/sync/mutex.go const ( mutexLocked = 1 \u003c\u003c iota // mutex is locked mutexWoken mutexStarving mutexWaiterShift = iota ..... 这些状态用二进制的位来表示 其中mutexWaiterShift的值是3，表示除了最低三位以外其他的位用来记录排队等待的goroutine个数 官方的注释中提到mutex有正常模式和饥饿模式(normal and starvation) 在正常模式中，阻塞的goroutine满足排队的先后顺序，即FIFO order，但是该模式有个问题：刚刚唤醒的goroutine和那些已经在CPU执行的goroutine竞争互斥锁并没有优势—–实际上也确实如此，那些拥有CPU的goroutine可能会有很多，因此唤醒的goroutine可能会被这个机制\"饿死\"，而饥饿模式正是为了解决这个问题的。如果到达的goroutine尝试获取互斥锁超过了1ms,互斥锁则会切换成饥饿模式。 在饥饿模式中，等待队列的第一个goroutine会直接获得CPU的执行权，那些刚刚到达的goroutine并不会和唤醒那个goroutine竞争互斥锁，也不会进入自旋状态，相反他们会直接追加到等待队列的尾部。 如果一个 goroutine 获得了互斥锁并且它在队列的末尾或者它等待的时间少于 1ms，那么当前的互斥锁就会切换回正常模式 相比于饥饿模式，正常模式能有更好的性能，但是饥饿模式能很好的解决等待队列尾部高延迟的情况 ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:1:0","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Lock sync.Mutex对外暴露的函数非常简单 –Lock Unlock，它实现了Locker 的接口。Lock是尝试持有锁的操作而Unlock释放锁的操作 // usr/local/go/src/sync/mutex.go // A Locker represents an object that can be locked and unlocked. type Locker interface { Lock() Unlock() } 在golang源码中，Lock长这个样子 ///usr/local/go/src/sync/mutex.go func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } atomic.CompareAndSwapInt32这是一个对int32类型变量的原子操作，函数名很直观–比较并交换，如果这一步是false(表示已经有其他的goroutine持有锁)，则进入lockSlow // usr/local/go/src/sync/mutex.go func (m *Mutex) lockSlow() { var waitStartTime int64 starving := false awoke := false iter := 0 old := m.state for { // Don't spin in starvation mode, ownership is handed off to waiters // so we won't be able to acquire the mutex anyway. if old\u0026(mutexLocked|mutexStarving) == mutexLocked \u0026\u0026 runtime_canSpin(iter) { // Active spinning makes sense. // Try to set mutexWoken flag to inform Unlock // to not wake other blocked goroutines. if !awoke \u0026\u0026 old\u0026mutexWoken == 0 \u0026\u0026 old\u003e\u003emutexWaiterShift != 0 \u0026\u0026 atomic.CompareAndSwapInt32(\u0026m.state, old, old|mutexWoken) { awoke = true } runtime_doSpin() iter++ old = m.state continue } 官方的注释中提到当互斥锁处于饥饿模式的时候，进来的goroutine不会自旋，而是将所有权交给刚刚唤醒的goroutine Tip 自旋(spin)是一种多线程同步机制，其思想非常简单：循环的去尝试检查某个条件是否为真。自旋过程会一直占用CPU，这样的一个优势是在多核的CPU上，自旋可以避免thread/routine的切换，减少上下文的切换的成本。但是自旋本身是个耗CPU的操作，操作不当可能会损耗系统的性能。 接下来是计算state新的状态 //usr/local/go/src/sync/mutex.go new := old // Don't try to acquire starving mutex, new arriving goroutines must queue. if old\u0026mutexStarving == 0 { new |= mutexLocked } if old\u0026(mutexLocked|mutexStarving) != 0 { new += 1 \u003c\u003c mutexWaiterShift } // The current goroutine switches mutex to starvation mode. // But if the mutex is currently unlocked, don't do the switch. // Unlock expects that starving mutex has waiters, which will not // be true in this case. if starving \u0026\u0026 old\u0026mutexLocked != 0 { new |= mutexStarving } if awoke { // The goroutine has been woken from sleep, // so we need to reset the flag in either case. if new\u0026mutexWoken == 0 { throw(\"sync: inconsistent mutex state\") } new \u0026^= mutexWoken } ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:1:1","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Unlock // usr/local/go/src/sync/mutex.go // Unlock unlocks m. // It is a run-time error if m is not locked on entry to Unlock. // // A locked Mutex is not associated with a particular goroutine. // It is allowed for one goroutine to lock a Mutex and then // arrange for another goroutine to unlock it. func (m *Mutex) Unlock() { if race.Enabled { _ = m.state race.Release(unsafe.Pointer(m)) } // Fast path: drop lock bit. new := atomic.AddInt32(\u0026m.state, -mutexLocked) if new != 0 { // Outlined slow path to allow inlining the fast path. // To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock. m.unlockSlow(new) } } 解锁的过程要分为两种情况进行处理 如果atomic.AddInt32返回的值是0代表解锁成功 如果不等于0则进入慢解锁的过程:unlockSlow // usr/local/go/src/sync/mutex.go func (m *Mutex) unlockSlow(new int32) { if (new+mutexLocked)\u0026mutexLocked == 0 { throw(\"sync: unlock of unlocked mutex\") } if new\u0026mutexStarving == 0 { old := new for { if old\u003e\u003emutexWaiterShift == 0 || old\u0026(mutexLocked|mutexWoken|mutexStarving) != 0 { return } new = (old - 1\u003c\u003cmutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { runtime_Semrelease(\u0026m.sema, false, 1) return } old = m.state } } else { runtime_Semrelease(\u0026m.sema, true, 1) } } 在正常模式下 如果不存在等待的gorutine队列或者互斥锁的mutexLocked，mutexWoken,mutexStarving都不为0直接返回 如果存在等待的gorutine队列，会通过runtime_Semrelease唤醒等待的gorutine,并让它持有锁 饥饿模式下，会直接通过runtime_Semrelease唤醒等待的gorutine,并让它持有锁。 ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:1:2","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"RWMutex 读写互斥锁是细粒度的锁，它不限制并发读的情况，但是不能并发的读写，写写 读 写 读 ✅ ❎ 写 ❎ ❎ 结构体 type RWMutex struct { w Mutex // held if there are pending writers writerSem uint32 // semaphore for writers to wait for completing readers readerSem uint32 // semaphore for readers to wait for completing writers readerCount int32 // number of pending readers readerWait int32 // number of departing readers } w 持有互斥锁的功能 writerSem 写的信号量 readerSem 读的信号量 readerCount 读操作个数 readerWait 表示写操作被阻塞时等待的读操作个数 对外暴露的函数有sync.RWMutex.Lock sync.RWMutex.Unlock sync.RWMutex.Rlock sync.RWMutex.Runlock 写操作对应的操作是sync.RWMutex.Lock 和 sync.RWMutex.Unlock 读操作对应的操作是 sync.RWMutex.Runlock 和 sync.RWMutex.Rlock ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:2:0","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Lock // /usr/local/go/src/sync/rwmutex.go func (rw *RWMutex) Lock() { // First, resolve competition with other writers. rw.w.Lock() // Announce to readers there is a pending writer. r := atomic.AddInt32(\u0026rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // Wait for active readers. if r != 0 \u0026\u0026 atomic.AddInt32(\u0026rw.readerWait, r) != 0 { runtime_SemacquireMutex(\u0026rw.writerSem, false, 0) } } 这个过程有以下操作 复用互斥锁的功能尝试的去获取锁，如果已经有其他goroutine已经持有锁，则自己进入自旋或者休眠 当获取锁后，还会判断是否有读的情况，atomic.AddInt32(\u0026rw.readerCount, -rwmutexMaxReaders)+ rwmutexMaxReaders 如果这个计算不为0,表示有其他的goroutine持有读锁，则调用runtime_SemacquireMutex进入休眠状态等待所有读锁所有者执行结束后释放 writerSem 信号量将当前goroutine唤醒 ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:2:1","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Unlock ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:2:2","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"WaitGroup ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:3:0","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Once sync.Once 的语义是保证某个函数只执行一次，这在一些初始化全局变量中非常有用。 once的结构体非常简单 // /usr/local/go/src/sync/once.go type Once struct { done uint32 m Mutex } done 是否已调用某函数的标志位 m互斥锁 ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:4:0","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"do 如果已经调用过，即atomic.LoadUint32(\u0026o.done) !=0，直接返回，否则进入doSlow // /usr/local/go/src/sync/once.go func (o *Once) Do(f func()) { if atomic.LoadUint32(\u0026o.done) == 0 { o.doSlow(f) } } ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:4:1","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"doSlow // /usr/local/go/src/sync/once.go func (o *Once) doSlow(f func()) { o.m.Lock() defer o.m.Unlock() if o.done == 0 { defer atomic.StoreUint32(\u0026o.done, 1) f() } } 可以看出如果传入的是不同的func,只会执行第一个func ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:4:2","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"Cond 扩展原语 ","date":"2022-07-29","objectID":"/2022/07/golang/go-sync/:5:0","tags":["golang"],"title":"go sync","uri":"/2022/07/golang/go-sync/"},{"categories":null,"content":"分布式锁 set joker 1 nx ex 10 var delScript = ` if redis.call(\"get\", KEYS[1]) == ARGV[1] then return redis.call(\"del\", KEYS[1]) else return 0 end` ","date":"2022-07-28","objectID":"/2022/07/redis/redis-application-scene/:0:0","tags":["redis"],"title":"redis 应用场景","uri":"/2022/07/redis/redis-application-scene/"},{"categories":null,"content":"蛮荒时代 细细算来，现存的第一篇markdown笔记距离现在应该有一年多，去年九月份的时候应该是出于工作原因到处寻找能够记录笔记的地方。CSDN和博客园都尝试过，无奈这些三方博客平台注册有很多限制，记得当时注册博客园人工审核了很多次，最终放弃了第三方，更何况CSDN并不优秀。当时的我已经知道github上是可以搭建自己的博客，但刚开始并没有去折腾而是用原生的markdown+本地图片写了一些笔记，这就是chengxiaowei_blog这个仓库的由来，其实这已经能满足基本需求了:远端同步+网页渲染，维持了相当一段时间的笔记需求 hugo+zzo主题 一次很偶然的机会，我在逛大佬私人博客的时候，瞥见一个不起眼的评论，是在询问这个博客是怎么搭建的，博主回答道用的hugo，我顿时来了兴趣。从此打开了新世界的大门。当时挑选主题也是挑选了很久，最终选择了 Zzo，就算现在来看该主题对代码的渲染是非常优秀的，至于为什么弃用，个人觉得略显臃肿。该主题的笔记位于joker_null的master分支 even主题 2022年3月21的东航坠机事件促使我寻找一种简约风格的皮肤主题来记录自己的感悟，编程，和生活，最终选择了even，这款皮肤也很棒，简约风格。美中不足的地方是代码的渲染有瑕疵golang的格式有时候会非常混乱，代码块没有粘贴功能，也尝试去修改样式，不过最终达不到效果放弃了。该主题位于joker_null的cat13分支 loveit 主题 这几天一直在寻找一款代码渲染非常棒并且简约风格的博客主题。在选了十几款皮肤后，最终选择了LoveIt，折腾了一下午把博客迁移进来，代码渲染的部分确实漂亮。主题位于joker_null的cat19分支 ","date":"2022-07-27","objectID":"/2022/07/home/change-theme/:0:0","tags":null,"title":"博客的变迁史","uri":"/2022/07/home/change-theme/"},{"categories":null,"content":"go 的异步抢占调度 go 1.13 在go 1.13 （包括1.13）之前，并不是真正意思上的抢占。什么是真正的抢占式调度，为了探究这个问题，网上广为流传这么一段代码 package main import \"fmt\" func main() { go func(i int) { for { i++ fmt.Println(i) } }(0) for { } } 在go 1.13中这段代码的表象是，程序会持续输出，过不了多久便停止了输出，但是程序并没有终止，通过top命令查看会发现程序的cpu负载很高，这是因为for{}导致，负责打印的goroutine应该是在某处\"卡住\"了。 ","date":"2022-07-25","objectID":"/2022/07/golang/goroutine-preemption/:0:0","tags":["golang"],"title":"go 异步抢占调度","uri":"/2022/07/golang/goroutine-preemption/"},{"categories":null,"content":"调试 本地高于go 1.13，可以用GODEBUG=asyncpreemptoff=1 go run main.go来调试。也可以在docker里面跑一个go 1.13的版本 docker 里面调试 $ docker pull golang:1.13 $ docker run -it golang:1.13 在容器里面，安装vim root@a2ea6ba48cc9:/go# apt-get update root@a2ea6ba48cc9:/go# apt-get install vim 还需要一个dlv的golang调试工具 go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.cn,direct go get github.com/go-delve/delve/cmd/dlv@latest 在容器里面复制上面代码 root@a2ea6ba48cc9:/go/src# vim main.go 运行 root@a2ea6ba48cc9:/go/src# go run main.go ","date":"2022-07-25","objectID":"/2022/07/golang/goroutine-preemption/:1:0","tags":["golang"],"title":"go 异步抢占调度","uri":"/2022/07/golang/goroutine-preemption/"},{"categories":null,"content":"doc docker attach docker attach容器的 ID 或名称将终端的标准输入、输出和错误（或三者的任意组合）附加到正在运行的容器 $ docker run -d --name topdemo ubuntu /usr/bin/top -t $ docker attach topdemo top - 13:13:27 up 8 days, 5:16, 0 users, load average: 0.04, 0.07, 0.03 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 7838.6 total, 2394.0 free, 1615.3 used, 3829.4 buff/cache MiB Swap: 2048.0 total, 2048.0 free, 0.0 used. 5758.8 avail Mem docker build docker run 运行容器的命令 ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:0:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"-d, –detach detach直译为脱离，分离模式启动容器，后台运行 下面例子只关心 -d这个参数 $ docker run --name some-nginx -d -p 8080:80 nginx ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:1:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"-a 可以指定三种标准输出 STDIN, STDOUT ,STDERR $ docker run -a stdin -a stdout -i -t ubuntu /bin/bash ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:2:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"-i -t –interactive Keep STDIN open even if not attached –tty Allocate a pseudo-TTY 交互模式，可以进入容器内部 $ docker run -i -t ubuntu /bin/bash $ echo test | docker run -i ubuntu cat ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:3:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"–name 给容器取名字，下面例子中some-nginx是这个容器的名字 $ docker run --name some-nginx -d -p 8080:80 nginx $ docker ps -f 'name=some-nginx' CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 028c1ac0cf08 nginx \"/docker-entrypoint.…\" 17 minutes ago Up 17 minutes 0.0.0.0:8080-\u003e80/tcp, :::8080-\u003e80/tcp some-nginx 唯一确定一个容器，有三种方式 Identifier type Example value UUID short identifier f78375b1c487 UUID long identifier f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778 Name myredis ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:4:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"–cidfile string docker 将容器 ID 写入指定的文件中。这类似于某些程序将其进程 ID (PID)写入文件 ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:5:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"–restart 重启策略 no :当容器退出后，不自动重启（默认） on-failure：[max-retries]:只有容器以非0的状态退出(异常退出)才重启，（可选）限制 Docker 守护程序尝试重新启动的次数 always：无论退出状态如何，始终重新启动容器。docker无限制地重启容器，容器也将始终在守护程序启动时启动 unless-stopped：无论退出状态如何，始终重新启动容器，包括docker的守护进程重启，除非容器自己处于stopped state $ docker run --name mysql -v ~/docker/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=12345678 -d -p 3307:3306 --restart=always mysql # 获取重启次数 $ docker inspect -f \"{{ .RestartCount }}\" my-container # 获取重启时间 $ docker inspect -f \"{{ .State.StartedAt }}\" my-container ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:6:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"–network 网络模式 ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:7:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"– link 容器互联 docker run --rm --name gameserver --link mysql-mysql-1:db --net mysql_default cat19/gameserver 参数--link name:alias name 是连接的容器名称，alias为别名 Note 如果连接的容器是docker-compose启动的，需要指定--net mysql_default ，否则会抛出错误 docker: Error response from daemon: Cannot link to /mysql-mysql-1, as it does not belong to the default network. ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:8:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"–add-host 向容器的/etc/hosts 添加映射 1221$ docker run --rm -it --add-host db-static:86.75.30.9 ubuntu cat /etc/hosts docker ps $ docker ps --help usage: docker ps [OPTIONS] List containers Options: -a, --all Show all containers (default shows just running) -f, --filter filter Filter output based on conditions provided --format string Pretty-print containers using a Go template -n, --last int Show n last created containers (includes all states) (default -1) -l, --latest Show the latest created container (includes all states) --no-trunc Don't truncate output -q, --quiet Only display container IDs -s, --size Display total file sizes ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:9:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"docker ps -a 退出的container也会展示 $ docker ps -a ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:9:1","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"docker ps -f $ docker ps -f 'name=redis' CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ab0141a96725 redis \"docker-entrypoint.s…\" About an hour ago Up About an hour 0.0.0.0:6379-\u003e6379/tcp, :::6379-\u003e6379/tcp myredis docker container docker logs ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:9:2","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"-f $ docker logs -f 028c1ac0cf08 172.17.0.1 - - [10/Jul/2022:13:34:59 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"curl/7.81.0\" \"-\" 192.168.0.103 - - [10/Jul/2022:13:35:41 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"Mozilla/5.0 docker search ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:9:3","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"–filter , -f 根据提供的条件过滤输出 格式 描述 .Name 镜像名字 .Description 镜像描述 .StarCount 镜像的点赞数 .IsOfficial OK:镜像是官方的 .IsAutomated OK:镜像构建是自动化的 $ docker search --format \"{{.Name}}: {{.StarCount}}\" nginx nginx: 17077 linuxserver/nginx: 169 bitnami/nginx: 135 ubuntu/nginx: 52 bitnami/nginx-ingress-controller: 19 rancher/nginx-ingress-controller: 10 clearlinux/nginx: 4 ibmcom/nginx-ingress-controller: 4 bitnami/nginx-ldap-auth-daemon: 3 bitnami/nginx-exporter: 2 rapidfort/nginx: 2 rancher/nginx: 2 rancher/nginx-ingress-controller-defaultbackend: 2 vmware/nginx: 2 docker compose 新版的Compose v2的命令已经 嵌入docker cli ，可以用docker compse 来代替 docker compse ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:9:4","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"-p 指定项目的名字 例如有以下docker-compose.yml services: MYSQLDB: image: mysql volumes: - db_data:/var/lib/mysql restart: always container_name: MYSQLDB command: [ 'mysqld', '--character-set-server=utf8mb4', '--collation-server=utf8mb4_unicode_ci','--default-time-zone=+08:00' ] ports: - 3309:3306 environment: - MYSQL_ROOT_PASSWORD=12345678 - TZ=Asia/Shanghai gameserver: image: cat19/gameserver ports: - 8840:8840 restart: always container_name: GameServer ulimits: nofile: soft: 65536 hard: 65536 links: - MYSQLDB:MysqlDb volumes: db_data $ docker compose -p gameserver up -d docker volume 备份 ","date":"2022-07-10","objectID":"/2022/07/docker/docker-cli/:10:0","tags":["docker"],"title":"docker cli","uri":"/2022/07/docker/docker-cli/"},{"categories":null,"content":"简介 部署redis 单节点 不要配置文件 ymal文件 version: '3.8' services: cache: image: redis restart: always container_name: myredis ports: - '6379:6379' command: redis-server --save 20 1 --loglevel warning --requirepass 12345 volumes: - cache:/data volumes: cache: driver: local 自定义配置文件 version: '3.8' services: cache: image: redis restart: always container_name: myredis ports: - '6379:6379' command: redis-server /usr/local/etc/redis/redis.conf volumes: - cache:/data - ./redis.conf:/usr/local/etc/redis/redis.conf volumes: cache: driver: local $ docker-compose -f docker-compose-redis.yml up -d ","date":"2022-07-10","objectID":"/2022/07/docker/docker-compose/:0:0","tags":["docker"],"title":"docker Compose","uri":"/2022/07/docker/docker-compose/"},{"categories":null,"content":"32位与64位 地址总线 CPU要想读取程序中数据，需要地址总线，通过MMU将虚拟地址转换成物理的内存地址。如果地址总线有8根，那么CPU的最大寻址空间是2^8 地址[0,255]，则表示最大可用的内存为256 byte;同理如果想要寻址4G的内存，则需要地址总线32根。 数据总线 数据总线是内存数据流向CPU的通道，对应的如果想要一次性把地址中的数据一起传输，就需要和地址总线对齐如：32位地址总线:32位数据总线，64位地址总线:64位数据总线。32位的一次性能操作4字节，64位的能一次性操作8字节长度。这里的字节长度其实就是机器字长 总结 所谓的32位操作系统和64位操作系统指的是数据总线能够一次性传输的位数，很显然64位的数据总线处理能力更高 内存布局 一块真实的物理内存条，有一面是几个黑色小方块组成。这个黑色的小方块就是chip，4G的内存条有4个chip，8G内存条有8个chip，而chip是由8个bank组成的三维结构 一次操作选中连续的8个字节 如果address不是8的倍数，就需要cpu取两次内存地址，再做拼接。例如:需要取1位后面8位的数据，则需要第一次从0开始取8个byte位，留下前面7个，第二次从8开始取8个byte位，留下前面一个，最终7+1得到想要的数据，但是必然有性能损耗。所以各个编程语言的编译器会把各式各样的数据类型放在合适的位置 内存对齐 根据上面的内容可以知道内存对齐的目的就是要满足一次性将数据取出，避免不必要的性能浪费。内存对齐的条件如下 类型的首地址是对齐边界的倍数 类型的字节长度是对齐边界的倍数 怎么确定每个数据类型的对齐边界呢？这和平台有关系，以golang为例，PtrSize指针宽度，RegSize寄存器宽度 Arch Name PtrSize RegSize 386 4 4 amd64 8 8 arm 4 4 arm64 8 8 以上称为平台对应的最大对齐边界，数据类型的对齐边界是取类型大小与平台最大对齐边界中较小的一个。例如（✅表示最小的一个） 64位平台下 类型 大小 regsize int8 1byte ✅ 8byte int16 2byte✅ 8byte int32 4byte✅ 8byte int64 8byte✅ 8byte string 16byte 8byte✅ slice 24byte 8byte✅ 32位平台下 类型 大小 regsize int8 1byte ✅ 4byte int16 2byte✅ 4byte int32 4byte✅ 4byte int64 8byte 4byte✅ string 8byte 4byte✅ slice 12byte 4byte✅ 结构的对齐边界 ","date":"2022-07-03","objectID":"/2022/07/op-system/memory-alignment/:0:0","tags":["操作系统"],"title":"内存对齐","uri":"/2022/07/op-system/memory-alignment/"},{"categories":null,"content":"KEY ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:0:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"DEL 删除key DEL key [key …] 示例 127.0.0.1:6379\u003e SET name joker OK 127.0.0.1:6379\u003e GET name joker 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e DEL name 1 127.0.0.1:6379\u003e GET name 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:1:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"DUMP ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:2:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"EXISTS 判断一个key是否存在 时间复杂度：O(1) 返回值： 若 key 存在，返回 1 ，否则返回 0 。 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e exists name 1 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e del name 1 127.0.0.1:6379\u003e exists name 0 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:3:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"EXPIRE EXPIRE key seconds 设置key的过期时间 以下几个操作不会改变key的过期时间 对一个key进行INCR,LPUSH,HSET等 RENAME，重命名某个key Note SET 同一个 key时会删除过期时间 127.0.0.1:6379\u003e set name joker OK 127.0.0.1:6379\u003e expire name 10000 1 127.0.0.1:6379\u003e ttl name 9997 127.0.0.1:6379\u003e set name top OK 127.0.0.1:6379\u003e ttl name -1 127.0.0.1:6379\u003e 另外如果想移除某个key的过期时间，可以用 PERSIST命令 时间复杂度 O(1) 127.0.0.1:6379\u003e get name haha 127.0.0.1:6379\u003e expire name 1000 1 127.0.0.1:6379\u003e ttl name 993 127.0.0.1:6379\u003e expire name 1000 1 127.0.0.1:6379\u003e ttl name 998 127.0.0.1:6379\u003e ttl name 976 127.0.0.1:6379\u003e PERSIST name 1 127.0.0.1:6379\u003e ttl name -1 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:4:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"EXPIREAT EXPIREAT key timestamp 从字面意思可知，设置过期日期 时间复杂度： O(1) // 1658997850 --\u003e2022-07-28 16:44:10 127.0.0.1:6379\u003e expireat name 1658997850 1 127.0.0.1:6379\u003e ttl name 7183 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:5:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"KEYS KEYS pattern KEYS * 匹配数据库中所有 key KEYS h?llo 匹配 hello ， hallo 和 hxllo 等。 KEYS h*llo 匹配 hllo 和 heeeeello 等。 KEYS h[ae]llo 匹配 hello 和 hallo ，但不匹配 hillo 。 Warning 虽然keys 的速度非常快，但是在存在大量key的数据库中尽量少用。原因之一是redis是单线程来处理逻辑，如果key的数量非常多就会很耗时，必定会影响其他的请求，此操作是O(n)的复杂度 127.0.0.1:6379\u003e MSET name1 joker name2 bob name3 haha OK 127.0.0.1:6379\u003e keys *am* name2 name1 name3 ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:6:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"MIGRATE MIGRATE host port key destination-db timeout [COPY] [REPLACE] [AUTH password] 这个命令是将指定的 key 原子性地从当前实例传送到目标实例的指定数据库上，一旦传送成功， key 保证会出现在目标实例上，而当前实例上的 key 会被删除。 Note 下面的例子中ip和密码已抹去真实性 127.0.0.1:6379\u003e get joker hahaha 127.0.0.1:6379\u003e MIGRATE 111.36.72.254 6375 AUTH 1234 joker 0 1000 REPLACE ERR syntax error 127.0.0.1:6379\u003e MIGRATE 111.36.72.254 6375 joker 0 1000 REPLACE ERR Target instance replied with error: NOAUTH Authentication required 127.0.0.1:6379\u003e MIGRATE 111.36.72.254 6375 joker 0 1000 REPLACE AUTH 1234 OK 111.36.72.254:6375\u003e get joker \"hahaha\" 127.0.0.1:6379\u003e get joker //本地已删除 该命令是原子操作，执行时会阻塞两个实例，直到以下任意结果发生：迁移成功，迁移失败，网络超时等。 命令的内部实现是这样的：在传输目标实例之前，会执行DUMP操作将本地指定的key序列化，然后在传送给目标实例，目标实例接收后执行RESTORE将数据反序列化，并添加到数据库中。如果可选参数是REPLACE会删除本地实例的key,如果是COPY则不会删除，但是如果目标实例有相同的key会发生错误ERR Target instance replied with error: BUSYKEY Target key name already exists. 127.0.0.1:6379\u003e MIGRATE 111.36.72.254 6375 joker 0 1000 COPY AUTH 1234 OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e get joker hahahhah 111.36.72.254:6375\u003e get joker \"hahahhah\" 127.0.0.1:6379\u003e MIGRATE 111.36.72.254 6375 joker 0 1000 COPY AUTH 1234 ERR Target instance replied with error: BUSYKEY Target key name already exists. Note MIGRATE 命令是原子操作，即使失败也不会丢key ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:7:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"MOVE MOVE key db 将指定key的移动到指定的数据库中 127.0.0.1:6379\u003e select 0 OK 127.0.0.1:6379\u003e get joker hahahhah 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e move joker 1 1 127.0.0.1:6379\u003e exists joker 0 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e select 1 OK 127.0.0.1:6379[1]\u003e get joker hahahhah 127.0.0.1:6379[1]\u003e 但是试图移动的key和目标数据库存在key相同，目标数据库的key对应的value并不会被覆盖 127.0.0.1:6379[1]\u003e select 0 OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e get joker 127.0.0.1:6379\u003e set joker jjjjj OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e move joker 1 0 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e get joker jjjjj 127.0.0.1:6379\u003e select 1 OK 127.0.0.1:6379[1]\u003e get joker hahahhah 127.0.0.1:6379[1]\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:8:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"OBJECT OBJECT subcommand [arguments [arguments]] OBJECT REFCOUNT key 返回给定 key 引用所储存的值的次数 OBJECT ENCODING key 储存的值所使用的内部表示 OBJECT IDLETIME key 自储存以来的空转时间 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e OBJECT REFCOUNT name 1 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e OBJECT ENCODING name embstr 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e OBJECT IDLETIME name 36 127.0.0.1:6379\u003e GET name haha 127.0.0.1:6379\u003e OBJECT IDLETIME name 2 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:9:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"PERSIST PERSIST key 移除给定 key 的生存时间 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e expire name 10000 1 127.0.0.1:6379\u003e ttl name 9997 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e PERSIST name 1 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e ttl name -1 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:10:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"PEXPIREAT PEXPIREAT key milliseconds-timestamp 以毫秒为单位设置过期时间 127.0.0.1:6379\u003e set name 123 OK 127.0.0.1:6379\u003e //1659166810000 --\u003e2022-07-30 15:40:10 127.0.0.1:6379\u003e PEXPIREAT name 1659166810000 1 127.0.0.1:6379\u003e ttl name 3547 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:11:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"PTTL PTTL key 以毫秒为单位返回 key 的剩余生存时间 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e expire name 10 1 127.0.0.1:6379\u003e pttl name 4075 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:12:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"RANDOMKEY 从当前数据库中随机返回一个key 127.0.0.1:6379\u003e MSET name haha name1 joker name2 jhsj name3 sasdad OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e RANDOMKEY name3 127.0.0.1:6379\u003e RANDOMKEY name2 127.0.0.1:6379\u003e RANDOMKEY name1 127.0.0.1:6379\u003e RANDOMKEY name3 ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:13:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"RENAME RENAME key newkey 将 key 改名为 newkey 可以看见rename不会删除原有过期时间 127.0.0.1:6379\u003e set name 123 OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e expire name 100 1 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e ttl name 92 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e rename name rename OK 127.0.0.1:6379\u003e ttl rename 77 如果已经存在新的key，则会覆盖 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e set name1 joker OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e rename name name1 OK 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e get name1 haha 127.0.0.1:6379\u003e get name 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:14:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"RENAMENX RENAMENX key newkey 当且仅当 newkey 不存在时，将 key 改名为 newkey 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e set name1 joker OK 127.0.0.1:6379\u003e RENAMENX name name1 0 127.0.0.1:6379\u003e MGET name name1 haha joker 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:15:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"RESTORE RESTORE key ttl serialized-value ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:16:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"SORT SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:17:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"TTL 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 返回值 当 key 不存在时，返回 -2 。 当 key 存在但没有设置剩余生存时间时，返回 -1 。 否则，以秒为单位，返回 key 的剩余生存时间。 127.0.0.1:6379\u003e set name haha OK 127.0.0.1:6379\u003e ttl name -1 127.0.0.1:6379\u003e ttl nam -2 127.0.0.1:6379\u003e expire name 100 1 127.0.0.1:6379\u003e ttl name 96 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:18:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"TYPE TYPE key 返回key的类型 none (key不存在) string (字符串) list (列表) set (集合) zset (有序集) hash (哈希表) ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:19:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"SCAN SCAN cursor [MATCH pattern] [COUNT count] SCAN是基于游标的迭代器。执行命令后都会返回一个新的游标，第二次SCAN根据新的游标进行迭代 SCAN从0开始，如果迭代已结束会返回0，如下所示 127.0.0.1:6379\u003e SCAN 0 1) \"0\" 2) 1) \"name1\" ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:20:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"MATCH 选项 过滤某种key 127.0.0.1:6375[5]\u003e SCAN 294913 MATCH first_pay* 1) \"16385\" 2) 1) \"first_pay_0455024283648\" 2) \"first_pay_4210839597056\" 3) \"first_pay_3615148937216\" ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:20:1","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"COUNT 选项 指定返回的个数 127.0.0.1:6375[5]\u003e SCAN 720896 COUNT 12 1) \"360448\" 2) 1) \"mobile_5347552301056\" 2) \"first_pay_7677150433280\" 3) \"firstPay:1455201652736\" 4) \"first_pay_7533806170112\" 5) \"firstPay:2564312084480\" 6) \"first_pay_8645317152768\" 7) \"first_pay_9162111070208\" 8) \"first_pay_8233627316224\" 9) \"first_pay_1507511316480\" 10) \"first_pay_9747146321920\" string ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:20:2","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"APPEND APPEND key value 如果key存在并且是字符串，APPEND将value追加到原来的value中 如果key不存在，就直接设置value,和 SET key value 一样 127.0.0.1:6379\u003e EXISTS name (integer) 0 127.0.0.1:6379\u003e APPEND name haha (integer) 4 127.0.0.1:6379\u003e get name \"haha\" 127.0.0.1:6379\u003e APPEND name 'love you' (integer) 12 127.0.0.1:6379\u003e get nmae (nil) 127.0.0.1:6379\u003e get name \"hahalove you\" 返回的是追加后的字符个数 ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:21:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"BITCOUNT 计算字符串中，被设置为1的比特位数量。可以指定start 或 end 参数 例如 127.0.0.1:6379\u003e SETBIT haha 0 1 (integer) 0 127.0.0.1:6379\u003e SETBIT haha 1 1 (integer) 0 127.0.0.1:6379\u003e SETBIT haha 2 1 (integer) 0 127.0.0.1:6379\u003e SETBIT haha 3 0 (integer) 0 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e 127.0.0.1:6379\u003e BITCOUNT haha 0 -1 (integer) 3 127.0.0.1:6379\u003e BITCOUNT haha 0 -1 -1表示最后一个，也可以不指定start end，表示全部 如图 HASH ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:22:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"HSET HSET key field value 返回值： 如果 field 是哈希表中的一个新建域，并且值设置成功，返回 1 。 如果哈希表中域 field 已经存在且旧值已被新值覆盖，返回 0 。 127.0.0.1:6379\u003e HSET website google \"www.g.cn\" (integer) 1 127.0.0.1:6379\u003e HSET website google \"www.google.com\" (integer) 0 127.0.0.1:6379\u003e ","date":"2022-06-27","objectID":"/2022/06/redis/redis-cli/:23:0","tags":["redis"],"title":"redis cli","uri":"/2022/06/redis/redis-cli/"},{"categories":null,"content":"原理 map是最常见的数据结构之一，在不同的编程语言中的申明和用法或多或少有点不同，但在底层原理上他们是一样的。数组访问某个元素是通过下标来获取，时间复杂度为O(1)。而map可以让key通过hash函数映射成类似数组下标，从而达到key具有索引功能。map查找某个value，理论上是O(1)的复杂度。一个性能优秀的map有两个关键因素—–哈希函数和解决冲突的方法 一个完美的hash的函数，必然是将不同key映射到不同的索引上。但世界上并不存在这种完美的hash函数，因为输入的范围远远大于输出范围。md5是一种将二进制（binary）映射到32位的字符串中（128bit）,可以表示2^128种可能。但是输入范围是无穷大的，必然存在某两种二进制的md5一模一样，这种现象就是hash碰撞 good hash bad hash 碰撞解决 hash碰撞会产生二义性，解决的办法一般是开放寻址法和拉链法 ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:0:0","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"开放寻址法 其思想非常简单：底层有个数组array用于存放键值对，key通过某种hash函数映射成hash数值并与数组的长度取模（确保索引不可越界），当经过上述操作过后会得到一个index，用index取出value并和目标value做对比，如果冲突了会向后寻找下一个空闲的位置 index:=hash(\"love\") % len(array) 如图所示：写入数据的时候，当key3的和key1，key2发生冲突，key3会被写入key2的后面空闲的地方。读数据的时候，根据hash函数得到hash值再与数组长度取模得到index，发现已经被占领了，这时候继续比较后移，直到数组的末尾或者找到目标元素 ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:1:0","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"装载因子 所谓的装载因子是指：元素个数与数组的大小的比值，这个比值越大其性能越差，平均比较次数会随着装载因子的增大而线性增加，当达到100%时，map会退化成线性表，O(n)的复杂度。 ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:1:1","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"拉链法 相比于开放寻址法，拉链法是常见的实现方式。实现方式是底层用数组+链表，也有的用的数组+红黑树比如java。 如图所示： key6和value6写入过程，先通过hash函数得到一个hash值然后与数组的长度取模 index := hash(\"Key6\") % len(array) index比如为2，选择了2号桶后，就遍历后面的链表 找到相同的键值的键值对-更新对应的值 没有找到相同键值的键值对-在链表后插入新的键值对 key7的读取过程，经过hash过后命中了1号桶，这时候会遍历链表并进行键的比较，正好查找到key7位于链表的第二个位置 map 数据结构 ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:2:0","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"hmap // /usr/local/go/src/runtime/map.go type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/reflectdata/reflect.go. // Make sure this stays in sync with the compiler's definition. count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details hash0 uint32 // hash seed buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) extra *mapextra // optional fields } mapextra type mapextra struct { // If both key and elem do not contain pointers and are inline, then we mark bucket // type as containing no pointers. This avoids scanning such maps. // However, bmap.overflow is a pointer. In order to keep overflow buckets // alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow. // overflow and oldoverflow are only used if key and elem do not contain pointers. // overflow contains overflow buckets for hmap.buckets. // oldoverflow contains overflow buckets for hmap.oldbuckets. // The indirection allows to store a pointer to the slice in hiter. overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap } count map中元素个数 B 可以推出buckets的数量，len(buckets) == 2^B hash0 随机数种子 oldbuckets 用于map扩容时存储之前的buckets，它的大小是当前buckets的一半 如图所示，bucket的基本单位是bmap，如果单个桶已经装满，这个时候会使用溢出桶，蓝色的是溢出桶，绿色的是正常桶，两个类型的桶在内存空间是连续的 ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:3:0","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"bmap bmap的结构长这个样子，一个bmap可以存储8个键值对，overflow指向溢出的下一个bmap，topbits存储了键的哈希的高8位以此减少访问键值对的次数 type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } map 读操作 map有两种读的方式 v:=hash[key] v,ok:=hash[key] 对应的源码 // /usr/local/go/src/runtime/map.go func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if raceenabled \u0026\u0026 h != nil { callerpc := getcallerpc() pc := abi.FuncPCABIInternal(mapaccess1) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026\u0026 h != nil { msanread(key, t.key.size) } if asanenabled \u0026\u0026 h != nil { asanread(key, t.key.size) } if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 } return unsafe.Pointer(\u0026zeroVal[0]) } if h.flags\u0026hashWriting != 0 { throw(\"concurrent map read and map write\") } hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash\u0026m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { // There used to be half as many buckets; mask down one more power of two. m \u003e\u003e= 1 } oldb := (*bmap)(add(c, (hash\u0026m)*uintptr(t.bucketsize))) if !evacuated(oldb) { b = oldb } } top := tophash(hash) bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u003c bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } if t.key.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e } } } return unsafe.Pointer(\u0026zeroVal[0]) } 以key，hash0为参数，算出key的hash hash := t.hasher(key, uintptr(h.hash0)) 根据hash和bucketMask计算落入哪个桶，add是指针位移操作 m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash\u0026m)*uintptr(t.bucketsize))) 计算hash的高8位，用于快速比较 top := tophash(hash) bucketloop 是查找键值对的主逻辑 bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u003c bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } if t.key.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e } } } 两层循环，第一层遍历桶的溢出桶，第二层遍历桶的8个tophash 通过比较目标topshash和桶内的tophash快速筛选 命中某个tophash后，通过偏移量取出对应的key： k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) 比较key与目标key是否相等，如果相等根据偏移量取出对应的value，如果不相等则进入下次循环，直到跳出循环 e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) 跳出循环，返回对应类型的零值，表示key不存在 return unsafe.Pointer(\u0026zeroVal[0]) 对于有bool值的读取方式，对应的源码，只是在之前的基础上加上了bool返回值，其他毫无差别。这种有bool返回值的取值方式可以更加精准的判断目标key是否是真的存在，如果仅仅判断零值有可能产生二义性：key本身存在，恰好值是零值 // /usr/local/go/src/runtime/map.go func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) { if raceenabled \u0026\u0026 h != nil { callerpc := getcallerpc() pc := abi.FuncPCABIInternal(mapaccess2) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026\u0026 h != nil { msanread(key, t.key.size) } if asanenabled \u0026\u0026 h != nil { asanread(key, t.key.size) } if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 } return unsafe.Pointer(\u0026zeroVal[0]), false } if h.flags\u0026hashWriting != 0 { throw(\"concurrent map read and map write\") } hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash\u0026m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { // There used to be half as many buckets; mask down one more power of two. m \u003e\u003e= 1 } oldb := (*bmap)(add(c, (hash\u0026m)*uintptr(t.bucketsize))) if !evacuated(oldb) { b = oldb } } top := tophash(hash) bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:4:0","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"内存泄露 go 的map随着长时间的系统运行，其占用的内存只增不减。其原因是因为在map中的bucket会随着容量不足进行扩容，但不会因为delete操作而缩小容量，而本身的bucket需要占用内存的 假设初始化100万元素的map，元素全部删除后，看看最后占多少内存。装载因子(loadfactor)最大为6.5，则B的值为18 计算装载因子 fmt.Println(math.Ceil(math.Log2(float64(1000000) / 6.5))) package main import ( \"fmt\" \"runtime\" ) const N = 128 func randBytes() [N]byte { return [N]byte{} } func printAlloc() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) fmt.Printf(\"%d MB\\n\", m.Alloc/1024/1024) } func main() { n := 1_000_000 m := make(map[int][N]byte, 0) printAlloc() for i := 0; i \u003c n; i++ { m[i] = randBytes() } printAlloc() for i := 0; i \u003c n; i++ { delete(m, i) } runtime.GC() printAlloc() // 避免m被GC掉 runtime.KeepAlive(m) } 跑出来的结果 ▶ ./t 0 MB 461 MB 293 MB 可以看见当所有元素被删除后，依然占用293 MB的内存，这实际上是map本身的内存。 啥元素都没有的map package main import ( \"fmt\" \"runtime\" ) const N = 128 func printAlloc() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) fmt.Printf(\"%d MB\\n\", m.Alloc/1024/1024) } func main() { n := 1_000_000 m := make(map[int][N]byte, n) printAlloc() runtime.KeepAlive(m) } 结果 ▶ go run main.go 293 MB 但是为啥在写入100w个元素后的map占用的内存为461 MB，这是因为在写入过程中扩容了。当value的大小 \u003c= 128时，value是直接存入bucket中的。如果初始化map的时候，指定容量大小，写入后的内存与初始化的内存一样 package main import ( \"fmt\" \"runtime\" ) const N = 128 func randBytes() [N]byte { return [N]byte{} } func printAlloc() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) fmt.Printf(\"%d MB\\n\", m.Alloc/1024/1024) } func main() { n := 1_000_000 // 指定初始化容量 m := make(map[int][N]byte, n) printAlloc() for i := 0; i \u003c n; i++ { m[i] = randBytes() } printAlloc() for i := 0; i \u003c n; i++ { delete(m, i) } runtime.GC() printAlloc() // 避免m被GC掉 runtime.KeepAlive(m) } 结果 ▶ go run main.go 293 MB 293 MB 293 MB 当 value 大小超过 128字节 后，bucket 不会直接放 value，转而变成一个指针。将 N 设为 129，结果： 0 MB 197 MB 38 MB 这和把value改成指针类型一样 func randBytes() *[N]byte { return \u0026[N]byte{} } m := make(map[int]*[N]byte, 0) for i := 0; i \u003c n; i++ { m[i] = randBytes() } 小结： map本身的bucket数量只增不减，bucket本身会占有部分内存，是\"内存泄露\"的根本原因 当value小于128字节，尽量用指针类型进行map的存储，尤其是大容量map，极其重要 和slice一样尽量指定map的初始容量，可以避免扩容带来的性能损耗和减少内存占有量 ","date":"2022-06-25","objectID":"/2022/06/golang/go-map/:5:0","tags":["golang"],"title":"go map","uri":"/2022/06/golang/go-map/"},{"categories":null,"content":"channel channel被设计用来goroutine之间通讯的。go在共享内存的设计理念是：不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存。 数据结构与原理 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } 字段 含义 buf 指向缓存区的指针 qcount 缓存区已有的元素个数 dataqsize 缓存区数组容量 elemsize 元素大小 elemtype 元素类型 sendx ch\u003c-x ,goroutine发送数据到缓存区，填充数据的下标，写操作 recvx x:=\u003c-ch,goroutine向channel读数据的下标，读操作 recvq 消费者阻塞队列 sendq 生产者阻塞队列 lock 锁，用于维持缓存区数据的完整性 closed c.closed != 0 表示channel已关闭 channel分为有缓存和无缓存两种。对于有缓存来说，需要缓存区来存储数据，缓存区是一个环形数组，sendx，recvx下标超过或等于数组长度会被置成0。 // /usr/local/go/src/runtime/chan.go func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { ..... if c.recvx == c.dataqsiz { c.recvx = 0 } ..... } ","date":"2022-06-24","objectID":"/2022/06/golang/go-channel/:0:0","tags":["go原理"],"title":"go channel","uri":"/2022/06/golang/go-channel/"},{"categories":null,"content":"有缓存channel 当有goroutine向channel发送数据时 如果这时候再向channel发送一个数据，recvx=sendx表示为缓存区为空或者已满 而判断channel的缓存区是否已满，源码是这样判断的 // /usr/local/go/src/runtime/chan.go func full(c *hchan) bool { // c.dataqsiz is immutable (never written after the channel is created) // so it is safe to read at any time during channel operation. if c.dataqsiz == 0 { // Assumes that a pointer read is relaxed-atomic. return c.recvq.first == nil } // Assumes that a uint read is relaxed-atomic. return c.qcount == c.dataqsiz } 至于缓存区满了之后需不需要阻塞是由block参数决定的 // /usr/local/go/src/runtime/chan.go func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { .... if !block \u0026\u0026 c.closed == 0 \u0026\u0026 full(c) { return false } .... } 怎么样才能让channel无阻塞呢，需要select语句中default关键字 func main() { ch := make(chan int, 10) go func() { for i := 0; i \u003c 10; i++ { ch \u003c- i } select { case ch \u003c- 1: default: fmt.Println(\"no block\") } }() time.Sleep(time.Minute) } 所谓的阻塞，是阻塞当前goroutine,不管是从channel中取数据，还是从channel发送数据，都有可能阻塞。所以channel的数据结构里面有两个等待队列：recvq,sendq。用来记录阻塞的goroutine。等待队列是sudog类型的双向列表，sudog记录了哪个goroutine在等待，等待哪个channel // /usr/local/go/src/runtime/chan.go type waitq struct { first *sudog last *sudog } type sudog struct { g *g next *sudog prev *sudog elem unsafe.Pointer acquiretime int64 releasetime int64 ticket uint32 isSelect bool success bool parent *sudog // semaRoot binary tree waitlink *sudog // g.waiting list or semaRoot waittail *sudog // semaRoot c *hchan // channel } 当满足条件时，要立刻唤醒等待的goroutine 字段 含义 g 等待的goroutine elem 元素的地址 当有goroutine读走一个数据的时候：v:=\u003c-ch ","date":"2022-06-24","objectID":"/2022/06/golang/go-channel/:1:0","tags":["go原理"],"title":"go channel","uri":"/2022/06/golang/go-channel/"},{"categories":null,"content":"无缓冲channel 这些字段都是零值 字段 值 buf nil qcount 0 dataqsize 0 sendx 0 recvx 0 工作的只有recvq,sendq，当有goroutine发送数据时，会直接把数据指针赋值给recvq的第一个元素，如果recvq为空，阻塞当前goroutine，并加入到sendq；当有goroutine接收数据时，会直接从sendq中取出队首元素的数据 ","date":"2022-06-24","objectID":"/2022/06/golang/go-channel/:2:0","tags":["go原理"],"title":"go channel","uri":"/2022/06/golang/go-channel/"},{"categories":null,"content":"无数据channel 有时候遇到的场景是，goroutine之间的通讯不涉及数据，但是需要有触发时机。这个时候，channel可以定义无数据的channel ：ch:=make(chan struct{},5)，此时buf字段是nil，其他的字段正常 ","date":"2022-06-24","objectID":"/2022/06/golang/go-channel/:3:0","tags":["go原理"],"title":"go channel","uri":"/2022/06/golang/go-channel/"},{"categories":null,"content":"defer defer 一词是延期，推迟的意思。在go中经常被用于关闭文件描述符、关闭数据库连接以及解锁资源。一般操作是紧挨着申请资源的下一句声明例如: func OpenFile() error { file, err := os.OpenFile(\"/var/log/go_log/test.log\", os.O_APPEND|os.O_CREATE|os.O_RDWR, 0777) if err != nil { return err } defer file.Close() .... } 可以看出defer用于一些扫尾工作，在函数OpenFile() 退出前，会执行file.Close()，从代码简洁的角度，defer比手动释放资源更加优雅 func OpenFile() error { file, err := os.OpenFile(\"/var/log/go_log/test.log\", os.O_APPEND|os.O_CREATE|os.O_RDWR, 0777) if err != nil { return err } if err := do1(); err != nil { file.Close() return err } if err := do2(); err != nil { file.Close() return err } .... } 现象 函数中多次调用defer，函数返回之前，defer的调用顺序是倒序调用，满足先入后出的栈原理 package main import \"fmt\" func main() { LoopDefer() } func LoopDefer() { for i := 0; i \u003c 5; i++ { defer fmt.Println(i) } } /** ▶ go run main.go 4 3 2 1 0 */ defer 传入的是一个带有参数的函数，会预先计算参数。 package main import \"fmt\" func main() { var i int defer fmt.Println(i + Add(Add(1, 1), Add(2, 2))) i++ } func Add(i, j int) int { return i + j } /** ▶ go run main.go 6 */ 上面的例子足以说明一切，在真正的调用fmt.Println(...)之前，已经把i + Add(Add(1, 1), Add(2, 2))算好了(6)，如果用匿名函数包裹一下，情况就不一样。 package main import \"fmt\" func main() { var i int defer func() { fmt.Println(i + Add(Add(1, 1), Add(2, 2))) }() i++ } func Add(i, j int) int { return i + j } /** ▶ go run main.go 7 */ 匿名函数包裹后，i + Add(Add(1, 1), Add(2, 2))，是延迟调用的，在i++后执行 ","date":"2022-06-22","objectID":"/2022/06/golang/go-defer/:0:0","tags":["go原理"],"title":"go defer","uri":"/2022/06/golang/go-defer/"},{"categories":null,"content":"数据结构 以上的一些defer特性，和它的数据结构和实现原理有关，_defer type _defer struct { siz int32 //参数和结果的内存大小 started bool heap bool // openDefer indicates that this _defer is for a frame with open-coded // defers. We have only one defer record for the entire frame (which may // currently have 0, 1, or more defers active). openDefer bool // 表示当前 defer 是否经过开放编码的优化 sp uintptr // 栈指针 pc uintptr // 调用方的程序计数器 fn *funcval // 传入的函数 _panic *_panic // 触发延迟调用的结构体，可能为空 link *_defer // 延迟调用链表 /* _defer---\u003e_defer---\u003e_defer---\u003e_defer */ // If openDefer is true, the fields below record values about the stack // frame and associated function that has the open-coded defer(s). sp // above will be the sp for the frame, and pc will be address of the // deferreturn call in the function. fd unsafe.Pointer // funcdata for the function associated with the frame varp uintptr // value of varp for the stack frame // framepc is the current pc associated with the stack frame. Together, // with sp above (which is the sp associated with the stack frame), // framepc/sp can be used as pc/sp pair to continue a stack trace via // gentraceback(). framepc uintptr } ","date":"2022-06-22","objectID":"/2022/06/golang/go-defer/:1:0","tags":["go原理"],"title":"go defer","uri":"/2022/06/golang/go-defer/"},{"categories":null,"content":"环境变量 总的来说，环境变量分为系统级和用户级 可以通过 set 来查看系统的环境变量，source ? 来更新环境变量(source /etc/profile) 系统级环境变量:每个登录到系统的用户都能够读取到的系统环境变量 用户级环境变量:每个登录到系统的用户只能读取属于自己的用户级别的环境变量 ","date":"2022-06-09","objectID":"/2022/06/linux/etc/:0:0","tags":["linux"],"title":"linux etc","uri":"/2022/06/linux/etc/"},{"categories":null,"content":"系统级 /etc/profile #环境变量配置 在系统启动后第一个用户登录时运行，并且会运行 /etc/profile.d 所有的环境变量shell，变量将应用于登录到系统的所有的用户，官方有句话 # It's NOT a good idea to change this file unless you know what you # are doing. It's much better to create a custom.sh shell script in # /etc/profile.d/ to make custom changes to your environment, as this # will prevent the need for merging in future updates. 翻译过来就是在说，要设置环境变量，推荐在/etc/profile.d/ 创建一个shell脚本，而不是直接修改/etc/profile #shell 脚本设置环境变量待补充 /etc/bashrc #ubuntu和debian 中是 /etc/bash.bashrc 在bash shell打开时运行，修改改文件配置的环境变量将会影响所有的用户使用bash shell 同样的官方提示 # System wide functions and aliases # Environment stuff goes in /etc/profile # It's NOT a good idea to change this file unless you know what you # are doing. It's much better to create a custom.sh shell script in # /etc/profile.d/ to make custom changes to your environment, as this # will prevent the need for merging in future updates. ","date":"2022-06-09","objectID":"/2022/06/linux/etc/:0:1","tags":["linux"],"title":"linux etc","uri":"/2022/06/linux/etc/"},{"categories":null,"content":"用户级 ~/.profile #推荐使用 当用户登录执行，每个用户都可以使用该文件来配置专属自己的的shell信息 ~/.bashrc 当用户登录时以及每次打开新的shell时改文件被读取 ~/.bash_profile 或 ~./bash_login ubuntu 如果有其中的一个文件存在的话, 当启动的是一个 登录shell时，Bash 会执行该文件而不会执行~/.profile ； 如果两个文件都存在的话，Bash 将会优先执行~/.bash_profile 而不是~/.bash_login ； 然而, 默认情况下，这些文件不会影响图形会话 ~/.bash_logout 当每次退出系统(退出bash shell)时执行该文件 ","date":"2022-06-09","objectID":"/2022/06/linux/etc/:0:2","tags":["linux"],"title":"linux etc","uri":"/2022/06/linux/etc/"},{"categories":null,"content":"总结 执行顺序是 /etc/profile ~/.bash_profile | ~/.bash_login | ~/.profile ~/.bashrc /etc/bashrc ~/.bash_logout 查看常用的系统配置 ","date":"2022-06-09","objectID":"/2022/06/linux/etc/:0:3","tags":["linux"],"title":"linux etc","uri":"/2022/06/linux/etc/"},{"categories":null,"content":"查看机器配置 查看内存容量 cat /proc/meminfo | grep MemTotal 查看cpu型号 cat /proc/cpuinfo | grep 'model name' |uniq 查看cpu个数 cat /proc/cpuinfo | grep \"physical id\" | uniq | wc -l # 查看内核/操作系统/CPU信息的linux系统信息 uname -a # 查看操作系统版本 head -n l /etc/issue # 查看CPU信息 cat /proc/cpuinfo # 查看计算机名的linux系统信息命令 hostname # 列出所有PCI设备 lspci -tv # 列出所有USB设备的linux系统信息命令 lsusb -tv # 列出加载的内核模块 lsmod # 查看环境变量资源 env # 查看内存使用量和交换区使用量 free -m # 查看各分区使用情况 df -h # 查看指定目录的大小 du -sh # 查看内存总量 grep MemTotal /proc/meminfo # 查看空闲内存量 grep MemFree /proc/meminfo # 查看系统运行时间、用户数、负载 uptime # 查看系统负载磁盘和分区 cat /proc/loadavg # 查看挂接的分区状态 mount | column -t # 查看所有分区 fdisk -l # 查看所有交换分区 swapon -s # 查看磁盘参数(仅适用于IDE设备) hdparm -i /dev/hda # 查看启动时IDE设备检测状况网络 dmesg | grep IDE # 查看所有网络接口的属性 ifconfig # 查看防火墙设置 iptables -L # 查看路由表 route -n # 查看所有监听端口 netstat -lntp # 查看所有已经建立的连接 netstat -antp # 查看网络统计信息进程 netstat -s # 查看所有进程 ps -ef # 实时显示进程状态用户 top # 查看活动用户 w # 查看指定用户信息 id # 查看用户登录日志 last # 查看系统所有用户 cut -d: -f1 /etc/passwd # 查看系统所有组 cut -d: -f1 /etc/group # 查看当前用户的计划任务服务 crontab -l # 列出所有系统服务 chkconfig –list # 列出所有启动的系统服务程序 chkconfig –list | grep on # 查看所有安装的软件包 rpm -qa #查看CPU相关参数的linux系统命令 cat /proc/cpuinfo #查看linux硬盘和分区信息的系统信息命令 cat /proc/partitions #查看linux系统内存信息的linux系统命令 cat /proc/meminfo #查看版本，类似uname -r cat /proc/version #查看设备io端口 cat /proc/ioports #查看中断 cat /proc/interrupts #查看pci设备的信息 cat /proc/pci #查看所有swap分区的信息 cat /proc/swaps fdisk -l | grep Disk ubuntu server 设置静态IP gateway4 has been deprecated, use default routes instead 老版本的gateway4不支持了，采用下面方式 routes: - to: default via: 192.168.1.1 vim /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: enp3s0f1: dhcp4: no addresses: [192.168.1.7/24] routes: - to: default via: 192.168.1.1 nameservers: addresses: [8.8.8.8,8.8.4.4] version: 2 addresses 设置静态IP和网段 routes 是网关 dhcp4: no 不采用动态获取IP ","date":"2022-06-09","objectID":"/2022/06/linux/etc/:1:0","tags":["linux"],"title":"linux etc","uri":"/2022/06/linux/etc/"},{"categories":null,"content":"一款开源的系统监控 官网 一款开源的系统监控 https://prometheus.io/ https://github.com/prometheus/prometheus ","date":"2022-06-09","objectID":"/2022/06/prometheus/guide/:0:0","tags":["prometheus"],"title":"prometheus guide","uri":"/2022/06/prometheus/guide/"},{"categories":null,"content":"网站 官网 文档 Mac 卸载 # 1 $ rm -rf ~/Library/Application\\ Support/Code # 2 $ rm -rf ~/.vscode # 3 $ cd /Applications $ open . # 删除vscode golang开发环境 Go for Visual Studio Code Atom One Dark Theme command+shift+p Go:Install 安装二进制插件 ","date":"2022-05-14","objectID":"/2022/05/software/vscode/:0:0","tags":["软件"],"title":"vscode","uri":"/2022/05/software/vscode/"},{"categories":null,"content":"准备 一块大于8G的U盘 制作启动盘工具 下下来长这样 最后u盘制作成了启动盘 安装 重启电脑，战神进入Logo时狂按F2和F7进入BIOS，进入BIOS后用U盘来boot。 接下来跟着步骤走就行 分区 分区这一块划重点，删掉所有的原来的分区，留下两个未分区的物理盘(机械+固态)，新建主分区，用固态盘作为系统盘(c盘)，接下来一定要选中刚刚建好的主分区，开始写入操作系统数据。 接下来就是熟悉的配置系统步骤了。 最后 装完系统后，此电脑中只有c盘的盘符，需要你手动创建其他的盘符 ","date":"2022-05-08","objectID":"/2022/05/software/install_windows/:0:0","tags":["软件"],"title":"windows install","uri":"/2022/05/software/install_windows/"},{"categories":null,"content":"sshd 启动 $ sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist 停止 $ sudo launchctl unload -w /System/Library/LaunchDaemons/ssh.plist 查看是否启动成功 $ sudo launchctl list | grep ssh 说明成功 # 0 com.openssh.sshd 配置vim 语法高亮 cp /usr/share/vim/vimrc ~/.vimrc vim ~/.vimrc ##添加以下内容 syntax on set nu! set autoindent ","date":"2022-05-02","objectID":"/2022/05/mac/mac-cli/:0:0","tags":["mac"],"title":"mac command","uri":"/2022/05/mac/mac-cli/"},{"categories":null,"content":"本篇是按照官方文档走一遍 概述 docker是一个用于开发，部署，运行的开放平台，可以为开发者虚拟出独立的沙箱环境用于程序运行。1 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:0:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Docker的架构 docker是典型的的C/S架构 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:1:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Docker daemon docker后台进程(dockerd)，接收docker命令，管理images，containers,networks,volumes 。dockerd和dockerd之间也可有通信。 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:2:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Docker client docker的客户端，是用户与dockerd的交互方式。例如 docker run -d -p 3001:3001 getting-started，客户端会将这些命令发送到dockerd，由dockerd执行。docker 客户端可以与多个dockerd通信 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:3:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Docker Desktop 集成了dockerd,docker,docker compose,docker content trust,kubernetes ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:4:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Docker registries 存储大量image的地方 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:5:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Images docker镜像 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:6:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"Containers 容器，是镜像的实例 CLI ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:7:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"docker version 参数 例如 描述 –format ,-f docker version 格式化输出版本信息 例如 docker version Client: Cloud integration: v1.0.23 Version: 20.10.14 API version: 1.41 Go version: go1.16.15 Git commit: a224086 Built: Thu Mar 24 01:49:20 2022 OS/Arch: darwin/amd64 Context: default Experimental: true Server: Docker Desktop 4.7.1 (77678) Engine: Version: 20.10.14 API version: 1.41 (minimum version 1.12) Go version: go1.16.15 Git commit: 87a90dc Built: Thu Mar 24 01:46:14 2022 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.5.11 GitCommit: 3df54a852345ae127d1fa3092b95168e4a88e2f8 runc: Version: 1.0.3 GitCommit: v1.0.3-0-gf46b6ba docker-init: Version: 0.19.0 GitCommit: de40ad0 docker version -f '{{json .}}' { \"Client\": { \"Platform\": { \"Name\": \"\" }, \"CloudIntegration\": \"v1.0.23\", \"Version\": \"20.10.14\", \"ApiVersion\": \"1.41\", \"DefaultAPIVersion\": \"1.41\", \"GitCommit\": \"a224086\", \"GoVersion\": \"go1.16.15\", \"Os\": \"darwin\", \"Arch\": \"amd64\", \"BuildTime\": \"Thu Mar 24 01:49:20 2022\", \"Context\": \"default\", \"Experimental\": true }, \"Server\": { \"Platform\": { \"Name\": \"Docker Desktop 4.7.1 (77678)\" }, \"Components\": [ { \"Name\": \"Engine\", \"Version\": \"20.10.14\", \"Details\": { \"ApiVersion\": \"1.41\", \"Arch\": \"amd64\", \"BuildTime\": \"Thu Mar 24 01:46:14 2022\", \"Experimental\": \"false\", \"GitCommit\": \"87a90dc\", \"GoVersion\": \"go1.16.15\", \"KernelVersion\": \"5.10.104-linuxkit\", \"MinAPIVersion\": \"1.12\", \"Os\": \"linux\" } }, { \"Name\": \"containerd\", \"Version\": \"1.5.11\", \"Details\": { \"GitCommit\": \"3df54a852345ae127d1fa3092b95168e4a88e2f8\" } }, { \"Name\": \"runc\", \"Version\": \"1.0.3\", \"Details\": { \"GitCommit\": \"v1.0.3-0-gf46b6ba\" } }, { \"Name\": \"docker-init\", \"Version\": \"0.19.0\", \"Details\": { \"GitCommit\": \"de40ad0\" } } ], \"Version\": \"20.10.14\", \"ApiVersion\": \"1.41\", \"MinAPIVersion\": \"1.12\", \"GitCommit\": \"87a90dc\", \"GoVersion\": \"go1.16.15\", \"Os\": \"linux\", \"Arch\": \"amd64\", \"KernelVersion\": \"5.10.104-linuxkit\", \"BuildTime\": \"2022-03-24T01:46:14.000000000+00:00\" } } docker version --format '{{.Server.Version}}' 20.10.14 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:8:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"docker run 例子 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"docker run --name test -it debian $ docker run --name test -it debian Unable to find image 'debian:latest' locally latest: Pulling from library/debian 6aefca2dc61d: Already exists Digest: sha256:6846593d7d8613e5dcc68c8f7d8b8e3179c7f3397b84a47c5b2ce989ef1075a0 Status: Downloaded newer image for debian:latest exit 13 echo $? 13 $ docker ps -a | grep test a30b8390fce8 debian \"bash\" 2 minutes ago Exited (13) About a minute ago 如果本地没有debian镜像，则去拉取最新的debian:latest ,-i 是交互模式，-t后台运行容器，–name取名字，echo $?：打印出刚才退出去的进程退出码 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:1","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"docker run --cidfile /tmp/docker_test.cid ubuntu echo \"test\" $ docker run --cidfile /tmp/docker_test.cid ubuntu echo \"test\" test 创建一个容器并打印test到控制台。cidfile 尝试创建一个新文件并将容器 ID 写入其中，如果文件已经存在返回错误，命令结束时关闭此文件。 在宿主机上 $ cat /tmp/docker_test.cid 759876e15d829ae04bd91a81410959be8630efcbb68a29dba344d105e87522d3 ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:2","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"–privileged 赋予最高权限 $ docker run -t -i --rm ubuntu bash root@e700a5cb0af4:/# root@e700a5cb0af4:/# mount -t tmpfs none /mnt mount: /mnt: permission denied. $ docker run -t -i --privileged ubuntu bash root@f66cf697d0d1:/# root@f66cf697d0d1:/# mount -t tmpfs none /mnt root@f66cf697d0d1:/# 参数–privileged，可以越过很多限制，意味着容器几乎可以做主机可以做的所有事情。该参数用于特殊用例，例如在 Docker 中运行 Docker ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:3","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"设置工作目录（-w） $ docker run -w /path/to/dir/ -i -t ubuntu pwd /path/to/dir -w让命令在给定的目录中执行， 这里是/path/to/dir/。如果路径不存在，则在容器内创建。 $ docker run -w /path/to/dir/ -i -t ubuntu root@83958f63bad9:/path/to/dir# ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:4","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"挂载 tmpfs (–tmpfs) $ docker run -d --tmpfs /run:rw,noexec,nosuid,size=65536k my_image ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:5","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"挂载卷（-v，–read-only） docker run \\ -p 9000:9000 \\ -p 9090:9090 \\ --name minio \\ -v ~/Desktop/testminio:/data \\ -e \"MINIO_ROOT_USER=ROOTNAME\" \\ -e \"MINIO_ROOT_PASSWORD=CHANGEME123\" \\ -d \\ minio/minio server /data --console-address \":9090\" -p ：端口映射，host_port:local_port –name：容器的名字 -v：挂载 -d：后台运行 -e：设置环境变量，MINIO_ROOT_USER和MINIO_ROOT_PASSWORD。 $ docker run -v `pwd`:`pwd` -w `pwd` -i -t ubuntu pwd -v参数将当前工作目录挂载到容器中。-w 让命令在当前工作目录中执行，最后在容器中执行pwd $ docker run -v `pwd`:`pwd` -w `pwd` -i -t ubuntu root@be74d2cdbc61:/Users/joker# root@be74d2cdbc61:/Users/joker# ls 1.mpeg Documents Library Music Postman blog java_error_in_goland_52033.log software_pkg Desktop Downloads Movies Pictures Public code software 很明显，进入容器后，访问的是是宿主机的家目录。 $ docker run --read-only -v /icanwrite busybox touch /icanwrite/here ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:6","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"–mount $ docker run -t -i --mount type=bind,src=/data,dst=/data busybox sh docker-compose Compose is a tool for defining and running multi-container Docker applications $ docker-compose -h Define and run multi-container applications with Docker. Usage: docker-compose [-f \u003carg\u003e...] [--profile \u003cname\u003e...] [options] [--] [COMMAND] [ARGS...] docker-compose -h|--help Options: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --profile NAME Specify a profile to enable -c, --context NAME Specify a context name --verbose Show more output --log-level LEVEL Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) --ansi (never|always|auto) Control when to print ANSI control characters --no-ansi Do not print ANSI control characters (DEPRECATED) -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent (DEPRECATED) --env-file PATH Specify an alternate environment file Commands: build Build or rebuild services config Validate and view the Compose file create Create services down Stop and remove resources events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show version information and quit # 后台运行 $ docker-compose up -d ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:9:7","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"docker-compose.yml docker-compose 读取这个配置文件来运行containers 例如minio version: '3.7' # Settings and configurations that are common for all containers x-minio-common: \u0026minio-common image: quay.io/minio/minio:RELEASE.2022-06-03T01-40-53Z command: server --console-address \":9001\" http://minio{1...4}/data{1...2} expose: - \"9000\" - \"9001\" # environment: # MINIO_ROOT_USER: minioadmin # MINIO_ROOT_PASSWORD: minioadmin healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 20s retries: 3 # starts 4 docker containers running minio server instances. # using nginx reverse proxy, load balancing, you can access # it through port 9000. services: minio1: \u003c\u003c: *minio-common hostname: minio1 volumes: - data1-1:/data1 - data1-2:/data2 minio2: \u003c\u003c: *minio-common hostname: minio2 volumes: - data2-1:/data1 - data2-2:/data2 minio3: \u003c\u003c: *minio-common hostname: minio3 volumes: - data3-1:/data1 - data3-2:/data2 minio4: \u003c\u003c: *minio-common hostname: minio4 volumes: - data4-1:/data1 - data4-2:/data2 nginx: image: nginx:1.19.2-alpine hostname: nginx volumes: - ./nginx.conf:/etc/nginx/nginx.conf:ro ports: - \"9000:9000\" - \"9001:9001\" depends_on: - minio1 - minio2 - minio3 - minio4 ## By default this config uses default local driver, ## For custom volumes replace with volume driver configuration. volumes: data1-1: data1-2: data2-1: data2-2: data3-1: data3-2: data4-1: data4-2: https://docs.docker.com/get-started/overview/#the-docker-platform ↩︎ ","date":"2022-04-25","objectID":"/2022/04/docker/tutorials/:10:0","tags":["docker"],"title":"docker guide","uri":"/2022/04/docker/tutorials/"},{"categories":null,"content":"官网 minio minio server #单点部署。--console-address 指定固定端口，~/Desktop/testminio 挂载点 $ minio server ~/Desktop/testminio --console-address \":9001\" sudo docker run \\ -p 9000:9000 \\ -p 7070:9090 \\ --name minio \\ --restart=always \\ -v /Users/joker/docker/minio/data:/data \\ -e \"MINIO_ROOT_USER=cat13\" \\ -e \"MINIO_ROOT_PASSWORD=cat12345\" \\ -e \"TZ=Asia/Shanghai\" \\ -d \\ quay.io/minio/minio server /data --console-address \":9090\" ","date":"2022-04-23","objectID":"/2022/04/minio/minio-cli/:0:0","tags":["minio"],"title":"minio command","uri":"/2022/04/minio/minio-cli/"},{"categories":null,"content":"本篇用于记录计算机的热点词汇 Standalone Deployments：单点部署 Distributed Deployments：分布式部署 workloads：工作负载 tutorials：教程 custom filesystem：自定义文件系统 diagnose：诊断 instruction：指令 ","date":"2022-04-22","objectID":"/2022/04/hot-words/cs-hot-word/:0:0","tags":null,"title":"热词","uri":"/2022/04/hot-words/cs-hot-word/"},{"categories":null,"content":"http 协议 http协议全称Hypertext Transfer Protocol，位于整个网络层的应用层，也不难发现很多语言关于http的封装都是第三方框架实现的如java 的spring，也有的是标准库实现的如go 的http包，本篇的重点探究go的标准库http。 ","date":"2022-04-20","objectID":"/2022/04/golang/go-http/:0:0","tags":["golang"],"title":"go http","uri":"/2022/04/golang/go-http/"},{"categories":null,"content":"底层原理 作为应用层协议需要依赖传输层来进行。传输层用的最广泛的是tcp和udp，HTTP /1.1和 HTTP /2采用的tcp作为底层传输协议，而HTTP /3采用的是QUIC作为底层传输协议，如图1 https://draveness.me/golang/docs/part4-advanced/ch09-stdlib/golang-net-http/#921-%e8%ae%be%e8%ae%a1%e5%8e%9f%e7%90%86 ↩︎ ","date":"2022-04-20","objectID":"/2022/04/golang/go-http/:1:0","tags":["golang"],"title":"go http","uri":"/2022/04/golang/go-http/"},{"categories":null,"content":"删除链表的倒数第n个节点 示例1： 输入：head = [1,2,3,4,5], n = 2 输出：[1,2,3,5] 示例2: 输入：head = [1], n = 1 输出：[] 示例3: 输入：head = [1,2], n = 1 输出：[1] 提示: 链表中结点的数目为 sz 1 \u003c= sz \u003c= 30 0 \u003c= Node.val \u003c= 100 1 \u003c= n \u003c= sz func removeNthFromEnd(head *ListNode, n int) *ListNode { p := head q := head i := 0 for q != nil { if i \u003e n { p = p.Next } i++ q = q.Next } if n-i \u003e= 0 { head = head.Next } else { tmp := p.Next if tmp != nil { p.Next = tmp.Next } } return head } 将下面的字符串转成map //下面若干个空格 //frame= 425 fps= 71 q=-1.0 Lsize= 5158kB time=00:00:17.11 bitrate=2468.7kbits/s dup=1 drop=0 speed=2.84x func Task00(s string) map[string]string { m := make(map[string]string) var start, end int for i := 0; i \u003c len(s); i++ { if s[i] == ' ' { continue } var key, value string for end = i; end \u003c len(s); end++ { if s[end] == '=' { key = s[start:end] break } } start = end + 1 for ; start \u003c len(s); start++ { if s[start] != ' ' { end = start break } } for ; end \u003c len(s); end++ { if s[end] == ' ' { value = s[start:end] break } if end == len(s)-1 { value = s[start:] } } i = end start = end + 1 m[key] = value } return m } func Task10(s string) map[string]string { list := strings.Fields(s) m := make(map[string]string) for i := 0; i \u003c len(list); i++ { if list[i] == \"\" { continue } v := list[i] if v[len(list[i])-1] == '=' { if i+1 \u003c len(list) { m[v[:len(v)-1]] = list[i+1] i++ } } else { strs := strings.Split(v, \"=\") if len(strs) \u003e 1 { m[strs[0]] = strs[1] } } } return m } ","date":"2022-04-11","objectID":"/2022/04/arithmetic/%E7%AE%97%E6%B3%95%E9%A2%98/:0:0","tags":null,"title":"算法题","uri":"/2022/04/arithmetic/%E7%AE%97%E6%B3%95%E9%A2%98/"},{"categories":null,"content":"应用场景 在开发当中，会经常遇到排序的需求，比如根据时间倒序排序；根据价格降序排序；等等。有时还会遇到几种维度的排序。一般的，这种排序规则都会在数据库层面处理好顺序再返回给应用服务，order by 语句就可以按照某个字段或者某几个字段倒序(desc)，正序(asc)排序 ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:1:0","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"例子 现在有张表，记录我高中时高二所有人的成绩 CREATE TABLE `student_grade` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT '主键id', `class` int NOT NULL DEFAULT '1' COMMENT '班级', `exam_code` varchar(16) NOT NULL DEFAULT '' COMMENT '考号', `name` varchar(64) NOT NULL DEFAULT '' COMMENT '姓名', `language_score` int NOT NULL COMMENT '语文分数', `math_score` int NOT NULL COMMENT '数学分数', `english_score` int NOT NULL COMMENT '英语分数', `total_score` int NOT NULL COMMENT '总分数', PRIMARY KEY (`id`), KEY `idx_class` (`class`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='成绩表'; 部分数据如下 现在我想看下我所处的班级(17)总成绩前10的都有哪些人，sql应该长这个样子 SELECT * FROM student_grade WHERE class = 17 ORDER BY total_score DESC LIMIT 10; 由表结构可以看出，class是索引列，执行下explain看看这个排序语句有没有用索引 EXPLAIN SELECT * FROM student_grade WHERE class = 17 ORDER BY total_score DESC LIMIT 10; key表示使用到的索引 Extra的Using filesort表示用到了排序 ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:2:0","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"简易原理 mysql会为每个查询的线程分配一块内存空间(sort buffer)用来排序。查询因子class作为索引列，可以很快的找到目标17班的数据 根据BPlusTree这个网站画出了class的索引树，绿色的数据域是数据的主键id 。还有一棵id的索引树，叶子节点存储了完整的数据。 主键索引树 那么上述例子的排序流程： 根据class的索引找到17对应的id集合 根据id集合到主键索引树中找到对应的字段(回表操作) 把所需字段放入sort_buffer中，根据total_score排序 按照排序结果取10条返回给客户端 如果所需要的字段全放进sort_buffer ,有可能sort_buffer放不下，这个时候需要文件辅助排序了 ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:3:0","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"文件辅助排序 sort_buffer的大小是由sort_buffer_size控制的。如果要排序的数据小于sort_buffer_size，排序在sort_buffer 内存中完成，如果要排序的数据大于sort_buffer_size，则借助磁盘文件来进行排序。下面的sql可以帮助我们查看是否用到文件辅助排序 set optimizer_trace = \"enabled=on\"; select * from student_grade where class =17 order by total_score desc limit 10; select * from information_schema.optimizer_trace 查看sort_buffer_size show variables like 'sort_buffer_size'; 如果排序的数据比这个值大，mysql会使用文件辅助排序（外部排序）。 从id索引树中取出数据放入sort_buffer，最大程度占用sort_buffer，开始排序，放入一个临时的文件中 重复第一步骤，知道数据完全取出，在把有序的小文件合并成有序的大文件 ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:3:1","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"rowid 排序 sort_buffer不够用，可以从另一种角度考虑问题。比如可以只需要查询用于排序的字段和主键id放进sort_buffer，排好序后再通过id回表取出想要的字段，这就是rowid排序。而控制是否使用rowid排序，有个参数max_length_for_sort_data，表示单行字段的长度，如果单行字段长度大于这个参数的值，就会使用rowid排序。 show variables like 'max_length_for_sort_data'; # max_length_for_sort_data 4096 上文的例子中全部字段的长度加起来小于4096，可以故意改小一点 SET max_length_for_sort_data = 32; SELECT * FROM student_grade WHERE class = 17 ORDER BY total_score DESC LIMIT 10; select * from information_schema.optimizer_trace trace 中有 \"rowid_ordered\": true 表示使用的是rowid 排序 ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:3:2","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"order by 注意点 查询没有where条件，order by 的字段不需要加索引，加了索引也是全表扫描，如果有limit是有可能用到索引 ALTER table student_grade ADD INDEX idx_total_scores(total_score); EXPLAIN SELECT * FROM student_grade ORDER BY total_score DESC # 用到了索引 EXPLAIN SELECT * FROM student_grade ORDER BY total_score DESC limit 1 Note 如果查询的字段就是索引列的字段，那么排序就会用到索引 ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:4:0","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"order by 优化 索引列本身就有顺序，可以利用这个特性将class和toal_score 做成联合索引 ALTER table student_grade ADD INDEX idx_class_total_score(class,total_score ); EXPLAIN SELECT * FROM student_grade WHERE class=17 ORDER BY total_score DESC 这样做就可以少一次回表。 但如果查询语句有in并且是多个值。还是会用到排序，这是因为单看17班是有序加入了18班，总成绩就需要重新排序 EXPLAIN SELECT * FROM student_grade WHERE class in (17,18) ORDER BY total_score DESC ","date":"2022-04-08","objectID":"/2022/04/mysql/mysql-orderby/:5:0","tags":["mysql"],"title":"mysql order by","uri":"/2022/04/mysql/mysql-orderby/"},{"categories":null,"content":"http应用层 http协议是应用层协议。如果抛开细节不谈，一个简单的网页请求可以用下面这个图表示整个过程。 ","date":"2022-04-05","objectID":"/2022/04/net/what-/:0:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"解析url 浏览器的第一步工作就是解析url。访问网页的基本原理就是将服务器存放的html文件内容将以http协议发送给客户端。 当没有路径名时，也就是http://server-domain/,会访问服务器根目录下的/index.html或者/defalut.html。 对url解析后，浏览器开始根据请求路径来封装http协议消息体，如图所示： http消息格式要发给服务器，需要tcp来承载，而tcp又需要IP来承载。所以浏览器的第二步需要知道服务器的IP是多少 ","date":"2022-04-05","objectID":"/2022/04/net/what-/:1:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"域名解析 需要注意的是url上如果直接填写的ip+port的方式，不会有域名解析的步骤例如: curl 'http://47.100.163.207:8888/api/index.html' 所谓的域名解析，实际上是域名和IP的映射关系。这个IP就是服务器的IP。存放映射关系的地方叫DNS(Domain Name System)。DNS中的域名以英文.分割，例如www.baidu.com，表示了不同层级，越靠右边层级越高。所以域名结构是一个棵树状结构 客户端要想通过域名拿到IP,需要分几种情况讨论。 查看浏览器的域名缓存 查看本地hosts (cat /etc/hosts) 请求本地域名服务器 本地域名服务器请求根域名服务器 本地域名服务器请求顶级域名服务器比如.com ……….. 最终查到IP 拿到IP后开始封装各种底层协议 传输层tcp协议 tcp头部报文格式，如图所示： source port：源端口，发起方端口 destination port：目的端口，接收方端口 sequence number：序列号，保证数据包不乱序 acknowledgment number：确认序列号，确认对方是否收到，如果没有收到就重发，保证不丢包 header len：首部长度 reserved：保留 状态位 FIN：结束连接请求 SYN：发起一个连接请求 RST：重新连接 PSH：? ACK：回复 URG：? window size：窗口大小，流量控制，通信双方声明一个窗口，表示自己当前处理能力。 Check sum：校验和 urgent pointer：紧急指针 option：选项 data：数据，data最大MSS字节 ","date":"2022-04-05","objectID":"/2022/04/net/what-/:2:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"三次握手 客户端发出SYN 包后进入SYS_SENT状态 服务器收到SYN 包后，发出SYN\u0026ACK包后，进入SYN_RCVD状态 客户端收到SYN\u0026ACK包，发出ACK包后，进入``ESTABLISHED`状态 服务器收到ACK包后，进入``ESTABLISHED`状态 tcp的连接状态，可以通过netstat -napt命令查看 ","date":"2022-04-05","objectID":"/2022/04/net/what-/:3:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"tcp拆分数据 如果http请求的消息过长，超过了MSS长度。tcp协议会把数据拆成一个个的数据包分别发送 一个网络包的最大长度，以太网中一般为 1500 字节。 除去 IP 和 tcp 头部之后，一个网络包所能容纳的 TCP 数据的最大长度。 ","date":"2022-04-05","objectID":"/2022/04/net/what-/:4:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"构造tcp报文 tcp头部中，源端口是客户端随机生成的，目的端口是服务器的端口，http默认端口是80，https默认端口是443 tcp数据部分用来存放整个http报文，如图所示： 网络层IP协议 传输层封装tcp报文接下来的工作就交给网络层，在这一层会封装IP数据包，IP数据包的头部如图所示 字段简要解释1 Version：IP的版本，ipv4，ipv6 Internet Header Length (IHL)：IP 数据包的头部长度 Type Of Service (TOS)：服务类型类型 Total Length (TL)：IP数据报的总长度，最大为65535字节 Identification：标识 Flags：标志位 Fragment Offset：当消息发生分片时，该字段指定该分片中的数据在整个消息中的偏移量或位置 Time To Live (TTL): 指定数据报允许在网络上“生存”多长时间，以路由器跳数为单位。每个路由器在传输之前将 TTL 字段的值递减（减一）。如果 TTL 字段下降到零，则认为数据报经过了太长的路由并被丢弃 Protocol：协议，表示，是tcp还是udp等等 Header Checksum：在标头上计算的校验和，以提供基本保护以防止传输中的损坏 Source Address：源ip地址 Destination Address：目的ip地址 Options：选项，在某些 IP 数据报的标准报头之后，可以包含几种类型的选项中的一种或多种 Padding：填充，如果包含一个或多个选项，并且用于它们的位数不是 32 的倍数，则添加足够的零位以将标头“填充”为 32 位（4 字节）的倍数 Data：数据，要在数据报中传输的数据，可以是整个高层消息，也可以是消息的片段。 tcp整个数据将填充于IP协议的Data中,如图所示 以太网II Ethernet II数据链路层 事实上只有iP是无法到达目的主机的，这是因为最终应用层交付的数据是由物理层面来接手。ip地址只是逻辑地址，真正地址是设备的mac地址，所谓的mac地址就是物理设备的\"指纹\"。整条链路下来如同套娃一样，一层层嵌套。以太网数据帧如图所示2： 当http协议的数据被封装到链路层的数据包准备发送时，需要知道两个很重要的参数Destination MAC Address 和Source MAC Address。机器的操作系统一启动就会将网卡的mac地址写入内存，所以很容易知道自己的mac地址，而对方的mac地址则需要ARP协议来确定。 ","date":"2022-04-05","objectID":"/2022/04/net/what-/:5:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"ARP协议 全称:Address Resolution Protocol,是一种ip地址到mac地址的映射，已知目标ip查询所对应的mac地址。 如果目标主机的ip地址不在一个链路，那么会查找一下跳路由器的mac地址。如何知道对方的mac地址呢，很容易想到的方案就是广播，情况分为两种。 两台主机在同一网段，arp将以广播的方式进行请求，如图所示： a想要发送数据包给b，比如http请求tcp建立连接的第一次握手syn包，a会检查自己的arp本地缓存(arp -a)，如果目的ip地址命中缓存就直接取出对应的mac地址，如果没有命中，那么a通过arp协议向局域网中广播192.168.61.191的mac地址是什么，局域网中的每个主机收到这个arp数据包后检查自己的ip地址和询问的ip地址是否匹配，如果匹配则返回自己的mac地址，如果不是则丢弃。这时候a就知道目的主机的mac地址了进而构造出L23的header,通过switch转发到目的主机。如果浏览器的网址是访问一个局域网中的服务器，那么到这里客户端的syn包已经发送到服务器端了，服务端接收到这个L2的数据包后，又层层解开封装，如同一层层剥开洋葱一样，直到tcp的包暴露出来，这时服务器才知道客户端想建立连接，然后就是三次握手，握手后客户端开始发送http协议的内容，服务端收到后将网页的内容返回回去，然后就是浏览器渲染html文本，进而用户看见了网页。 两台主机不在同一网段，此时就需要Proxy ARP4。整个过程还需要路由器参与将数据包送到不同的网段。通过子网掩码可以判断两个ip是不是同一个网段。如图所示，每个路由器之间mac地址都会被剥离并且生成新的目的mac地址使其到达下一跳。第一个主机的生成的ip数据包只会被接收的主机剥离。因此ip数据包的头部是“端到端“的处理，而数据链路的数据包头部是”跳到跳“处理 总结 当浏览器发出页面请求的时候，就会层层封装消息，从应用层到物理层，到了服务器又会层层解封，最终解出http协议并将目录下的网页文件内容发送给客户端这个过程又涉及层层封装消息，客户端收到后层层解出得到网页的html文本，浏览器渲染，最终生成用户看见的页面。如动图所示5： http://www.tcpipguide.com/free/t_IPDatagramGeneralFormat.htm ↩︎ https://en.wikipedia.org/wiki/Ethernet_frame ↩︎ L2 是Layer 2的缩写，指的是数据链路层，以此类推，L3是指网络层即ip层 ↩︎ https://www.practicalnetworking.net/series/arp/proxy-arp/ ↩︎ https://www.practicalnetworking.net/series/packet-traveling/osi-model/ ↩︎ ","date":"2022-04-05","objectID":"/2022/04/net/what-/:6:0","tags":["网络协议"],"title":"浏览器输入网址回车后会发什么","uri":"/2022/04/net/what-/"},{"categories":null,"content":"二叉查找树 对于树中的每个节点X，它的左子树中所有关键字值小于X的关键值，它的右子树中所有的关键值大于X的关键值 ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:0:0","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"数据结构 #ifndef _Tree_H typedef int ElementType; struct TreeNode; typedef struct TreeNode *Postion; typedef struct TreeNode *SearchTree; SearchTree MakeEmpty(SearchTree T); Postion Find(ElementType e, SearchTree T); Postion FindMin(SearchTree T); Postion FindMax(SearchTree T); SearchTree Insert(ElementType e, SearchTree T); ElementType Retrieve(Postion p); #endif typedef struct TreeNode { ElementType Element; SearchTree Left; SearchTree Right; } TreeNode; ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:1:0","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"具体实现 ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:0","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"插入 SearchTree Insert(ElementType e, SearchTree T) { if (T == NULL) { T = (TreeNode *)malloc(sizeof(TreeNode)); if (T == NULL) { printf(\"out of space !\"); } else { T-\u003eElement = e; T-\u003eLeft = T-\u003eRight = NULL; } } else if (e \u003c T-\u003eElement) { T-\u003eLeft = Insert(e, T-\u003eLeft); } else if (e \u003e T-\u003eElement) { T-\u003eRight = Insert(e, T-\u003eRight); } return T; } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:1","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"最大值 Postion FindMax(SearchTree T) { if (T == NULL) { return NULL; } else if (T-\u003eRight == NULL) { return T; } else { return FindMax(T-\u003eRight); } } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:2","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"最小值 Postion FindMin(SearchTree T) { if (T == NULL) { return NULL; } else if (T-\u003eLeft == NULL) { return T; } else { return FindMin(T-\u003eLeft); } } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:3","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"查找某个节点 Postion Find(ElementType e, SearchTree T) { if (T == NULL) return NULL; if (T-\u003eElement \u003c e) { return Find(e, T-\u003eRight); } else if (T-\u003eElement \u003e e) { return Find(e, T-\u003eLeft); } else { return T; } } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:4","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"初始化 SearchTree MakeEmpty(SearchTree T) { if (T != NULL) { MakeEmpty(T-\u003eLeft); MakeEmpty(T-\u003eRight); free(T); } return NULL; } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:5","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"测试 int main() { TreeNode *root = Insert(0, NULL); Insert(2, root); Insert(3, root); Insert(-1, root); Insert(-2, root); Insert(-3, root); TreeNode *min = FindMin(root); printf(\"min:%d\\n\", min-\u003eElement); TreeNode *max = FindMax(root); printf(\"max:%d\\n\", max-\u003eElement); TreeNode *some = Find(2, root); printf(\"some:%d\\n\", some-\u003eElement); root = MakeEmpty(root); printf(\"some:%d\\n\", root == NULL); } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:6","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"删除元素 SearchTree Delete(ElementType e, SearchTree T) { if (T == NULL) { printf(\"not found target\\n\"); } SearchTree tmpCell; if (e \u003c T-\u003eElement) { T-\u003eLeft = Delete(e, T-\u003eLeft); } else if (e \u003e T-\u003eElement) { T-\u003eRight = Delete(e, T-\u003eRight); } else if (T-\u003eRight \u0026\u0026 T-\u003eLeft) { tmpCell = FindMin(T-\u003eRight); T-\u003eElement = tmpCell-\u003eElement; T-\u003eRight = Delete(T-\u003eElement, T-\u003eRight); } else { tmpCell = T; if (T-\u003eLeft == NULL) { T = T-\u003eRight; } else if (T-\u003eRight == NULL) { T = T-\u003eLeft; } free(tmpCell); } return T; } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:7","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"测试删除 int main() { TreeNode *root = Insert(0, NULL); Insert(2, root); Insert(3, root); Insert(-1, root); Insert(-2, root); Insert(-3, root); printf(\"max:%d\\n\", FindMax(root)-\u003eElement); Delete(3, root); printf(\"max:%d\\n\", FindMax(root)-\u003eElement); } ","date":"2022-03-27","objectID":"/2022/03/c-d-s/chapter4-tree/:2:8","tags":["数据结构与算法c描述"],"title":"第四章 树","uri":"/2022/03/c-d-s/chapter4-tree/"},{"categories":null,"content":"壁纸 https://wallhaven.cc/ 生成Favicon https://www.favicon-generator.org/ 流程图 https://app.diagrams.net/ wordpress 建站 https://cn.wordpress.org/ Mac软件破解 https://macwk.com/ 模型网站 https://www.mixamo.com/#/ ascii图像 http://www.ascii-fr.com/index.html Windows系统级的软件 https://msdn.itellyou.cn/ 代码图 https://carbon.now.sh/ ","date":"2022-03-27","objectID":"/2022/03/website/all/:0:0","tags":["网站"],"title":"收藏网站","uri":"/2022/03/website/all/"},{"categories":null,"content":"概述 服务器上的进程可能有上千条连接，如何快速的知道哪条连接来了网络数据，正是epoll要解决的问题。在早期，linux内核采用的select 和 poll。结论是在连接数很多，活跃的连接少的情况下epoll有高效的性能优势，原因之一是epoll唤醒进程后，只需要查看就绪（有数据）的socket，而不是遍历整个socket集合。多路复用指的是进程的复用，本文重点描述epoll原理1 Socket 数据结构 当服务器执行accept后会阻塞，等待客户端的连接请求。如果客户端发起连接，三次握手后，服务器会创建一个新的socket，用于客户端和服务器的通信。本着linux一切皆为文件的原则，socket 也是一种文件描述符。并且把它加入当前进程的打开文件列表中 socket socket 内核对象具体的细节如图所示 socket_detail ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:0:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"创建 Socket 用户（用户程序）调用accept系统调用后，当接收到连接时会创建socket对象。其源码位于net/socket.c截取部分代码所示: SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr, int __user *, upeer_addrlen, int, flags) { struct socket *sock, *newsock; //根据 fd 查找到监听的 socket sock = sockfd_lookup_light(fd, \u0026err, \u0026fput_needed); // 申请并初始化新的 socket newsock = sock_alloc(); newsock-\u003etype = sock-\u003etype; newsock-\u003eops = sock-\u003eops; //申请新的 file 对象，并设置到新 socket 上 newfile = sock_alloc_file(newsock, flags, sock-\u003esk-\u003esk_prot_creator-\u003ename); ...... //接收连接 err = sock-\u003eops-\u003eaccept(sock, newsock, sock-\u003efile-\u003ef_flags); //添加新文件到当前进程的打开文件列表 fd_install(newfd, newfile); 上述代码中可以看出，先找出一个listen状态的socket，随即malloc 出一个新的socket对象，拷贝type，ops（指针拷贝）。由前面的socket数据结构可知ops是struct proto_ops inet_stream_ops。如图所示： socket_ops 其中 inet_stream_ops 起源码位于net/ipv4/af_inet.c，截取部分代码如下 const struct proto_ops inet_stream_ops = { .family = PF_INET, .owner = THIS_MODULE, .release = inet_release, .bind = inet_bind, .connect = inet_stream_connect, .socketpair = sock_no_socketpair, .accept = inet_accept, .getname = inet_getname, .poll = tcp_poll, ...... ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:1:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"创建file对象 前面提到的socket数据结构中，有个重要字段file。在accept方法里面会调用sock_alloc_file来初始化，然后赋值给新的socket对象，如图所示： socket_file sock_alloc_file 源码如下： struct file *sock_alloc_file(struct socket *sock, int flags, const char *dname) { struct file *file; file = alloc_file(\u0026path, FMODE_READ | FMODE_WRITE, \u0026socket_file_ops); ...... sock-\u003efile = file; } sock_alloc_file 里面回调用 alloc_file。注意在 alloc_file 方法中，把 socket_file_ops 函数集合一并赋到了新 file-\u003ef_op 里了。其源码位于fs/file_table.c struct file *alloc_file(struct path *path, fmode_t mode, const struct file_operations *fop) { struct file *file; file-\u003ef_op = fop; ...... } 可以看出，在accept里创建的新socket 的file-\u003ef_op-\u003epoll 函数指向的是 sock_poll。另外file对象内部有个socket指针，指向socket对象。 ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:2:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"接收连接 socket结构中除了file，还有一个核心的成员sock。其源码位于include/linux/net.h。部分源码如下： struct socket { socket_state state; short type; unsigned long flags; struct file *file; struct sock *sk; const struct proto_ops *ops; struct socket_wq wq; }; sock 是socket的核心对象。发送队列、接收队列、等待队列等核心数据结构都位于此 在accept 函数中，其源码位于net/socket.c，部分源码如下 SYSCALL_DEFINE4(accept4, ...) ... //1.3 接收连接 err = sock-\u003eops-\u003eaccept(sock, newsock, sock-\u003efile-\u003ef_flags); } sock-\u003eops-\u003eaccept 对应的方法是 inet_accept。它执行的时候会从握手队列里直接获取创建好的 sock。 sock 对象的完整创建过程涉及到三次握手。 struct sock 初始化过程中用到的一个函数： void sock_init_data(struct socket *sock, struct sock *sk) { sk-\u003esk_wq = NULL; sk-\u003esk_data_ready = sock_def_readable; } 在这里把 sock 对象的 sk_data_ready 函数指针设置为 sock_def_readable。后面还会提到这个sock_def_readable ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:3:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"加入当前进程的打开文件列表中 当file,socket,sock等对象初始化完毕后，还需要将其挂到当前进程的打开文件列表中 void fd_install(unsigned int fd, struct file *file) { __fd_install(current-\u003efiles, fd, file); } void __fd_install(struct files_struct *files, unsigned int fd, struct file *file) { ... fdt = files_fdtable(files); BUG_ON(fdt-\u003efd[fd] != NULL); rcu_assign_pointer(fdt-\u003efd[fd], file); } epoll数据结构 epoll 暴露出来的API非常简单，epoll_create,epoll_ctl,epoll_wait epoll_create：创建一个epoll对象 epoll_ctl：管理epoll_item，epoll_item与socket关联 epoll_wait：等待管理的socket IO事件 和socket一样，调用epoll_create创建出来的event_poll会和当前进程打开的文件列表关联。 eventpoll epoll_create 其源代码位于fs/eventpoll.c，其中struct eventpoll 的定义也在这个源文件中。部分代码如下所示： SYSCALL_DEFINE1(epoll_create1, int, flags) { struct eventpoll *ep = NULL; //创建eventpoll 对象 error = ep_alloc(\u0026ep); } struct eventpoll { //sys_epoll_wait用到的等待队列 wait_queue_head_t wq; //接收就绪的描述符都会放到这里 struct list_head rdllist; //每个epoll对象中都有一颗红黑树 struct rb_root rbr; ...... } wq：等待进程队列双向链表。软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户进程 rbr：红黑树。通过这棵树来管理用户进程下添加的socket连接 rdllist: 就绪socket描述符双向链表。当有的连接就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样进程只需要判断链表就能找出就绪进程，而不用去遍历整棵树 eventpoll被创建出来，还需要初始化， 其源代码位于fs/eventpoll.c ，部分代码如下所示： static int ep_alloc(struct eventpoll **pep) { struct eventpoll *ep; //申请 epollevent 内存 ep = kzalloc(sizeof(*ep), GFP_KERNEL); //初始化等待队列头 init_waitqueue_head(\u0026ep-\u003ewq); //初始化就绪列表 INIT_LIST_HEAD(\u0026ep-\u003erdllist); //初始化红黑树指针 ep-\u003erbr = RB_ROOT; ...... } ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:4:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"epoll_ctl 添加socket 当epoll_ctl注册每个socket的时候，会做三件事情 分配一个红黑树节点对象epitem 添加等待事件到socket的等待队列中，其回调函数是ep_poll_callback 将epitem插入 rdllist红黑树中 通过epoll_ctl 添加两个socket后，结构如下图 epoll_ctl epoll_ctl 添加socket 的源码位于fs/eventpoll.c，部分代码如下所示： SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event) { struct eventpoll *ep; struct file *file, *tfile; //根据 epfd 找到 eventpoll 内核对象 file = fget(epfd); ep = file-\u003eprivate_data; //根据 socket 句柄号， 找到其 file 内核对象 tfile = fget(fd); switch (op) { case EPOLL_CTL_ADD: if (!epi) { epds.events |= POLLERR | POLLHUP; error = ep_insert(ep, \u0026epds, tfile, fd); } else error = -EEXIST; clear_tfile_check_list(); break; } epoll_ctl 添加操作，会走到这个EPOLL_CTL_ADD case，重点看下ep_insert函数，其源码位于fs/eventpoll.c，部分代码如下所示： static int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd) { //3.1 分配并初始化 epitem //分配一个epi对象 struct epitem *epi; if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; //对分配的epi进行初始化 //epi-\u003effd中存了句柄号和struct file对象地址 INIT_LIST_HEAD(\u0026epi-\u003epwqlist); epi-\u003eep = ep; ep_set_ffd(\u0026epi-\u003effd, tfile, fd); //3.2 设置 socket 等待队列 //定义并初始化 ep_pqueue 对象 struct ep_pqueue epq; epq.epi = epi; init_poll_funcptr(\u0026epq.pt, ep_ptable_queue_proc); //调用 ep_ptable_queue_proc 注册回调函数 //实际注入的函数为 ep_poll_callback revents = ep_item_poll(epi, \u0026epq.pt); ...... //3.3 将epi插入到 eventpoll 对象中的红黑树中 ep_rbtree_insert(ep, epi); ...... } ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:5:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"epitem 初始化 epitem 数据结构 位于struct epitem ，部分代码如下所示： struct epitem { //红黑树节点 struct rb_node rbn; //socket文件描述符信息 struct epoll_filefd ffd; //所归属的 eventpoll 对象 struct eventpoll *ep; //等待队列 struct list_head pwqlist; } 对epitem部分初始化，包括关联一个eventpoll对象，关联socket 的file字段 epitem 关联file字段代码位于ep_set_ffd /* Setup the structure that is used as key for the RB tree */ static inline void ep_set_ffd(struct epoll_filefd *ffd, struct file *file, int fd) { ffd-\u003efile = file; ffd-\u003efd = fd; } ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:6:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"设置socket等待队列 ep_poll_callback epitem初始化后，接下来就设置socket等待队列。并把ep_poll_callback设置成回调函数 static inline unsigned int ep_item_poll(struct epitem *epi, poll_table *pt) { pt-\u003e_key = epi-\u003eevent.events; return epi-\u003effd.file-\u003ef_op-\u003epoll(epi-\u003effd.file, pt) \u0026 epi-\u003eevent.events; } 结合socket的结构图中可以看出，sock-\u003eops-\u003epoll 其实指向的是 tcp_poll。 unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait) { struct sock *sk = sock-\u003esk; sock_poll_wait(file, sk_sleep(sk), wait); } sock_poll_wait 第二个参数是调用sk_sleep获得的，其实就是socket的等待队列 static inline wait_queue_head_t *sk_sleep(struct sock *sk) { BUILD_BUG_ON(offsetof(struct socket_wq, wait) != 0); return \u0026rcu_dereference_raw(sk-\u003esk_wq)-\u003ewait; } 进入sock_poll_wait static inline void sock_poll_wait(struct file *filp, wait_queue_head_t *wait_address, poll_table *p) { poll_wait(filp, wait_address, p); } static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p) { if (p \u0026\u0026 p-\u003e_qproc \u0026\u0026 wait_address) p-\u003e_qproc(filp, wait_address, p); } 这里的 qproc 是个函数指针，它在前面的 init_poll_funcptr 调用时被设置成了 ep_ptable_queue_proc 函数。 static int ep_insert(...) { ... init_poll_funcptr(\u0026epq.pt, ep_ptable_queue_proc); ... } //file: include/linux/poll.h static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc) { pt-\u003e_qproc = qproc; pt-\u003e_key = ~0UL; /* all events enabled */ } 在 ep_ptable_queue_proc 函数中，新建了一个等待队列项，并注册其回调函数为ep_poll_callback函数。然后再将这个等待项添加到 socket 的等待队列中。 static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead, poll_table *pt) { struct eppoll_entry *pwq; f (epi-\u003enwait \u003e= 0 \u0026\u0026 (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) { //初始化回调方法 init_waitqueue_func_entry(\u0026pwq-\u003ewait, ep_poll_callback); //将ep_poll_callback放入socket的等待队列whead（注意不是epoll的等待队列） add_wait_queue(whead, \u0026pwq-\u003ewait); } 在epoll之前，由于需要在数据就绪的时候唤醒用户进程，所以等待对象项的 private 会设置成当前用户进程描述符 current。 而我们今天的 socket 是交给 epoll 来管理的，不需要在一个 socket 就绪的时候就唤醒进程，所以这里的 q-\u003eprivate 没有用就设置成了 NULL static inline void init_waitqueue_func_entry( wait_queue_t *q, wait_queue_func_t func) { q-\u003eflags = 0; q-\u003eprivate = NULL; //ep_poll_callback 注册到 wait_queue_t对象上 //有数据到达的时候调用 q-\u003efunc q-\u003efunc = func; } 可以看出，软中断将数据拷贝到socket 的接收队列后，会通过ep_poll_callback回调函数通知epoll对象 ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:7:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"插入红黑树 分配完epitem 对象后，会把它插入到红黑树中 rbr 为什么要红黑树呢 ？ ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:8:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"epoll_wait等待接收 当epoll_wait被调用的时候，会判断eventpoll-\u003erdllist 链表里有没有数据，如果有数据就返回，没有数据就创建一个等待队列的元素，然后添加到eventpoll等待队列上。其过程大致如下 epoll_wait 值得注意的是epoll_ctl 添加 socket 时也创建了等待队列项。不同的是这里的等待队列项是挂在 epoll 对象上的，而前者是挂在 socket 对象上的。 其源代码如下： //file: fs/eventpoll.c SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout) { ... error = ep_poll(ep, events, maxevents, timeout); } static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout) { wait_queue_t wait; ...... fetch_events: //4.1 判断就绪队列上有没有事件就绪 if (!ep_events_available(ep)) { //4.2 定义等待事件并关联当前进程 init_waitqueue_entry(\u0026wait, current); //4.3 把新 waitqueue 添加到 epoll-\u003ewq 链表里 __add_wait_queue_exclusive(\u0026ep-\u003ewq, \u0026wait); for (;;) { ... //4.4 让出CPU 主动进入睡眠状态 if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) timed_out = 1; ... } 判断就绪队列有没有就绪的socket，代码如下所示 static inline int ep_events_available(struct eventpoll *ep) { return !list_empty(\u0026ep-\u003erdllist) || ep-\u003eovflist != EP_UNACTIVE_PTR; } 阻塞当前进程，将创建出来的待队列对象与当前进程关联，设置唤醒回调函数 static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p) { q-\u003eflags = 0; q-\u003eprivate = p; q-\u003efunc = default_wake_function; } 添加到等待队列 static inline void __add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait) { wait-\u003eflags |= WQ_FLAG_EXCLUSIVE; __add_wait_queue(q, wait); } 通过 set_current_state 把当前进程设置为可打断。 调用 schedule_hrtimeout_range 让出 CPU，主动进入睡眠状态 int __sched schedule_hrtimeout_range(ktime_t *expires, unsigned long delta, const enum hrtimer_mode mode) { return schedule_hrtimeout_range_clock( expires, delta, mode, CLOCK_MONOTONIC); } int __sched schedule_hrtimeout_range_clock(...) { schedule(); ... } 在 schedule 中选择下一个进程调度 static void __sched __schedule(void) { next = pick_next_task(rq); ... context_switch(rq, prev, next); } 数据从网卡到进程 在前面 epoll_ctl 执行的时候，会给每一个 socket 添加回调函数（等待队列对象）。 在 epoll_wait 运行完的时候，会给event_poll添加等待队列对象。数据开始接收之前，先把这些队列项的内容再稍微总结一下。 epoll socket-\u003esock-\u003esk_data_ready 设置的就绪处理函数是 sock_def_readable 在socket等待队列中，回调函数是ep_poll_callback,private字段没有作用，置为NULL 在 eventpoll 的等待队列项中，回调函数是 default_wake_function。其 private 指向的是等待该事件的用户进程。 ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:9:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"接收数据到任务队列 软中断来的时候会一步一步往协议上层分解。网络帧-\u003etcp数据包，这里重点分析tcp协议栈的处理入口函数 tcp_v4_rcv // file: net/ipv4/tcp_ipv4.c int tcp_v4_rcv(struct sk_buff *skb) { ...... th = tcp_hdr(skb); //获取tcp header iph = ip_hdr(skb); //获取ip header //根据数据包 header 中的 ip、端口信息查找到对应的socket sk = __inet_lookup_skb(\u0026tcp_hashinfo, skb, th-\u003esource, th-\u003edest); ...... //socket 未被用户锁定 if (!sock_owned_by_user(sk)) { { if (!tcp_prequeue(sk, skb)) ret = tcp_v4_do_rcv(sk, skb); } } } 在 tcp_v4_rcv 中首先根据收到的网络包的 header 里的 source 和 dest 信息来在本机上查询对应的 socket。找到以后，就进入接收的主体函数 tcp_v4_do_rcv 。 //file: net/ipv4/tcp_ipv4.c int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb) { if (sk-\u003esk_state == TCP_ESTABLISHED) { //执行连接状态下的数据处理 if (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb-\u003elen)) { rsk = sk; goto reset; } return 0; } //其它非 ESTABLISH 状态的数据包处理 ...... } 如果处理的是 ESTABLISH 状态下的包，这样就又进入 tcp_rcv_established 函数中 //file: net/ipv4/tcp_input.c int tcp_rcv_established(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th, unsigned int len) { ...... //接收数据到队列中 eaten = tcp_queue_rcv(sk, skb, tcp_header_len, \u0026fragstolen); //数据 ready，唤醒 socket 上阻塞掉的进程 sk-\u003esk_data_ready(sk, 0); 在 tcp_rcv_established 中通过调用 tcp_queue_rcv 函数中完成了将接收数据放到 socket 的接收队列上。 tcp_queue_rcv 如下源码所示： //file: net/ipv4/tcp_input.c static int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb, int hdrlen, bool *fragstolen) { //把接收到的数据放到 socket 的接收队列的尾部 if (!eaten) { __skb_queue_tail(\u0026sk-\u003esk_receive_queue, skb); skb_set_owner_r(skb, sk); } return eaten; } ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:10:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"查找就绪回调函数 调用 tcp_queue_rcv 接收完成之后，接着再调用 sk_data_ready 。 在 accept 函数创建 socket 流程里提到的 sock_init_data 函数，在这个函数里已经把 sk_data_ready 设置成 sock_def_readable 函数了。它是默认的数据就绪处理函数。当 socket 上数据就绪时候，内核将以 sock_def_readable 这个函数为入口，找到 epoll_ctl 添加 socket 时在其上设置的回调函数ep_poll_callback。 poll_callback 具体代码如下所示： //file: net/core/sock.c static void sock_def_readable(struct sock *sk, int len) { struct socket_wq *wq; rcu_read_lock(); wq = rcu_dereference(sk-\u003esk_wq); //这个名字起的不好，并不是有阻塞的进程， //而是判断等待队列不为空 if (wq_has_sleeper(wq)) //执行等待队列项上的回调函数 wake_up_interruptible_sync_poll(\u0026wq-\u003ewait, POLLIN | POLLPRI | POLLRDNORM | POLLRDBAND); sk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN); rcu_read_unlock(); } 重点看下wake_up_interruptible_sync_poll //file: include/linux/wait.h #define wake_up_interruptible_sync_poll(x, m) \\ __wake_up_sync_key((x), TASK_INTERRUPTIBLE, 1, (void *) (m)) //file: kernel/sched/core.c void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, void *key) { ... __wake_up_common(q, mode, nr_exclusive, wake_flags, key); } 接着进入 __wake_up_common static void __wake_up_common(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, int wake_flags, void *key) { wait_queue_t *curr, *next; list_for_each_entry_safe(curr, next, \u0026q-\u003etask_list, task_list) { unsigned flags = curr-\u003eflags; if (curr-\u003efunc(curr, mode, wake_flags, key) \u0026\u0026 (flags \u0026 WQ_FLAG_EXCLUSIVE) \u0026\u0026 !--nr_exclusive) break; } } 在 __wake_up_common 中，选出等待队列里注册某个元素 curr， 回调其 curr-\u003efunc。 在 ep_insert 调用的时候，把这个 func 设置成 ep_poll_callback 了。 ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:11:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"执行ep_poll_callback //file: fs/eventpoll.c static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key) { //获取 wait 对应的 epitem struct epitem *epi = ep_item_from_wait(wait); //获取 epitem 对应的 eventpoll 结构体 struct eventpoll *ep = epi-\u003eep; //1. 将当前epitem 添加到 eventpoll 的就绪队列中 list_add_tail(\u0026epi-\u003erdllink, \u0026ep-\u003erdllist); //2. 查看 eventpoll 的等待队列上是否有在等待 if (waitqueue_active(\u0026ep-\u003ewq)) wake_up_locked(\u0026ep-\u003ewq); 在 ep_poll_callback根据等待任务队列项上的额外的 base 指针可以找到 epitem， 进而也可以找到 eventpoll 对象。首先它做的第一件事就是把自己的 epitem 添加到 epoll 的就绪队列中。接着它又会查看 eventpoll 对象上的等待队列里是否有等待项（epoll_wait 执行的时候会设置）如果没执行软中断的事情就做完了。如果有等待项，那就查找到等待项里设置的回调函数default_wake_function。 ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:12:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"执行epoll就绪通知 在 default_wake_function 中找到等待队列项里的进程描述符，然后唤醒之 代码如下所示： //file:kernel/sched/core.c int default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags, void *key) { return try_to_wake_up(curr-\u003eprivate, mode, wake_flags); } 等待队列项 curr-\u003eprivate 指针是在 epoll 对象上等待而被阻塞掉的进程。将 epoll_wait 进程推入可运行队列，等待内核重新调度进程。然后 epoll_wait 对应的这个进程重新运行后，就从 schedule 恢复当进程醒来后，继续从 epoll_wait 时暂停的代码继续执行。把 rdlist 中就绪的事件返回给用户进程 //file: fs/eventpoll.c static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout) { ...... __remove_wait_queue(\u0026ep-\u003ewq, \u0026wait); set_current_state(TASK_RUNNING); } check_events: //返回就绪事件给用户进程 ep_send_events(ep, events, maxevents)) } 总结 event_poll_all 所有的回调： sock_def_readable： sock 对象初始化时设置的 ep_poll_callback : epoll_ctl 时添加到 socket 上 default_wake_function: epoll_wait 是设置到 epoll 上的 为什么epoll 会如此高效呢，我想大概是因为，socket接收到软中断后不用马上唤醒进程，由epoll接管。当epoll唤醒进程后，只需要查看rdllist 就绪队列就知道哪些socket有数据，多路复用，复用的是进程。因为频繁的进程切换是效率低下的重要原因之一。 https://zhuanlan.zhihu.com/p/487497556 本文借鉴了这篇文章，由于处于学习阶段有很多相似的地方 ↩︎ ","date":"2022-03-27","objectID":"/2022/03/linux/epoll/:13:0","tags":["linux"],"title":"epoll","uri":"/2022/03/linux/epoll/"},{"categories":null,"content":"切片 go的切片是动态数组，所谓的动态是指切片的容量（底层数组长度）随着元素的增加而扩充，由于数组的长度一经确定就不能更改，所以扩充的原理很自然就想到malloc 一个新的数组空间，将原有的数组元素拷贝到新数组中。 切片有四种申明方式： list:=make([]int,0,10) var list2 []int list3:=list[0:1] list4:=[]int{1,2,3} make 关键字的第二个参数是切片长度，第三个参数是容量。估计初始化容量可以减少扩容带来的性能损耗。 数据结构 在谈数据结构之前，谈谈几个go切片的特性 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:0:0","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"slice的一些特性 下面输出是什么？ package main import \"fmt\" func main() { list := make([]int, 0) add(list) fmt.Println(\"main func:\", list) add1(\u0026list) fmt.Println(\"main func:\", list) } func add(list []int) { list = append(list, 1, 2, 3) fmt.Println(\"add func:\", list) } func add1(list *[]int) { *list = append(*list, 1, 2, 3) fmt.Println(\"add1 func:\", list) } 答案 add func: [1 2 3] main add: [] add1 func: \u0026[1 2 3] main add1 [1 2 3] 分析 因为go 的函数参数是值传递方式，切片底层是个结构体，由于切片的容量不够，发生了扩容，底层的数组地址已经改变了，但是调用者无法感知这种改变 如果传入的是个切片指针，调用者是能感知扩容的变化 下面输出是什么？ func main() { list := []int{1, 2, 3} list1 := list[:2] list1 = append(list1, 9) fmt.Println(list, list1) list1 = append(list1, 10) fmt.Println(list, list1) } 答案 [1 2 9] [1 2 9] [1 2 9] [1 2 9 10] 分析 再不发生扩容的情况下，list1 截取list部分元素，它们共享同一个数组，当list1 = append(list1, 9) 会直接覆盖原来的切片，新切片如果发生扩容 list1 = append(list1, 10)，则不影响原来的底层数组 下面输出是什么？ func main() { list := []int{1, 2, 3} list1 := list[:2] fmt.Println(len(list), cap(list)) fmt.Println(len(list1), cap(list1)) list2 := list[1:] fmt.Println(len(list2), cap(list2)) } 答案 3 3 2 3 2 2 分析 当新切片是从其他切片截取的，新切片的容量会随着左下标增加而减少，如果list2 := list[3:] cap(list3) =0 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:1:0","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"slice的数据结构 /usr/local/go/src/reflect/value.go Data 是指向数组的指针 Len 是切片的长度 Cap 是当前切片的容量，即Data数组的长度 type SliceHeader struct { Data uintptr Len int Cap int } ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:2:0","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"图解上面的特性 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:3:0","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"1 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:3:1","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"2 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:3:2","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"3 追加与扩容 可以使用append关键字向切片中追加元素，在追加元素过程中如果底层数组容量不够会触发扩容机制 对于list=append(list,1,2,3)，会有下面代码步骤: 获取旧的长度，容量，如果新的长度大于容量，则进行扩容growslice *(ptr+len) = 1 等一系列操作，给新的切片赋值 // slice = append(slice, 1, 2, 3) a := \u0026slice ptr, len, cap := slice newlen := len + 3 if uint(newlen) \u003e uint(cap) { newptr, len, newcap = growslice(slice, newlen) vardef(a) *a.cap = newcap *a.ptr = newptr } newlen = len + 3 *a.len = newlen *(ptr+len) = 1 *(ptr+len+1) = 2 *(ptr+len+2) = 3 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:3:3","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"扩容逻辑 如果期望的容量大于当前容量的两倍，新的容量就是期望的容量 如果当前的切片的长度小于1024，新的容量翻倍 如果当前的切片的长度大于1024，就会每次增加容量的25%，直到新容量大于期望容量 // /usr/local/go/src/runtime/slice.go newcap := old.cap doublecap := newcap + newcap if cap \u003e doublecap { newcap = cap } else { if old.cap \u003c 1024 { newcap = doublecap } else { // Check 0 \u003c newcap to detect overflow // and prevent an infinite loop. for 0 \u003c newcap \u0026\u0026 newcap \u003c cap { newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u003c= 0 { newcap = cap } } } ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:4:0","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"内存对齐 关于内存对齐是怎么回事，专门有篇文章探讨内存对齐 上面的步骤只是初步计算所需要的容量，还需要内存对齐计算精确的内存大小。如果切片的元素所占有的字节位1，8(sys.PtrSize)，2的幂数，需要内存对齐，过程如下 var overflow bool var lenmem, newlenmem, capmem uintptr // Specialize for common values of et.size. // For 1 we don't need any division/multiplication. // For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch { case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) \u003e maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) \u003e maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): var shift uintptr if sys.PtrSize == 8 { // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) \u0026 63 } else { shift = uintptr(sys.Ctz32(uint32(et.size))) \u0026 31 } lenmem = uintptr(old.len) \u003c\u003c shift newlenmem = uintptr(cap) \u003c\u003c shift capmem = roundupsize(uintptr(newcap) \u003c\u003c shift) overflow = uintptr(newcap) \u003e (maxAlloc \u003e\u003e shift) newcap = int(capmem \u003e\u003e shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) } roundupsize(uintptr(newcap) * sys.PtrSize)会将将要申请的内存向上取整得到capmem，新的容量为 newcap = int(capmem / sys.PtrSize) 实际怎样，我们用dlv调试下 ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:5:0","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"调试 func main() { var arr []int64 fmt.Println(cap(arr)) arr = append(arr, 1, 2, 3, 4, 5) fmt.Println(cap(arr)) } cat3306/gosrclearn/slice ▶ dlv debug Type 'help' for list of commands. (dlv) ▶ dlv debug Type 'help' for list of commands. (dlv) break runtime.growslice Breakpoint 1 set at 0x1047fea for runtime.growslice() /usr/local/go/src/runtime/slice.go:162 (dlv) (dlv) continue \u003e runtime.growslice() /usr/local/go/src/runtime/slice.go:162 (hits goroutine(1):5 total:9) (PC: 0x1047fea) Warning: debugging optimized function 157: // NOT to the new requested capacity. 158: // This is for codegen convenience. The old slice's length is used immediately 159: // to calculate where to write new values during an append. 160: // TODO: When the old backend is gone, reconsider this decision. 161: // The SSA backend might prefer the new length or to return only ptr/cap and save stack space. =\u003e 162: func growslice(et *_type, old slice, cap int) slice { 163: if raceenabled { 164: callerpc := getcallerpc() 165: racereadrangepc(old.array, uintptr(old.len*int(et.size)), callerpc, funcPC(growslice)) 166: } 167: if msanenabled { (dlv) print old runtime.slice {array: unsafe.Pointer(0x0), len: 0, cap: 0} (dlv) print cap 5 (dlv) \u003e runtime.growslice() /usr/local/go/src/runtime/slice.go:219 (PC: 0x1048085) Warning: debugging optimized function 214: newcap = int(capmem) 215: case et.size == sys.PtrSize: 216: lenmem = uintptr(old.len) * sys.PtrSize 217: newlenmem = uintptr(cap) * sys.PtrSize 218: capmem = roundupsize(uintptr(newcap) * sys.PtrSize) =\u003e 219: overflow = uintptr(newcap) \u003e maxAlloc/sys.PtrSize 220: newcap = int(capmem / sys.PtrSize) 221: case isPowerOfTwo(et.size): 222: var shift uintptr 223: if sys.PtrSize == 8 { 224: // Mask shift for better code generation. (dlv) print capmem 48 (dlv) (dlv) \u003e runtime.growslice() /usr/local/go/src/runtime/slice.go:255 (PC: 0x10481eb) Warning: debugging optimized function 250: // 251: // func main() { 252: // s = append(s, d, d, d, d) 253: // print(len(s), \"\\n\") 254: // } =\u003e 255: if overflow || capmem \u003e maxAlloc { 256: panic(errorString(\"growslice: cap out of range\")) 257: } 258: 259: var p unsafe.Pointer 260: if et.ptrdata == 0 { (dlv) print newcap 6 由上面细节可知，预期容量是40(5*8)，但是由于内存对齐的逻辑，capmem是48，所以新的容量长度为6 拷贝切片 copy(dst, src []Type) // /usr/local/go/src/runtime/slice.go func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int { if fromLen == 0 || toLen == 0 { return 0 } n := fromLen if toLen \u003c n { n = toLen } if width == 0 { return n } size := uintptr(n) * width if raceenabled { callerpc := getcallerpc() pc := funcPC(slicecopy) racereadrangepc(fromPtr, size, callerpc, pc) racewriterangepc(toPtr, size, callerpc, pc) } if msanenabled { msanread(fromPtr, size) msanwrite(toPtr, size) } if size == 1 { // common case worth about 2x to do here // TODO: is this still worth it with new memmove impl? *(*byte)(toPtr) = *(*byte)(fromPtr) // known to be a byte pointer } else { memmove(toPtr, fromPtr, size) } return n } 延伸阅读 Arrays, slices (and strings): The mechanics of ‘append’ Go Slices: usage and internals Array vs Slice: accessing speed ","date":"2022-03-26","objectID":"/2022/03/golang/go-slice/:5:1","tags":["golang"],"title":"go slice","uri":"/2022/03/golang/go-slice/"},{"categories":null,"content":"连接数据库 $ mysql -u root -p 表结构语句 创建 CREATE TABLE `forbidden_msg_from_aliyun` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT '自增id', `words` varchar(128) NOT NULL DEFAULT '' COMMENT '脏字', `level` varchar(36) NOT NULL DEFAULT '' COMMENT '级别:block,review', `label` varchar(36) NOT NULL DEFAULT '' COMMENT '类型', `words_state` int(4) NOT NULL DEFAULT 0 COMMENT '1:use,0:del', `vtype` int(4) NOT NULL DEFAULT 0 COMMENT 'TypeAliYunRealLexicon:4', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`id`), UNIQUE KEY `vtype` (`words`,`vtype`), KEY `forbidden_msg_vtype` (`vtype`), KEY `level_index` (`level`), KEY `label_index` (`label`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='阿里云词库'; 已有表结构加字段 多个字段 ALTER TABLE app_update ADD COLUMN `app_version` varchar(16) NOT NULL default '' COMMENT 'app 版本号', ADD COLUMN `update_app_type` tinyint(4) NOT NULL default 0 COMMENT '0:不限制咕咚app版本,1:大于等于某个版本,2:小于等于某个版本'; 单个字段 ALTER TABLE ota_conf ADD COLUMN `expr` text COMMENT 'user_id 尾号或者user_id用逗号隔开'; 已有表结构加(减)索引 多个普通索引 ALTER table modified_nick_log ADD INDEX idx_user_id(user_id), ADD INDEX idx_new_nick(new_nick); 单个唯一联合索引 ALTER table extend_new_user_profile ADD unique unqi_user_id_name_id_card(`user_id`,`encrypt_id_card`,`encrypt_real_name`); 删除索引 ALTER table extend_new_user_profile DROP INDEX uniq_index_user_id; 单个索引 ALTER table modified_nick_log ADD INDEX idx_user_id(user_id); 单个唯一索引 ALTER table modified_nick_log ADD unique unqi_user_id(user_id); 普通联合索引 ALTER table extend_new_user_profile ADD INDEX idx_user_id_name_id_card(`user_id`,`encrypt_id_card`,`encrypt_real_name`); 展示数据库中的表 SHOW TABLES LIKE 'user%'; # or SHOW TABLES FROM winner_chicken_dinner LIKE 'user%'; # or SHOW TABLES FROM winner_chicken_dinner； 展示表中的详细信息 mysql\u003e show create table user_profile \\G Create Table: CREATE TABLE `user_profile` ( `id` int unsigned NOT NULL AUTO_INCREMENT, `nick_name` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT '', `pwd` varchar(256) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT '', `email` varchar(36) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT '', `create_time` datetime NOT NULL, `update_time` datetime NOT NULL, `user_id` varchar(36) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL, `head_icon` int NOT NULL, `level` int NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uniq_email` (`email`), UNIQUE KEY `uniq_nick_name` (`nick_name`) USING BTREE, UNIQUE KEY `uniq_user_id` (`user_id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 数据库语句 查看当前数据库 SELECT DATABASE(); 显示当前时间、用户名、数据库版本 SELECT now(), user(), version(); 创建数据库 CREATE DATABASE IF NOT EXISTS RUNOOB DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 查看已有的库 SHOW DATABASES like 'win%'; # or SHOW DATABASES; 查看数据库信息 SHOW CREATE DATABASE winner_chicken_dinner 删除库 drop database RUNOOB; 选择数据库 use winner_chicken_dinner; ","date":"2022-03-25","objectID":"/2022/03/mysql/mysql-cli/:0:0","tags":["mysql"],"title":"mysql command","uri":"/2022/03/mysql/mysql-cli/"},{"categories":null,"content":"shell #查看存在的shell cat /etc/shells #查看正在使用的shell echo $SHELL #切换shell chsh -s /usr/bin/fish ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:1:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"adduser adduser 和 useradd 是一个意思的命令，用于增加一个linux系统的用户账号添加 参数如下： -c comment 指定一段描述，保存在 /etc/passwd 的备注栏 -d 指定用户的主目录，如果目录不存在可以用-m，创建 -g 用户组 指定用户所属的用户组 -s shell，指定登录时用的shell -u 指定用户的id号 -r 创建系统账号 -m 自动创建用户的家目录 -M 不要自动创建用户的家目录 -e 指定用户的失效时间，日期格式为MM/DD/YY。默认永久有效 -G：指定用户所属的附加群组。 useradd -u 520 -d /usr/myhome -g users -m myhome #创建一个myhome用户uid 520 家目录是/usr/myhome,-m是自动创建家目录 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:2:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"bc 通用计算器 “+” 加法 “-” 减法 “*” 乘法 “/” 除法 “^” 指数 “%” 余数 $ echo \"15+5\" | bc 20 $ echo \"scale=2;(2.777-1.4744)/1\" | bc 1.30 ## scale 设置小数点 $ echo \"obase=2;ibase=10;111\" | bc 1101111 #obase=2(输出二进制),ibase=10(输入十进制),110是输入参数 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:3:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"cat 命令用于连接文件并打印到标准输出设备上 #对文件由1开始编号输出 $ cat -n file_name #文件1的类容加上行号输入文件2 $ cat -n file_name_1 \u003e file_name_2 #对文件由1开始编号输出,空白行不做处理 $ cat -b file_name #将文件1，文件2(加上行号，空白不加行号)追加到文件3 $ cat -b file_name_1 file_name_2 \u003e\u003e file_name_3 #清空文件内容 $ cat /dev/null \u003e file_name ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:4:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"chgrp 更改文件或者目录的所属群组 #查看所有群组 $ groups #更改文件或者目录群组 并打印执行过程 $ chgrp -v group_name file_name #递归更改文件或者目录群组 并打印执行过程 $ chgrp -v -R group_name file_name ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:5:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"chmod #“a”表示所有用户，“u”表示创建者、“g”表示创建者同组用户、“o”表示其他用户；“+”表示添加权限，“-”表示取消权限；“r”表示读权限、“w”表示写权限、“x”表示写权限。 #当前文件夹下所有的文件追加执行权限，u 表示创建者 chmod u+x ./* ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:6:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"chown chown(change owner) 设置文件或文件夹的所有者和关联组，该命令只能root执行 #设置所有者为root $ chown root /var/log/go_log/on.log #设置所有者和所属群组为root $ chown root:root /var/log/go_log/on.log #设置/var/log/go_log 下所有子文件和文件夹的所有者为joker，所属的组joker $ chown -R joker:joker /var/log/go_log #设置当前文件下所有子文件和文件夹的所有者为joker，所属的组joker $ chown -R joker:joker * ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:7:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"cut cut用于截取每行的类容 -b ：以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。 -c ：以字符为单位进行分割。 -d ：自定义分隔符，默认为制表符。 -f ：与-d一起使用，指定显示哪个区域。 -n ：取消分割多字节字符。仅和 -b 标志一起使用。如果字符的最后一个字节落在由 -b 标志的 List 参数指示的 范围之内，该字符将被写出；否则，该字符将被排除 配合grep可提取关键字符串，常用于日志提取，列如下面这个日志我想提取user_id:xxx,routeId:xxx 2021-04-05 09:08:17.155 INFO 1eb829c [go:4661107/730] ▶ api_server.go:3110 PostSportOss PostSportOss ok,user_id:07f438bb-28b5-4f37-a6e7-8312ff90cabc,ossKey:sport/07f438bb-28b5-4f37-a6e7-8312ff90cabc/2021-04-05T09.08.16/4B3cPIr7b2R3q1iY9z,routeId:074ae03c-95ab-11eb-a48a-0178905c8dc0,userAgent:map[device_type:iphone iner_version:4470 ip: platform:0 platform_version:13.6 platfrom_version: version:9.48.1] 可用和grep搭配 grep \"PostSportOss ok\" gps_data_server.log.2021-04-05* | grep \"platform:0\" | cut -d ',' -f 2,4 user_id:07f438bb-28b5-4f37-a6e7-8312ff90cabc,routeId:074ae03c-95ab-11eb-a48a-0178905c8dc0 cut -d 表示用自定义分割符 , -f 表示哪个域，1表示：以, 分割的第一个子串,2表示分割的第二个子串，例子中第二个子串是user_id:07f438bb-28b5-4f37-a6e7-8312ff90cabc 依次类推 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:8:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"du (全称:disk usage) 参数说明 -a或-all 显示目录下所有的文件,文件夹 -h或--human-readable 用k,m,g显示，人类读取友好 -s或--summarize 仅显示统计 例子 $ du -a -h | grep /.git\\$ #22M ./chengxiaowei_blog/.git $ du -s -h 45M . ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:9:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"find find 指定目录下查找文件或者文件夹 $ find path -name \"file_name\" ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:10:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"rm ## 直接删除，不需要询问 $ rm -f path or file ## 递归强删 $ rm -rf path or file ## 找出目录下的所有后缀为.go并强删除 $ find path -name \".go\" | xargs rm -rf ## 过滤 '1.md' 删除所有文件 $ find * | grep -v '1.md' | xargs rm ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:11:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"ssh ## 生成公私钥 $ ssh-keygen -t rsa ssh -V #查看版本 OpenSSH_6.4p1, OpenSSL 1.0.1e-fips 11 Feb 2013 #链接我那台ubuntu ssh joker@192.168.0.104 #指定端口链接 ssh -p 2223 joker@192.168.0.104 #可以用域名代替ip vim /etc/hosts ssh joker 修改默认端口 sudo vim /etc/ssh/sshd_config 工作流程 使用简单防火墙禁用22端口,开放2223 sudo ufw deny 22 sudo ufw allow 2223 禁止密码登录,钥匙登录 vim /etc/ssh/sshd_config PasswordAuthentication no PubkeyAuthentication yes 将本地的公钥上传到服务器的这个文件中 ~/.ssh/authorized_keys 流程就是本地ssh服务用~/.ssh/id_rsa 私钥将一个随机字符串加密，发到服务器，服务器将你的公钥解密 ssh use .pem file $ ssh -i ~/Downloads/demo.pem root@192.168.0.100 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:12:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"tar 压缩 touch a.c tar -czvf test.tar.gz a.c //压缩 a.c文件为test.tar.gz 解压 tar -xzvf test.tar.gz 参数 -c或--create 建立新的备份文件 -z或--gzip 通过gzip指令处理备份文件 -v或--verbose 显示指令执行过程 -f或--file 指定备份文件 -x或--extract或--get 从备份文件还原文件 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:13:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"top 命令 用于实时显示各个进程的状态 工作流程 界面 第一行： 系统时间:16:49:59 运行时间:up 627 days ，22:54 //627天22小时54分 当前登录的用户:1 user //一个用户 load avage :负载均衡0.61,0.83,0.95 三个数字分别表示1分钟，5分钟，15分钟的负载情况 第二行:进程信息 Tasks 82 total :总量有82进程在跑 5 running :活跃的有5个 74 sleeping :74个休眠 0 stopped ：0个停止 3 zombie：3个僵尸进程 第三行:cpu状态 8.1 us (user space) –用户空间cpu的百分比 16.2 sy (sysctl) – 内核cpu的百分比 0.0 ni ()– 改变过优先级的进程cpu的百分比 75.8 id (idolt) –空闲cpu的百分比 0.0 wa (wait) –IO 等待cpu的百分比 0.0 hi (Hardware IRQ) –硬中断cpu的百分比 0.0 si (Software Interrupts) –软中断cpu的百分比 第四行: 内存状态（Kib Mem） 1882232 total: 总计大约1838M内存 303244 free: 大约有296M空闲内存 523444 used : 大约用了511M 1055544 buff/cache: 1030M 缓冲区 第五行: 交换内存情况(Kib Swap) 第七行:各个进程的状态监控 PID –进程id USER –进程的所有者 PR –进程优先级 NI – nice 值。小于0表示高优先级，大于0表示低优先级 VIRT– 进程使用的虚拟进程总量 RES –进程使用的，未被换出的物理内存 SHR –共享内存 S –进程状态.D =不可中断的睡眠状态，R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程 %CPU – cpu占用量 %MEM –内存占用量 TIME+ – 使用cpu时间 单位1/100 秒 CMMAND–进程名字 top 运行中可以通过 top 的内部命令对进程的显示方式进行控制。内部命令如下： s – 改变画面更新频率 l – 关闭或开启第一部分第一行 top 信息的表示 t – 关闭或开启第一部分第二行 Tasks 和第三行 Cpus 信息的表示 m – 关闭或开启第一部分第四行 Mem 和 第五行 Swap 信息的表示 N – 以 PID 的大小的顺序排列表示进程列表 P – 以 CPU 占用率大小的顺序排列进程列表 M – 以内存占用率大小的顺序排列进程列表 h – 显示帮助 n – 设置在进程列表所显示进程的数量 q – 退出 top s – 改变画面更新周期 根据第七行的各个字段来排序 进入top。默认是cpu占用量排序的，怎么改变其他字段排序？ 敲击键盘 “b”(打开或关闭加亮效果)，效果如下,此时高亮的是running的运行状态的进程，可以键入y 键关闭或打开运行状态(running)的进程高亮效果: 工作流程 键入x 键(打开或关闭列的排序高亮效果)此时是按照cpu占用量这个列排序，效果如下 工作流程 可以键入 shift + \u003c 右移动高亮列，shift+\u003e 左移动高亮列，效果如下: 工作流程 工作流程 top -p pid #根据pid搜索进程 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:14:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"uniq Linux uniq 命令用于检查及删除文本文件中重复出现的行列，一般与 sort 命令结合使用。 uniq 可检查文本文件中重复出现的行列。 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:15:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"vim 命令 vim test.txt :q! :q :wq :set nu #设置行号 :set nu! #取消行号 快捷键 gg //连按两下键盘g到文件顶部 shift+gg //按住shift，连按两次g到文件底部 0 //键盘上的0键，跳到句首 shift+$ //跳到句尾 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:16:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"wc wc 用于计算文件的byte，字数，或是列数 wc -c或–byte或–chars 只显示byte wc -l 或–lines 显示行数 wc –help 在线帮助 wc –version 显示版本信息 ⋊\u003e ~/Desktop wc -l swim.txt 1613 swim.txt #swim.txt 有1613行 #统计有多少个错误 grep \"ERRO\" /var/log/go_log/tcp_game.log | wc ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:17:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"grep ## -C10 匹配行和它的前后10行 grep -C10 panic userinfo_center.out.2021-11-02.07:28:34 ## -B 匹配行和它的前10行 grep -C10 panic userinfo_center.out.2021-11-02.07:28:34 ## -A 匹配行和它的后10行 grep -A10 panic userinfo_center.out.2021-11-02.07:28:34 过滤大于500ms的sql日志 grep -E '([5-9][0-9]{2}|[0-9]{4,}).[0-9]{3}ms' ak-user-info-sql_WARN.log 2024-01-21 06:00:12.874 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/mobile_user_speed_time.go:46 SLOW SQL \u003e= 200ms[1765.106ms] [rows:1] 2024-01-21 06:00:12.880 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[778.238ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 1 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 06:00:12.881 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[759.098ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 0 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 06:00:12.888 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[663.101ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 0 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 12:54:18.589 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:78 SLOW SQL \u003e= 200ms[660.299ms] [rows:1] SELECT id,nick_name,user_account,user_status,head_image_url,start_mobile_time FROM `yebao_user_prod`.`user_infos` WHERE user_id = '9788226285568' 2024-01-21 13:24:53.178 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[738.466ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 1 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 14:09:49.139 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:101 SLOW SQL \u003e= 200ms[779.674ms] [rows:0] 2024-01-21 14:50:39.152 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[658.196ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 1 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 17:13:51.964 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[693.042ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 0 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 20:51:53.990 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/free_limit_conf.go:32 SLOW SQL \u003e= 200ms[526.559ms] [rows:1] SELECT * FROM `yebao_game_prod`.`mobile_free_limit_conf` WHERE os = 0 ORDER BY `mobile_free_limit_conf`.`id` DESC LIMIT 1 2024-01-21 23:02:29.098 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:78 SLOW SQL \u003e= 200ms[672.740ms] [rows:1] SELECT id,nick_name,user_account,user_status,head_image_url,start_mobile_time FROM `yebao_user_prod`.`user_infos` WHERE user_id = '5917042364416' 2024-01-22 19:10:34.677 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:78 SLOW SQL \u003e= 200ms[593.420ms] [rows:1] SELECT id,nick_name,user_account,user_status,head_image_url,start_mobile_time FROM `yebao_user_prod`.`user_infos` WHERE user_id = '8759533686784' 2024-01-23 01:35:27.799 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:78 SLOW SQL \u003e= 200ms[547.189ms] [rows:1] SELECT id,nick_name,user_account,user_status,head_image_url,start_mobile_time FROM `yebao_user_prod`.`user_infos` WHERE user_id = '0927665164288' 2024-01-23 14:01:51.064 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:78 SLOW SQL \u003e= 200ms[626.121ms] [rows:1] SELECT id,nick_name,user_account,user_status,head_image_url,start_mobile_time FROM `yebao_user_prod`.`user_infos` WHERE user_id = '7936392409088' 2024-01-23 18:56:20.766 WARN /home/cat13/go/src/cloud/AK/ak-user-info/models/user_info.go:78 SLOW SQL \u003e= 200ms[590.405ms] [rows:1] SELECT id,nick_name,user_account,user_status,head_image_url,start_mobile_time ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:18:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"SS ## 进程id pid 端口号 进程名 $ ss -tunlp | grep 6666 LISTEN 0 128 :::6666 :::* users:((\"shoe\",28547,18)) ## 统计链接服务器的链接数，9095为服务器端口 ss | grep 9095 |wc -l ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:19:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"netstat #网络监听，使用端口情况 netstat -tulnp -t -显示TCP端口 -u -显示UDP端口 -n -显示数字地址而不是主机 -l - 仅显示监听地址端口 -p - 显示进程的PID和名称，仅当root 或者sudo用户身份，才会显示信息 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:20:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"scp -1： 强制scp命令使用协议ssh1 -2： 强制scp命令使用协议ssh2 -4： 强制scp命令只使用IPv4寻址 -6： 强制scp命令只使用IPv6寻址 -B： 使用批处理模式（传输过程中不询问传输口令或短语） -C： 允许压缩。（将-C标志传递给ssh，从而打开压缩功能） -p：保留原文件的修改时间，访问时间和访问权限。 -q： 不显示传输进度条。 -r： 递归复制整个目录。 -v：详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。 -c cipher： 以cipher将数据传输进行加密，这个选项将直接传递给ssh。 -F ssh_config： 指定一个替代的ssh配置文件，此参数直接传递给ssh。 -i identity_file： 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。 -l limit： 限定用户所能使用的带宽，以Kbit/s为单位。 -o ssh_option： 如果习惯于使用ssh_config(5)中的参数传递方式， -P port：注意是大写的P, port是指定数据传输用到的端口号 -S program： 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。 本地–\u003e远端 $ scp -r ./* joker@192.168.0.102:~/Desktop/oldimg $ scp test.png joker@192.168.0.102:~/Desktop/test.png $ scp test.png 192.168.0.102:~/Desktop/oldimg $ scp test.png 192.168.0.102:~/Desktop/test.png 远端–\u003e本地 $ scp -r joker@192.168.0.102:~/Desktop/oldimg ~/oldimg $ scp joker@192.168.0.102:~/Desktop/test.png ~/test.png $ scp -r 192.168.0.102:~/Desktop/oldimg ~/oldimg $ scp 192.168.0.102:~/Desktop/test.png ~/test.png ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:21:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"sudo ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:22:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"免密 $ sudo chmod 640 /etc/sudoers $ vim /etc/sudoer $ echo 'cat13 ALL=(ALL) NOPASSWD:ALL' \u003e\u003e /etc/sudoers $ chmod 440 /etc/sudoers #or $ cd /etc/sudoers.d/ $ vim cat13 #添加 'cat13 ALL=(ALL) NOPASSWD:ALL' ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:22:1","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"lsblk 列出块设备信息 $ lsblk -h Usage: lsblk [options] [\u003cdevice\u003e ...] List information about block devices. Options: -D, --discard print discard capabilities -E, --dedup \u003ccolumn\u003e de-duplicate output by \u003ccolumn\u003e -I, --include \u003clist\u003e show only devices with specified major numbers -J, --json use JSON output format -O, --output-all output all columns -P, --pairs use key=\"value\" output format -S, --scsi output info about SCSI devices -T, --tree[=\u003ccolumn\u003e] use tree format output -a, --all print all devices -b, --bytes print SIZE in bytes rather than in human readable format -d, --nodeps don't print slaves or holders -e, --exclude \u003clist\u003e exclude devices by major number (default: RAM disks) -f, --fs output info about filesystems -i, --ascii use ascii characters only -l, --list use list format output -M, --merge group parents of sub-trees (usable for RAIDs, Multi-path) -m, --perms output info about permissions -n, --noheadings don't print headings -o, --output \u003clist\u003e output columns -p, --paths print complete device path -r, --raw use raw output format -s, --inverse inverse dependencies -t, --topology output info about topology -w, --width \u003cnum\u003e specifies output width as number of characters -x, --sort \u003ccolumn\u003e sort output by \u003ccolumn\u003e -z, --zoned print zone model --sysroot \u003cdir\u003e use specified directory as system root -h, --help display this help -V, --version display version Available output columns: NAME device name KNAME internal kernel device name PATH path to the device node MAJ:MIN major:minor device number FSAVAIL filesystem size available FSSIZE filesystem size FSTYPE filesystem type FSUSED filesystem size used FSUSE% filesystem use percentage FSROOTS mounted filesystem roots FSVER filesystem version MOUNTPOINT where the device is mounted MOUNTPOINTS all locations where device is mounted LABEL filesystem LABEL UUID filesystem UUID PTUUID partition table identifier (usually UUID) PTTYPE partition table type PARTTYPE partition type code or UUID PARTTYPENAME partition type name PARTLABEL partition LABEL PARTUUID partition UUID PARTFLAGS partition flags RA read-ahead of the device RO read-only device RM removable device HOTPLUG removable or hotplug device (usb, pcmcia, ...) MODEL device identifier SERIAL disk serial number SIZE size of the device STATE state of the device OWNER user name GROUP group name MODE device node permissions ALIGNMENT alignment offset MIN-IO minimum I/O size OPT-IO optimal I/O size PHY-SEC physical sector size LOG-SEC logical sector size ROTA rotational device SCHED I/O scheduler name RQ-SIZE request queue size TYPE device type DISC-ALN discard alignment offset DISC-GRAN discard granularity DISC-MAX discard max bytes DISC-ZERO discard zeroes data WSAME write same max bytes WWN unique storage identifier RAND adds randomness PKNAME internal parent kernel device name HCTL Host:Channel:Target:Lun for SCSI TRAN device transport type SUBSYSTEMS de-duplicated chain of subsystems REV device revision VENDOR device vendor ZONED zone model DAX dax-capable device For more details see lsblk(8). $ lsblk -Jnbo NAME,MAJ:MIN,RM,SIZE,RO,TYPE,MOUNTPOINT,TRAN ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:23:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"apt 全称Advanced Packaging Tool,Debian 和 Ubuntu 的包管理软件 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:24:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"rsync rsync is an open source utility that provides fast incremental file transfer Rsync是一个开源实用程序，提供快速增量文件传输 #拷贝某个忽略某个文件夹 rsync -rv --exclude=.git ../Joker_null/ ./ ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:25:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"crontab 定时任务 编辑定时任务配置 crontab -e 重载定时任务配置 service cron reload ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:26:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"IP socket_detail 更改目标地址112.16.229.104 走ens3网卡 ip route add 112.16.229.104 via 0.0.0.0 dev ens3 ","date":"2022-03-25","objectID":"/2022/03/linux/linux-cli/:27:0","tags":["linux"],"title":"linux cli","uri":"/2022/03/linux/linux-cli/"},{"categories":null,"content":"准备 git，服务器(ubuntu为例)，ssh服务 ","date":"2022-03-25","objectID":"/2022/03/git/raw/:0:1","tags":["git"],"title":"git raw","uri":"/2022/03/git/raw/"},{"categories":null,"content":"创建一个git用户组和用户 $ useradd -m git #添加用户，-m创建用户文件夹 $ sudo passwd git #设置密码 $ sudo su git #切换git用户，输入密码 $ chsh -s /bin/bash #指定shell ","date":"2022-03-25","objectID":"/2022/03/git/raw/:0:2","tags":["git"],"title":"git raw","uri":"/2022/03/git/raw/"},{"categories":null,"content":"收集远端的公钥 把用户的公钥导入 /home/git/.ssh/authorized_keys 一行一个 $ cd /home/git $ mkdir .ssh \u0026\u0026 chmod 755 .shh $ touch authorized_keys ","date":"2022-03-25","objectID":"/2022/03/git/raw/:0:3","tags":["git"],"title":"git raw","uri":"/2022/03/git/raw/"},{"categories":null,"content":"创建仓库目录 $ mkdir -p repositories \u0026\u0026 cd repositories $ git init --bare test.git ","date":"2022-03-25","objectID":"/2022/03/git/raw/:0:4","tags":["git"],"title":"git raw","uri":"/2022/03/git/raw/"},{"categories":null,"content":"注意事项 ssh 服务需要配置 $ vim /stc/ssh/sshd_config #or vim /stc/ssh/ssh_config #将权限放开 RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys 出于安全考虑，git用户不允许登录shell，可以编辑/etc/passwd 完成 git: x:1001:1001::/home/git:/bin/bash #改成 git: x:1001:1001::/home/git:/bin/git-shell 必须创建git用户搭建，并且ssh鉴权在git用户的家目录下 远端可以拉仓库了 git@192.168.0.104:/home/git/repositories/test.git ","date":"2022-03-25","objectID":"/2022/03/git/raw/:0:5","tags":["git"],"title":"git raw","uri":"/2022/03/git/raw/"},{"categories":null,"content":"官网 ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:1","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"创建 #当前目录下创建git仓库 $ git init #? 文件名字 $ git init ? #? url 或者 ssh地址 $ git clone ? ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:2","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"配置 #显示当前git 配置 $ git config --list #设置提交代码时名字 $ git config --global user.name \"\" #设置提交代码邮箱 $ git config --global user.email \"\" #设置编辑器 $ git config --global core.editor vim #包拉取(尤其是go module)的时候用ssh方式代替http方式 [url \"ssh://git@github.com/\"] insteadOf = https://github.com/ ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:3","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"增加和删除 #添加指定文件到缓存区 $ git add file1 file2 #添加当前目录下所有的的文件 $ git add . #添加指定文件夹包括子文件到缓存区 $ git add dir # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm file1 file2 ... # 暂存区删除指定文件，但该文件会保留在工作区 $ git rm --cached file # 改名文件，并且将这个改名放入暂存区 $ git mv file-original file-renamed ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:4","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"代码提交 # 提交暂存区到仓库区,?提交原因 $ git commit -m \"?\" # 提交暂存区的指定文件到仓库区 $ git commit file1 file2 ... -m \"?\" # 对于那些已经存在的文件修改，可以一步到仓库区，不需要git add. # 但是对于新增文件 还是要 git add file 再 git commit \"\" $ git commit -a -m \"?\" # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 # 场景，刚刚git commit后,发现又有东西要改 $ git commit --amend -m '?' # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend file1 file2 ... ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:5","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # ?分支名,新建一个分支，但依然停留在当前分支 $ git branch ? # ?分支名,新建一个分支，并切换到该分支 $ git checkout -b ? # ?1分支名,?2commit,新建一个分支，指向指定commit $ git branch ?1 ?2 # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch-name] [remote-branch-name] # ?分支名，切换到指定分支，并更新工作区 $ git checkout ? # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch-name] [remote-branch-name] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:6","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push origin v1 # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:7","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其\"提交说明\"必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat \"@{0 day ago}\" # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示所有的git 操作 包括切分支，pull push，reset 等。 # 可以找到丢失的commit号 $ git reflog ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:8","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:9","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区,⚠️使用前确定好 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] #暂时将未提交的变化移除，稍后再移入 $ git stash # 将暂存的内容弹出 $ git stash pop #列出暂存内容 $ git stash list #清除暂存内容 $ git stash clear ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:10","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"命令以英文的方式展示 LANG=en_GB git echo \"alias git='LANG=en_GB git'\" \u003e\u003e ~/.bash ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:11","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"流程图 ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:12","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"fork后的仓库怎么与原仓库同步 删掉仓库，重新fork 设置上游代码库，进行merge $ cd ~/go/src/github.com/genDopamine ##查看远端链接 $ git remote -v origin https://github.com/genDopamine/gin (fetch) origin https://github.com/genDopamine/gin (push) ##设置upstream $ git remote add upstream git@github.com:gin-gonic/gin.git ##再次查看git remote -v $ git remote -v origin https://github.com/genDopamine/gin (fetch) origin https://github.com/genDopamine/gin (push) upstream git@github.com:gin-gonic/gin.git (fetch) upstream git@github.com:gin-gonic/gin.git (push) ##更新原仓库的类容 $ git fetch upstream #合并原仓库类容 $ git merge upstream/master #push本仓库 $ git push ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:13","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"社区书 git community book ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:0:14","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"git 子模块 ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:1:0","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"添加子模块 git submodule add git@github.com:cat3306/cat3306.github.io.git ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:1:1","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"更新子模块 git submodule update --remote ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:1:2","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"子模块push cd my_submodule git add . git commit -m \"your message\" git push origin master ","date":"2022-03-25","objectID":"/2022/03/git/git-cli/:1:3","tags":["git"],"title":"git command","uri":"/2022/03/git/git-cli/"},{"categories":null,"content":"最大子序列和 立方复杂度 #include\u003cstdio.h\u003e int MaxSubSeqSum(const int A[],int N){ int thisSum,maxSum,i,j,k; maxSum=0; for(i=0;i\u003cN;i++){ for(j=i;j\u003cN;j++){ thisSum=0; for(k=i;k\u003c=j;k++){ thisSum+=A[k]; } if (thisSum\u003emaxSum){ maxSum=thisSum; } } } return maxSum; } int main(){ int a[6] ={-2,11,-4,13,-5,-2}; int max=MaxSubSeqSum(a,6); printf(\"%d\\n\",max); } 平方复杂度 #include \u003cstdio.h\u003e int MaxSubSeqSum(const int A[], int len) { int maxSum = -100; for (int i = 0; i \u003c len; i++) { int thisSum = 0; for (int j = i; j \u003c len; j++) { thisSum += A[j]; if (thisSum \u003e maxSum) { maxSum = thisSum; } } } return maxSum; } int main() { int a[6] = {-2, 11, -4, 13, -5, -2}; printf(\"%d\\n\", MaxSubSeqSum(a, 6)); return 0; } NlogN 复杂度 #include \u003cstdio.h\u003e int max3(int a, int b, int c) { int max = a \u003e b ? a : b; return max \u003e c ? max : c; } int MaxSubSeqSum(const int A[], int left, int right) { if (left == right) { return A[left] \u003e 0 ? A[left] : 0; } int mid = (right - left) / 2 + left; MaxSubSeqSum(A, left, mid); MaxSubSeqSum(A, mid + 1, right); int sumLeft = 0; int sumRight = 0; int maxLeft = 0; int maxRight = 0; for (int i = mid; i \u003e= left; i--) { sumLeft += A[i]; if (sumLeft \u003e maxLeft) maxLeft = sumLeft; } for (int i = mid + 1; i \u003c= right; i++) { sumRight += A[i]; if (sumRight \u003e maxRight) maxRight = sumRight; } return max3(maxLeft, maxRight, maxLeft + maxRight); } int main() { int A[6] = {-2, 11, -4, 13, -5, -2}; printf(\"%d\\n\", MaxSubSeqSum(A, 0, 6 - 1)); return 0; } 最后O(n)复杂度 #include \u003cstdio.h\u003e int MaxSubSum(const int A[], int len) { int maxSum, sum; maxSum = sum = 0; for (int i = 0; i \u003c len; i++) { sum += A[i]; if (sum \u003e maxSum) maxSum = sum; else if (sum \u003c 0) sum = 0; } return maxSum; } int main() { int A[6] = {-2, 11, -4, 13, -5, -2}; printf(\"%d\\n\", MaxSubSum(A, 6)); return 0; } ","date":"2022-03-25","objectID":"/2022/03/c-d-s/chapter2/:1:0","tags":["数据结构与算法c描述"],"title":"第二章算法分析","uri":"/2022/03/c-d-s/chapter2/"},{"categories":null,"content":"二分法 #include \u003cstdio.h\u003e int BinarySearch(const int A[], int len, int target) { int left = 0; int right = len - 1; while (left \u003c= right) { int mid = (right - left) / 2 + left; if (A[mid] == target) { return 1; } else if (mid \u003c target) { left = mid + 1; } else { right = mid - 1; } } return 0; } int main() { int A[10] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; printf(\"%d\\n\", BinarySearch(A, 10, 10)); return 0; } ","date":"2022-03-25","objectID":"/2022/03/c-d-s/chapter2/:2:0","tags":["数据结构与算法c描述"],"title":"第二章算法分析","uri":"/2022/03/c-d-s/chapter2/"},{"categories":null,"content":"官网 https://golang.google.cn/ 文档 https://golang.google.cn/cmd/go/ 常见问题 https://golang.google.cn/doc/faq 工具库 https://github.com/golang/tools/ ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:0:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"Go install https://go.dev/dl/ Remove any previous Go installation $ rm -rf /usr/local/go \u0026\u0026 tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz Add /usr/local/go/bin to the PATH environment variable. # vim $HOME/.profile #or # /etc/profile (for a system-wide installation) #Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile. export PATH=$PATH:/usr/local/go/bin source $HOME/.profile # or source /etc/profile Verify that you’ve installed Go by opening a command prompt and typing the following command: $ go version 目录结构 $ tree -L 1 . ├── bin ├── pkg └── src src 编写代码的根目录 pkg go module 模式下 生成的依赖包根目录 bin 放着 go install 生成的二进制文件 ，可用于制作命令工具。由于要在系统的任何地方执行这里面的二进制，需要将 ~/go/bin 加入系统可执行目录中 vim ~/.bash_profile export GOPATH=~/code/go export GOBIN=~/code/go/bin export PATH=$PATH:$GOBIN 当然也可以 go env -w GOPATH=~/code/go go env -w GOBIN=~/code/go/bin 命令 The commands are: bug start a bug report build compile packages and dependencies clean remove object files and cached files doc show documentation for package or symbol env print Go environment information fix update packages to use new APIs fmt gofmt (reformat) package sources generate generate Go files by processing source get add dependencies to current module and install them install compile and install packages and dependencies list list packages or modules mod module maintenance run compile and run Go program test test packages tool run specified go tool version print Go version vet report likely mistakes in packages Use \"go help \u003ccommand\u003e\" for more information about a command. 可直接查阅，例如go help build 常见命令 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:1:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go build # force rebuilding of packages that are already up-to-date. # 强制重新构建二进制 $ go build -a # print the commands but do not run them # 打印编译细节，不执行 $ go build -n # the number of programs, such as build commands or # test binaries, that can be run in parallel. # The default is GOMAXPROCS, normally the number of CPUs available # 编译时设置逻辑核数，以便并发编译。默认CPU逻辑核数 例如8 $ go build -p n # such as go build -p 1 # enable data race detection. # Supported only on linux/amd64, freebsd/amd64, darwin/amd64, darwin/arm64, windows/amd64, # linux/ppc64le and linux/arm64 (only for 48-bit VMA). # 并发竞争检测 $ go build -race # print the commands. # 编译打印编译细节 $ go build -x $ go build #无参数编译，到本目录 $ go build src1.go src2.go ... #指定源码文件编译 $ go build -o ? src1.go src2.go ... #指定二进制文件名 $ go build -o ? #指定二进制文件名 $ go build + package # 指定包名编译 $ go build -v #编译显示包名 $ go build -n #打印编译时所用到的所有命令，但不真正执行 $ go build -x #打印编译时所用到的所有命令 $ go build -a #强制重新构建 $ go build -race #检测go携程竞争 $ go build -gcflags=\"-N -l -S\" #输出汇编 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:2:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"交叉编译 Mac -\u003e Linux $ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build main.go Mac -\u003e Windows $ CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build main.go Linux -\u003e Mac $ CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build main.go Linux -\u003e Windows $ CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build main.go Windows -\u003e Mac SET CGO_ENABLED=0 SET GOOS=darwin SET GOARCH=amd64 go build main.go Windows -\u003e Linux SET CGO_ENABLED=0 SET GOOS=linux SET GOARCH=amd64 go build main.go ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:2:1","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"编译传参 有时候需要将一些版本信息代入二进制中，这个时候就需要go的编译传参，分为两种情况 如果是main包的参数 $ go build -ldflags \"-X main.date=$(date -u +%Y%m%d.%H%M%S) -X main.version=1.0\" main.go //例子 package main import \"fmt\" var date string var version string func main() { fmt.Println(date) fmt.Println(version) } 如果是其它的包，就需要全路径了(相对于GOPATH路径) LDFLAGS += -X \"github.com/pingcap/tidb/parser/mysql.TiDBReleaseVersion=$(shell git describe --tags --dirty --always)\" LDFLAGS += -X \"github.com/pingcap/tidb/util/versioninfo.TiDBBuildTS=$(shell date -u '+%Y-%m-%d %H:%M:%S')\" LDFLAGS += -X \"github.com/pingcap/tidb/util/versioninfo.TiDBGitHash=$(shell git rev-parse HEAD)\" LDFLAGS += -X \"github.com/pingcap/tidb/util/versioninfo.TiDBGitBranch=$(shell git rev-parse --abbrev-ref HEAD)\" LDFLAGS += -X \"github.com/pingcap/tidb/util/versioninfo.TiDBEdition=$(TIDB_EDITION)\" go build $(LDFLAGS) ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:2:2","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go run $ go run *.go #编译，并执行,但是不留下二进制文件。适合单文件，类似脚本执行方式 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:3:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go clean 清除编译后的二进制文件 $ go clean -n #打印要执行的命令，但不执行 $ go clean -x #打印要执行的命令,执行 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:4:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go install 编译后的二进制放到$GOPATH/bin/ $ go install #一般的，在项目的根目录执行 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:5:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go get go get github.com/davyxu/cellnet go get -v github.com/davyxu/cellnet #显示操作流程的日志及信息 go get -u github.com/davyxu/cellne #下载丢失的包，但不会更新已经存在的包 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:6:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go test #测试这个文件下的所有的测试用例 go test helloworld_test.go #测试这个文件下的所有的测试用例，显示详细流程 go test --count=1 -v helloworld_test.go #指定测试文件下的特定测试函数--常用 go test --count=1 -v -run ^TestFunc$ helloworld_test.go #指定测试文件下的特定测试函数(正则匹配有可能多个)--常用 go test --count=1 -v -run TestFunc helloworld_test.go #不使用缓存，指定测试文件下的特定测试函数(正则匹配有可能多个)--常用 go test --count=1 -v -run TestFunc helloworld_test.go # 上面都是gopath方式，如果项目是go mod的，例如 go/src/github.com/golib go test --count=1 -v -run ^TestXxx$ github.com/cat3306/golib/ziputil 环境变量 go env 编译模式 export GO111MODULE=off #用GOPATH编译 export GO111MODULE=on #用go mod编译 ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:7:0","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"go 模块代理 代理 $ go env -w GO111MODULE=on $ go env -w GOPROXY=https://goproxy.cn,direct 有时需要访问私库 $ go env -w GOPRIVATE=\"github.com/cat3306/golib\" 下载包用ssh方式 $ vim .gitconfig [url \"ssh://git@github.com/\"] insteadOf = https://github.com/ 导入fork 的包 go.mod replace github.com/panjf2000/gnet/v2 =\u003e github.com/cat3306/gnet/v2 v2.1.2 docker golang build 用docker容器的golang编译本地代码 需要设置GOPROXY docker run --rm -v \"$PWD\":/go/src/mobile_cron -v \"/Users/joker/code/go/src/cloud/mobile_service_project/mobile_base\":/go/src/mobile_base -w /go/src/mobile_cron -it golang:1.17 bash -c \"go env -w GOPROXY=https://goproxy.cn,direct \u0026\u0026 go build\" ","date":"2022-03-25","objectID":"/2022/03/golang/go-cli/:7:1","tags":["golang"],"title":"go cli","uri":"/2022/03/golang/go-cli/"},{"categories":null,"content":"系统调度 进程调度 最近在看一些操作系统的文章，有篇文章说进程是对cpu的抽象，细细想来确实如此。计算机的问题都可以加一个中间层来解决。程序员写的程序，经过编译器编译(编译性语言)会变成一堆低级的，难以阅读的机器指令。机器按照指令的顺序，取指执行。这个指令集合可以姑且理解为进程，然而操作系统运行着上百个进程，如何管理这些进程变得至关重要。 CPU的速度永远是其他设备无法比拟的，一个进程执行过程中可能会陷入一些比较耗时的操作，比如磁盘IO，这时候就需要把这个进程调走，让其他的进程获取CPU资源，来提升整个系统的性能。随着而来的问题是如何调度这些进程。 线程调度 事实上进程并不是系统调度的实体，线程才是，线程是更小的单位，线程是轻量级进程。这不难理解，一个进程会有很多操作，这些操作是可以并行进行的，有些很消耗CPU，有些是IO费时操作，如果遇到IO的操作，如果系统是调度进程的策略会把整个进程给调走，那么这个进程就直接卡住了，用户体验肯定是糟糕的。 ","date":"2022-02-23","objectID":"/2022/02/golang/go-scheduler/:1:0","tags":["golang"],"title":"go scheduler","uri":"/2022/02/golang/go-scheduler/"},{"categories":null,"content":"线程切换 系统调度线程的依据是线程的状态，线程有三种状态(简化模型)，Waiting，Runnable Executing Waiting 等待状态，线程等待某个事件的返回，例如网络数据的返回，磁盘数据的返回，调用系统API；内存同步访问条件atomic, mutexes等 Runnable 状态，就绪状态，获取CPU资源就可以运行 Executing 正在执行的状态，CPU正在执行的线程 线程可以分为计算型，和IO型。也可以说是CPU密集型，IO密集型。 IO型需要外部资源，如网络数据，磁盘数据，或者是内存的同步访问原语。线程陷入IO操作后，系统调度会让线程跳到Waiting 状态 CPU密集型，做计算任务，这种类型的线程会消耗CPU，例如一个程序执行一个死循环的打印语句，可以观察他的CPU占有率非常的高 对于这两种的线程切换态度是不一样的，CPU密集型，由于一直有指令需要CPU执行。调度策略就不会考虑这种类型的进程。对于IO密集型的线程就不一样了，在等待IO资源的时候，调度策略会把当前线程换下来，让其他的线程使用CPU。 对于系统调度来说，最重要的是尽可能利用CPU，尽可能让所有的CPU核心有事情做 if you have a program that is focused on IO-Bound work, then context switches are going to be an advantage. Once a Thread moves into a Waiting state, another Thread in a Runnable state is there to take its place. This allows the core to always be doing work. This is one of the most important aspects of scheduling. Don’t allow a core to go idle if there is work (Threads in a Runnable state) to be done. 翻译过来就是说，如果程序陷入IO时，上下文切换将是个优点 ","date":"2022-02-23","objectID":"/2022/02/golang/go-scheduler/:1:1","tags":["golang"],"title":"go scheduler","uri":"/2022/02/golang/go-scheduler/"},{"categories":null,"content":"安装 https://www.jetbrains.com/zh-cn/ 卸载 Goland* ~/Library/Preferences/ ~/Library/Caches/ ~/Library/Application Support/ ~/Library/Logs/ 远端编码 插件 Atom Material Icons One Dark ","date":"2022-02-17","objectID":"/2022/02/software/goland/:0:0","tags":["软件"],"title":"Goland","uri":"/2022/02/software/goland/"},{"categories":null,"content":"下载 wireshark mac 必装软件 wireshark官网 ","date":"2022-01-26","objectID":"/2022/01/tool/tcpdump_wireshark/:1:0","tags":["工具"],"title":"抓包工具wireshark 和tcpdump","uri":"/2022/01/tool/tcpdump_wireshark/"},{"categories":null,"content":"使用 服务器上只能用命令tcpdump 来抓取网络包，没有wireshark直观，但是可以tcpdump 可以保存抓包文件，用wireshark打开 ","date":"2022-01-26","objectID":"/2022/01/tool/tcpdump_wireshark/:2:0","tags":["工具"],"title":"抓包工具wireshark 和tcpdump","uri":"/2022/01/tool/tcpdump_wireshark/"},{"categories":null,"content":"官网 https://www.ssh.com/ ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:0:0","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"认识SSH SSH是 Secure Shell 的缩写，是一种的网络协议 ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:1:0","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"应用 SSH 主要用于登录服务器。例如以用户user，登录远程主机host，user是远程主机的用户 $ ssh user@host $ ssh root@192.168.0.5 如果本地用户名远程的用户名一致，登录可以省略用户名 $ ssh host SSH 的默认端口是22，上面的命令没有带端口号就是默认会去连接22端口。 $ ssh -p 2222 user@host SSH 之所以安全的是因为它采用了公私钥加密，非对称加密。如果是密码登录是以下过程 远程主机收到用户的登录请求，把自己的公钥发个用户 用户使用这个公钥，将登录密码加密后，发送个远程主机 远程主机收到加密后的登录密码，用自己的私钥解密，如果密码正确，可登录 整个流程看似安全实际上存在中间人攻击的风险。如果我是一个黑客，我在中间拦截了用户的登录请求，并将我伪造的公钥发给用户。用户将加了密的登录密码发给我，我再用私钥解密，这样我就获取了远程主机的登录密码。所以，HTTPS协议由CA来证明公钥的身份，当然CA也是可以伪装的，这是一个无穷无尽的信任问题。SSH 没有CA机构来认证，都是自己签发。 ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:2:0","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"密码登录 如果是第一次登录，系统会这样提示 The authenticity of host '192.168.1.5 (192.168.1.5)' can't be established. ECDSA key fingerprint is SHA256:0OFCrK6ijuT+GWZ/qLTfnbww8si6AEEeUhKcXnmE5xU. Are you sure you want to continue connecting (yes/no/[fingerprint])? 意思是无法确认主机“192.168.1.5(192.168.1.5)”的真实性。只知道它的公钥指纹是这个0OFCrK6ijuT+GWZ/qLTfnbww8si6AEEeUhKcXnmE5xU 意思是需要你自己去核对，是不是你期望的远端公钥 OK确认是你要的公钥，键入yes Are you sure you want to continue connecting (yes/no/[fingerprint])? yes 系统会一个warning ，表示host主机已经被认可 Warning: Permanently added '192.168.1.5' (ECDSA) to the list of known hosts. 然后要求输入密码 Password: (enter password) 如果密码正确，就登录成功 当远程主机的公钥被接受以后，它就会被保存在文件~/.ssh/known_hosts之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示输入密码。 每个SSH用户都有自己的known_hosts文件，此外系统也有一个这样的文件，通常是/etc/ssh/ssh_known_hosts，保存一些对所有用户都可信赖的远程主机的公钥。 ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:2:1","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"公钥登录 每次使用终端登录服务器输密码，不够丝滑。SSH 支持公钥登录，一劳永逸的操作 $ ssh-keygen 会在~/.ssh/目录下生成 id_rsa.pub和id_rsa。前者是你的公钥，后者是你的私钥。将你的公钥上传到服务器上 在 ~/.ssh/authorized_keys 末尾添加即可 ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:2:2","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"配置 $ cat /etc/ssh/sshd_config # This is the sshd server system-wide configuration file. See # sshd_config(5) for more information. # This sshd was compiled with PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games # The strategy used for options in the default sshd_config shipped with # OpenSSH is to specify options with their default value where # possible, but leave them commented. Uncommented options override the # default value. Include /etc/ssh/sshd_config.d/*.conf #Port 22 #AddressFamily any #ListenAddress 0.0.0.0 #ListenAddress :: #HostKey /etc/ssh/ssh_host_rsa_key #HostKey /etc/ssh/ssh_host_ecdsa_key #HostKey /etc/ssh/ssh_host_ed25519_key # Ciphers and keying #RekeyLimit default none # Logging #SyslogFacility AUTH #LogLevel INFO # Authentication: #LoginGraceTime 2m #PermitRootLogin prohibit-password #StrictModes yes #MaxAuthTries 6 #MaxSessions 10 #PubkeyAuthentication yes # Expect .ssh/authorized_keys2 to be disregarded by default in future. #AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 #AuthorizedPrincipalsFile none #AuthorizedKeysCommand none #AuthorizedKeysCommandUser nobody # For this to work you will also need host keys in /etc/ssh/ssh_known_hosts #HostbasedAuthentication no # Change to yes if you don't trust ~/.ssh/known_hosts for # HostbasedAuthentication #IgnoreUserKnownHosts no # Don't read the user's ~/.rhosts and ~/.shosts files #IgnoreRhosts yes # To disable tunneled clear text passwords, change to no here! #PasswordAuthentication yes #PermitEmptyPasswords no # Change to yes to enable challenge-response passwords (beware issues with # some PAM modules and threads) KbdInteractiveAuthentication no # Kerberos options #KerberosAuthentication no #KerberosOrLocalPasswd yes #KerberosTicketCleanup yes #KerberosGetAFSToken no # GSSAPI options #GSSAPIAuthentication no #GSSAPICleanupCredentials yes #GSSAPIStrictAcceptorCheck yes #GSSAPIKeyExchange no # Set this to 'yes' to enable PAM authentication, account processing, # and session processing. If this is enabled, PAM authentication will # be allowed through the KbdInteractiveAuthentication and # PasswordAuthentication. Depending on your PAM configuration, # PAM authentication via KbdInteractiveAuthentication may bypass # the setting of \"PermitRootLogin without-password\". # If you just want the PAM account and session checks to run without # PAM authentication, then enable this but set PasswordAuthentication # and KbdInteractiveAuthentication to 'no'. UsePAM yes #AllowAgentForwarding yes #AllowTcpForwarding yes #GatewayPorts no X11Forwarding yes #X11DisplayOffset 10 #X11UseLocalhost yes #PermitTTY yes PrintMotd no #PrintLastLog yes #TCPKeepAlive yes #PermitUserEnvironment no #Compression delayed #ClientAliveInterval 0 #ClientAliveCountMax 3 #UseDNS no #PidFile /run/sshd.pid #MaxStartups 10:30:100 #PermitTunnel no #ChrootDirectory none #VersionAddendum none # no default banner path #Banner none # Allow client to pass locale environment variables AcceptEnv LANG LC_* # override default of no subsystems Subsystem sftp /usr/lib/openssh/sftp-server # Example of overriding settings on a per-user basis #Match User anoncvs # X11Forwarding no # AllowTcpForwarding no # PermitTTY no # ForceCommand cvs server ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:3:0","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"修改端口 $ sudo vim /etc/ssh/sshd_config # ubuntu下重启更新配置 $ systemctl restart sshd ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:3:1","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"禁用密码 ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:3:2","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"开启公钥 vim /etc/ssh/sshd_config PubkeyAuthentication yes 将本地的公钥上传到服务器的这个文件中 ~/.ssh/authorized_keys ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:3:3","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"root登录 $ vim /etc/ssh/sshd_config # 禁止密码登录 PermitRootLogin prohibit-password # 开启root密码登录 PermitRootLogin yes ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:3:4","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"超时 # vi /etc/ssh/sshd_config ClientAliveInterval 5m # 5 minutes ClientAliveCountMax 2 # 2 times ","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:3:5","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"原理","date":"2022-01-19","objectID":"/2022/01/linux/ssh/:5:0","tags":["linux"],"title":"sshd","uri":"/2022/01/linux/ssh/"},{"categories":null,"content":"原因是mysql 8.0之前的版本加密规则是mysql_native_password，在8.0之后加密规则是caching_sha2_password。解决办法是一种升级第三方客户端，第二种是把mysql用户登录密码加密规则还原成mysql_native_password 第一步先登录mysql $ mysql -u root -p #输入密码 修改账户密码的加密规则并更新用户密码 #修改加密规则 ALTER USER 'root'@'localhost' IDENTIFIED BY 'password' PASSWORD EXPIRE NEVER; #更新一下用户的密码 ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password'; 刷新权限 FLUSH PRIVILEGES; 重置密码 alter user 'root'@'localhost' identified by 'pwd'; ","date":"2022-01-15","objectID":"/2022/01/mysql/mysql_8.0_pwd/:0:0","tags":["mysql"],"title":"MySQL8.0及以上 第三方客户端连接出现 Authentication plugin 'caching_sha2_password' cannot be loaded","uri":"/2022/01/mysql/mysql_8.0_pwd/"},{"categories":null,"content":"https://macwk.com/soft/navicat-premium ","date":"2022-01-13","objectID":"/2022/01/software/navicat_premium/:0:0","tags":["软件"],"title":"navicat 破解版","uri":"/2022/01/software/navicat_premium/"},{"categories":null,"content":"认识TCP 还是那张图，tcp位于整个网络层次中的传输层 工作流程 tcp报文长这个样子 工作流程 源端口：是指发起连接的端口，及客户端口 目的端口：接受方端口，服务器端口 可以看出端口数量最大有2^16 -1（65535）个 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数 据都已经被正常接收。用来解决不丢包的问题。 控制位： ACK ，该位为1时，确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须 设置为 1 。 RST ，该位为1时，表示TCP连接中出现异常必须强制断开连接 SYN，该位为1时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN，该位为1时，表示希望今后不会再有数据发送，希望断开连接，当通信结束希望断开连接时，通信双方的主 机之间就可以相互交换 FIN 位为 1 的 TCP 段。 事实上，tcp协议很复杂，因为tcp定位是可靠的传输协议。由于IP层不可靠，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 面向连接：一定是一对一才能连接，是成对出现，不像UDP一样可以一个主机同时向多个主机发送消息。 可靠的：无论网络情况有多复杂，TCP 都可以保证一个报文一定能够到达接收端， 字节流：消息是没有边界，像流水一样。所以，需要应用层进行数据分割。http协议的换行符，和header里面的content-length 都是同一个目的，为了区分这次请求返回的数据界限。再比如，基于游戏服务的数据格式，需要留四位(int32)用于分包，处理正确的数据包。消息是「没有边界」的，所以无论我们消息有多大都可以进行传输。并且消息是「有序的」，当「前 一个」消息没有收到的时候，即使它先收到了后面的字节，那么也不能扔给应用层去处理，同时对「􏰀复」的 报文会自动丢弃。 简单来说就是，用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口 大小称为连接。 工作流程 socket：由 IP 地址和端口号组成 序列号：用来解决乱序问题等 窗口大小：用来做流量控制 TCP 通过四元组可以唯一确定一个链接 源IP 目的IP 源端口 目的端口 源IP和目的IP（32位）是通过IP协议发送IP数据报给对方主机 理论上client最多可以和65536-1（2^16-1）个server ip相连 理论上server最多可以和2^48( 2^32*2^16 )client相连，对 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是服务端单机最大 TCP 连接数，约为 2 的 48 次方。当然这只是理论，实际上远远达不到这么多的连接数 首先主要是文件描述符限制，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目; 另一个是内存限制，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的。 TCP数据长度=IP总长度-IP首部长度-TCP首部长度 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:1:0","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"TCP 连接建立 TCP 是面向连接的协议，所以使用 TCP 前必须先建立连接，而建立连接是通过三次握手来进行的。 工作流程 开始的时候，服务器和客户端都出于CLOSED状态。服务器监听某个端口，出于``LISTEN` 状态 第一次握手：客户端是随机初始化序号（client_isn）,并将这个序号填入TCP首部的序号字段中，同时把控制位SYN置为1，表示SYN报文。做完这些，将这个报文发给服务器，表示向服务器发起连接，该报文不包含应用数据，之后客户端出于SYN-SENT 状态 工作流程 第二次握手：服务端收到客户端的SYN 报文后，服务器也随机初始化一个序号（server_isn），将序号填充TCP首部的序号字段中，然后把client_isn+1填入TCP首部的「确认应答号」字段中，并把SYN 和 ACK 标志位置为1。最后把该报文发给客户端，该报文不包含应用数据，服务器此时处于SYN-RCVD 状态 工作流程 第三次握手：客户端收到服务器发送的SYN/ACK 报文后，还需要给服务器一个应答报文，及ACK 报文。首先该应答报文 TCP 首部 ACK 标志位置为1 ，其次「确认应答号」字段填入server_isn+1 最后将这个报文发个服务器，这次报文可以携带应用数据，之后客户端出于ESTABLISHED状态，服务器收到客户端的应答报文后，也进入ESTABLISHED状态 一旦完成三次握手，双方都处于ESTABLISHED状态，此时连接已经建立完成，客户端和服务端就可以互相发数据了。TCP 的连接状态查看，在 Linux 可以通过 netstat -napt 命令查看。 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:2:0","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"为什么一定是三次握手，不是两次，不是四次 很常见的回答是：只有三次握手才能保证双方的接受能力和发送能力 这不是主要原因 三次握手才可以阻止重复历史连接的初始化（主要原因） 三次握手才可以同步双方的初始化序列号 三次握手才可以避免资源浪费 原因一 三次握手最重要的原因是防止旧的重复连接初始化造成混乱 实际上的网络环境错综复杂，有可能旧的数据包先到达目的主机，这会造成连接混乱。为了规避这个问题，才有了三次握手 在网络拥堵的情况下，客户端连续发起多次SYN建立连接的报文 一个旧的SYN报文比最新的SYN报文早到达服务器 此时服务会回一个SYN/ACK 报文给客户端 客户端收到后根据自身的上下文，判断是否是个历史连接。三次握手巧就巧在，客户端可以根据情况来告诉服务器 如果是历史连接，则发一个RST 报文，终止历史连接 如果不是历史连接，则发一个ACK报文。双发进入ESTABLISHED 原因二 TCP 协议的通信双方，都必须维护一个序列号，序列号是可靠传输的关键因素 接收方可以去除重复数据 接收方可以根据数据包的序列号接收 可以标识发出去的数据包中，哪些是已经被对方收到的 可以看见，序列号和应答号很重要，当客户端发送带有初始化序列号的SYN 时候，需要服务端回一个带有应答号的ACK报文，标识客户端的SYN报文已被服务器成功接收，当服务器发送初始化序列号SYN包给客户端时候，客户端也需要回一个ACK报文。这样一来一回才能确保双方的初始化序列号可能被可靠同步 原因三 四次握手可以满足tcp连接要求，但是没有必要 原因四：避免资源浪费 如果是两次握手，当客户端的SYN报文在网络中阻塞，客户端由于没有收到服务的ACK报文，就会重新发送SYN报文。服务器收到一个SYN就会回一个ACK报文然后建立连接，就会造成资源浪费的情况 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:2:1","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"认识 MTU 和 MSS MTU：（Maximum Transmission Unit）最大传输单元，一个网络包的最大的长度，以太网一般为1500字节 MSS：除去IP和TCP头部之后，一个网络包所能容纳的TCP数据的最大长度 如果TCP的整个报文（头部+数据）交给IP层进行分片，会怎么样 当IP层有一个超过MTU大小的数据（TCP头部和TCP数据）要发送时，IP层就会进行分片，把数据分成若干片，保证每一分片都小于MTU，最后有目的进行重新组装。再交给上一层TCP传输层。这样有个问题如果一个IP分片丢失，整个IP报文的所有分片都要重传。因为一个大的TCP报文被MTU分片，那么只有第一个分片才具有TCP头部，后面的分片则没有TCP头部，接受方IP层只有重组装这些分片，才发现是个TCP报文，如果丢失了其中一个分片，接受方IP层不会把TCP报文丢给TCP层，会等待对方超时重传整个TCP报文。 但是如果一个大的TCP报文被MSS分片，那么所有的分片都具有TCP头部，这样一来其中一个MSS分片丢失，只需要重传一个分片就可以了。 为了达到最佳的传输效能 TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过MSS 时，则就先会进行分片，当然由它形成的 IP 包的⻓度也就不会大于 MTU ，自然也就不用 IP 分片了。 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:3:0","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"SYN攻击 TCP的连接建立需要三次握手，假设攻击者短时间内伪造了不同的IP地址的SYN报文，服务器接收后就进入SYN_RCVD 状态。但服务端发送出去的ACK/SYN 报文，无法得到未知IP主机的ACK应答，久而久之就会占满服务端的SYN接收队列（未连接队列），使得服务器不能为正常的用户服务 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:4:0","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"避免SYN攻击方法一 通过修改Linux 内核参数，控制队列大小和当队列满是应该怎么处理 查看内核参数 sysctl -a | grep netdev_max_backlog SYN_RCVD 状态连接的最大个数 $ cat /etc/sysctl.conf | grep net.ipv4.tcp_max_syn_backlog net.ipv4.tcp_max_syn_backlog = 1024 超出处理能时，对新的 SYN 直接回报 RST，丢弃连接: 当TCP连接已经建立，并塞到程序监听backlog队列时，如果检测到backlog队列已经满员后，TCP连接状态会回退到SYN+ACK状态，假装TCP三次握手第三次客户端的ACK包没收到，让客户端重传ACK，以便快速进入ESTABLISHED状态。如果设置了net.ipv4.tcp_abort_on_overflow参数，那么在检测到监听backlog 队列已满时，直接发 RST 包给客户端终止此连接，此时客户端程序会收到104 Connection reset by peer错误。这个参数很暴力，慎用 net.ipv4.tcp_abort_on_overflow #参数表示网卡接受数据包的队列最大长度，在阿里云服务器上，默认值是1000，可以适当调整。 net.core.netdev_max_backlog ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:4:1","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"避免SYN攻击方法二 Linux 内核的SYN （未完成连接建立）队列与Accept（已完成连接建立）队列是如何工作的？ 正常流程： 当服务端接收到了客户端发的SYN报文时，会将其放入内核的SYN队列中 接着发送SYN+ACK 给客户端，等待客户端回应ACK报文 服务端接收到ACK报文后，从SYN队列移除到Accept队列 应用通过调用accept() socket 接口，从Accept队列取出连接。 当应用程序处理，就会导致Accept队列被占满，SYN队列也被占满 当服务端受到SYN攻击时，SYN队列会被占满 内核参数tcp_syncookies 的方式可以应对SYN攻击 net.ipv4.tcp_syncookies = 1 当SYN队列被占满后，后续的服务端收到SYN报文，不进入SYN队列 计算出一个cookie 值，再以SYN+ACK 中的序列号返回客户端 服务端接收客户端的ACK报文，服务端会检查ACK包的合法性。如果合法，直接Accept 队列 最后应用通过调用 accpet() socket 接口，从「 Accept 队列」取出的连接。 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:4:2","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"TCP 的四次挥手，断开连接 双方都可以主动断开连接，断开后主机的资源将被释放 客户端打算关闭连接，此时会发送一个TCP头部FIN 标志位被置为1 的报文，也就是FIN报文，之后客户端进入FIN_WAIT_1 状态 服务端收到报文后，就向客户端发送ACK应答报文，接着服务端进入CLOSED_WAIT 状态 客户端收到ACK报文后进入FIN_WAIT_2状态。 等服务端处理完数据后，会向客户端发送FIN报文，之后进入LAST_ACK 状态 客户端收到服务端的FIN报文后，回一个ACK报文，进入TIME_WAIT 状态 服务器收到了ACK报文后，进入CLOSED状态，至此服务端已经完成了连接的关闭 客户端在经过2MSL 一段时间后，自动进入CLOSED状态，至此客户端已经完成了连接的关闭 有一点需要注意的是：主动关闭连接的，才会有TIME_WAIT 状态 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:5:0","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"为什么一定要四次交互呢 关闭连接的时候，客户端向服务端发送FIN 时，仅仅表示客户端不再发送数据了，但是还能接收。 服务端收到客户端的FIN报文后，先回一个ACK报文，而服务端可能还有数据需要处理和发送，等服务器不再发送数据时，才发送FIN报文给客户端表示现在关闭连接 服务端通常要等待完成数据的处理和发送，所以ACK和FIN 报文会分开发送，这就是四次交互的由来 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:5:1","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"为什么需要TIME_WAIT 防止具有相同的四元组的旧数据包被收到 保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。 原因一：防止旧连接的数据包 如果TIME_WAIT 没有等待时间或者时间很短，被延迟的数据包抵达会发什么什么？ 原因二：保证连接正确关闭 TIME_WAIT 作用是等待足够的时间以确保最后的ACK能让被动关闭方接收，从而帮助其正确关闭。 如果 TIME_WAIT 等待足够长 服务端正常收到四次挥手的最后一个ACK报文，则服务器正常关闭连接 服务端没有收到四次挥手的最后一个ACK报文，最会重发FIN，并等待新的ACK报文 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:5:2","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"为什么TIME_WAIT 等待时间时2MSL MSL 全称 Maximum Segment Lifetime，报文的最大生存时间，它是任何报文在网络上的存在的最长时间，超过这个时间的报文将被丢弃。因为TCP报文是基于IP协议的，而IP头部有个TTL字段，是IP数据报可以经过的最大路由数，每经过一个处理他的路由器该值就会减一，当为0时，就会被丢弃，同时发送ICMP报文通知源主机。 MSL与TTL的区别：MSL的单位是时间，而TTL是经过路由的跳数。所以MSL应该大于等于TTL消耗为0 的时间，以确保报文已被自然消亡。 TIME_WAIT 等待2倍的MSL，比较合理的解释是：网络中可能存在来自对发送的数据包，当这些发送方的数据包被接收方处理又会向对方发送响应，一来二去正好2个MSL。 2MSL 的时间是从客户端发送FIN后发送ACK开始计时。如果在TIME_WAIT 时间内，因为客户端的ACK没有传输到服务器，客户端右接收到了服务端重发的FIN报文，那么2MSL时间将会重新计时。 在Linux系统里2MSL默认是60秒，其定义在Linux内核代码里面的名称为TCP_TIMEWAIT_LEN: #define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds */ 如果要修改 TIME_WAIT 的时间⻓度，只能修改 Linux 内核代码里 TCP_TIMEWAIT_LEN 的值，并重新编译 Linux 内核 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:5:3","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"TIME_WAIT 过多有什么危害 内存资源占用 对端口资源的占用 ","date":"2022-01-11","objectID":"/2022/01/net/tcp/:5:4","tags":["网络协议"],"title":"tcp 协议","uri":"/2022/01/net/tcp/"},{"categories":null,"content":"HTTP HTTP 全称HyperText Transfer Protocol，是一种基于TCP的超文本传输协议。所谓的协议通俗的讲就是制定一系列规则，要想沟通就必须遵守这些规则。稍微熟悉网络协议栈的朋友都知道HTTP协议位于应用层 一个完整的HTTP报文长这个样子，请求的数据格式和响应的数据格式都是这样。 Tip TCP 协议并没有数据包的概念，传输的二进制流（binary stream），所以基于TCP的应用层协议都需要自己处理数据边界。HTTP数据格式的那些分隔符就是为了确定信息边界的。例如一个简易的游戏服务，数据包是基于TCP设计的，那么它的消息体有可能长这个样子 ","date":"2021-12-28","objectID":"/2021/12/net/http/:0:0","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"HTTP 状态码 状态码比较多，如果忘记可随时查阅 ","date":"2021-12-28","objectID":"/2021/12/net/http/:1:0","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"100 - 199 ","date":"2021-12-28","objectID":"/2021/12/net/http/:1:1","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"200–299 200 OK ","date":"2021-12-28","objectID":"/2021/12/net/http/:1:2","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"300–399 ","date":"2021-12-28","objectID":"/2021/12/net/http/:1:3","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"400–499 401 Unauthorized 403 Forbidden 404 Not Found 405 Method Not Allowed ","date":"2021-12-28","objectID":"/2021/12/net/http/:1:4","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"500–599 500 Internal Server Error 502 Bad Gateway ","date":"2021-12-28","objectID":"/2021/12/net/http/:1:5","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"HTTP 请求方法 ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:0","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"GET get 请求并不建议带有body 幂等 ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:1","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"POST ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:2","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"HEAD ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:3","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"PUT ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:4","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"DELETE ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:5","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"CONNECT ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:6","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"OPTIONS ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:7","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"TRACE ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:8","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"PATCH ","date":"2021-12-28","objectID":"/2021/12/net/http/:2:9","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"HTTP 数据报文 http协议主要分为三个部分 起始行(start line) 头部字段(header) key-value 结构 消息主体(entity) 实际传输的的数据，也称为body。可能是纯文本，图片，二进制流，视频等 ","date":"2021-12-28","objectID":"/2021/12/net/http/:3:0","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"HTTP 请求过程 ","date":"2021-12-28","objectID":"/2021/12/net/http/:4:0","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"HTTPS 由于http协议都是明文传输，并不安全，容易遭到劫持和修改。所以为数据安全用 http+SSL(安全套接层)和TLS(安全传输协议) 就变成https。协议层次图 工作流程 } ","date":"2021-12-28","objectID":"/2021/12/net/http/:5:0","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"加密方式 对称加密 加密端和解密端持有同一份秘钥 工作流程 } 非对称加密 非对称算法需要两个秘钥，公钥(public key)，私钥(private key)。公钥和私钥是成对的。用公钥加密的密文只有对应的私钥能解开，私钥加密对应的公钥才能解开。非对称加密解决了对称加密的一个问题就是，秘钥传输本身不是安全的。 工作流程 } 在https 这两种加密方式都采用了，为什么呢，http的报文可能很大，如果用非对称加密，性能就很差。所以http报文采用对称加密来加密的。 工作流程 } ","date":"2021-12-28","objectID":"/2021/12/net/http/:5:1","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"数字证书认证 由于公钥的下发是不安全的，可能会被劫持篡改，所以需要三方介入，这个三方就是CA。CA是一个数字证书认证机构，一个客户端和服务器都信任的机构。该机构负责数字签名服务器的上报的公钥。 数字签名：一种认证机制。 HTTPS 数字认证的流程如图 工作流程 } ","date":"2021-12-28","objectID":"/2021/12/net/http/:5:2","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"握手过程 工作流程 } Client Hello:握手第一步是客户端向服务端发送 Client Hello 消息，这个消息里包含了一个客户端生成的随机数 Random1、客户端支持的加密套件（Support Ciphers）和 SSL Version 等信息 Server Hello:第二步是服务端向客户端发送 Server Hello 消息，这个消息会从 Client Hello 传过来的 Support Ciphers 里确定一份加密套件，这个套件决定了后续加密和生成摘要时具体使用哪些算法，另外还会生成一份随机数 Random2。注意，至此客户端和服务端都拥有了两个随机数（Random1+ Random2），这两个随机数会在后续生成对称秘钥时用到。 Certificate:这一步是服务端将自己的证书下发给客户端，让客户端验证自己的身份，客户端验证通过后取出证书中的公钥 Server Hello Done:通知客户端 Server Hello 过程结束。 Certificate Verify:客户端收到服务端传来的证书后，先从 CA 验证该证书的合法性，验证通过后取出证书中的服务端公钥，再生成一个随机数 Random3，再用服务端公钥非对称加密 Random3生成 PreMaster Key Client Key Exchange:上面客户端根据服务器传来的公钥生成了 PreMaster Key，Client Key Exchange 就是将这个 key 传给服务端，服务端再用自己的私钥解出这个 PreMaster Key 得到客户端生成的 Random3。至此，客户端和服务端都拥有 Random1 + Random2 + Random3，两边再根据同样的算法就可以生成一份秘钥，握手结束后的应用层数据都是使用这个秘钥进行对称加密。为什么要使用三个随机数呢？这是因为 SSL/TLS 握手过程的数据都是明文传输的，并且多个随机数种子来生成秘钥不容易被暴力破解出来。 Change Cipher Spec(Client):这一步是客户端通知服务端后面再发送的消息都会使用前面协商出来的秘钥加密了，是一条事件消息 Encrypted Handshake Message(Client):这一步对应的是 Client Finish 消息，客户端将前面的握手消息生成摘要再用协商好的秘钥加密，这是客户端发出的第一条加密消息。服务端接收后会用秘钥解密，能解出来说明前面协商出来的秘钥是一致的 Change Cipher Spec(Server):这一步是服务端通知客户端后面再发送的消息都会使用加密，也是一条事件消息 Encrypted Handshake Message(Server):这一步对应的是 Server Finish 消息，服务端也会将握手过程的消息生成摘要再用秘钥加密，这是服务端发出的第一条加密消息。客户端接收后会用秘钥解密，能解出来说明协商的秘钥是一致的。 Application Data:到这里，双方已安全地协商出了同一份秘钥，所有的应用层数据都会用这个秘钥加密后再通过 TCP 进行可靠传输 ","date":"2021-12-28","objectID":"/2021/12/net/http/:5:3","tags":["网络协议"],"title":"http 协议","uri":"/2021/12/net/http/"},{"categories":null,"content":"前言 mysql的事务不管是面试还是工作中，都有理由去掌握并应用。因为这关系到数据一致性问题，如果数据一致性(包括缓存)处理不好，就会带来毁灭性的灾难。 ","date":"2021-12-09","objectID":"/2021/12/mysql/mysql_transaction/:1:0","tags":["mysql"],"title":"mysql 事务","uri":"/2021/12/mysql/mysql_transaction/"},{"categories":null,"content":"相关语法 查看全局，会话事务的隔离级别 #8.0后 #全局 select @@global.transaction_isolation; #会话 select @@session.transaction_isolation; #全局 SELECT @@transaction_isolation; #全局 SELECT @@global.tx_isolation; #会话 SELECT @@session.tx_isolation; #全局 SELECT @@tx_isolation; 设置 SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE} 概念 READ UNCOMMITTED 读未提交，一个事务可以读到另一个事务的未提交(commit) 的数据，该隔离级别会导致脏读现象 READ COMMITTED 读已提交，一个事物能读到另一个事务的已提交的(commit) 的数据，但是如果这个事务重复的读有可能会导致数据不一致的情况，后面详细介绍，该隔离级别会导致两次读数据不一致的现象。 REPEATABLE READ 可重复的读，一个事务能读到另一个事务的已提交的(commit) 的数据，并且重复读数据得到的是相同的结果，该隔离级别会导致幻读 SERIALIZABLE 串行，并发量最低，数据正确度最高，最高隔离级别 总结 ","date":"2021-12-09","objectID":"/2021/12/mysql/mysql_transaction/:2:0","tags":["mysql"],"title":"mysql 事务","uri":"/2021/12/mysql/mysql_transaction/"},{"categories":null,"content":"演示 READ UNCOMMITTED 读未提交 SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; 终端1会话隔离级别为 工作流程 设置终端1当前会话隔离级别为READ UNCOMMITTED 工作流程 再开一个终端叫终端2，查看数据为level 为0 工作流程 在终端1上，起事务更新数据 level 为1 工作流程 在终端2上设置会话隔离级别为READ UNCOMMITTED，并查看数据，发现level已经变成了1 工作流程 READ COMMITTED 读已提交 设置终端1，终端2的隔离级别为 READ COMMITTED SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED; 终端1起事务，更新数据 工作流程 终端2上查看数据，发现level并没有发生改变 工作流程 终端1提交数据 工作流程 终端2上查看数据，发现level变成了9 工作流程 REPEATABLE READ 可重复读 设置终端1，终端2的隔离级别为 REPEATABLE READ SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; 终端1起事务，更新数据 工作流程 终端2起事务，查看数据 工作流程 终端1提交数据 工作流程 终端2查看数据，发现level还是9 工作流程 终端2提交，再次查看发现level变成了199了 工作流程 如果终端1，插入数据会怎么样，其他查询事务会看见吗 终端2起事务，查看数据 工作流程 终端1插入数据 SERIALIZABLE 串行 终端1，终端2，设置隔离级别为 SERIALIZABLE SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE; 终端1，起事务插入数据 工作流程 终端2，起事务，查看数据发现，卡住了直到超时 工作流程 ","date":"2021-12-09","objectID":"/2021/12/mysql/mysql_transaction/:3:0","tags":["mysql"],"title":"mysql 事务","uri":"/2021/12/mysql/mysql_transaction/"},{"categories":null,"content":"jmeter ","date":"2021-11-17","objectID":"/2021/11/tool/%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7/:0:0","tags":["工具"],"title":"jmeter","uri":"/2021/11/tool/%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7/"},{"categories":null,"content":"引入问题 有相当一部分时间，我对于mysql的索引理解是错误的。什么最左匹配原则，什么覆盖索引，什么聚集索引等概念似懂非懂。直到某天的一个上线需求让我认识到索引的重要性。 原有表: CREATE TABLE `extend_new_user_profile` ( `id` bigint(64) NOT NULL AUTO_INCREMENT COMMENT '主键ID', `user_id` varchar(36) NOT NULL COMMENT '用户ID', `md5_mobile_number` varchar(32) NOT NULL COMMENT '手机号码md5加密', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`id`), UNIQUE KEY `unqi_user_id_name_id_card` (`user_id`), KEY `idx_md5_mobile_number` (`md5_mobile_number`), ) ENGINE=InnoDB AUTO_INCREMENT=326199 DEFAULT CHARSET=utf8mb4 COMMENT=''; 当时的上线sql: alter table extend_new_user_profile add `is_authenticate` tinyint(4) NOT NULL DEFAULT 0 COMMENT '是否认证:0否,1是', add `encrypt_id_card` varchar(128) NOT NULL DEFAULT '' COMMENT '身份id aes加密', add `encrypt_real_name` varchar(128) NOT NULL DEFAULT '' COMMENT '真实名字 aes加密'; ALTER table extend_new_user_profile ADD unique index unqi_user_id_name_id_card(`user_id`,`encrypt_id_card`,`encrypt_real_name`); 其中业务代码上有个查询语句是这样的 select * from extend_new_user_profile where encrypt_real_name=? and encrypt_id_card= ? ok 现在来看这句查询没有走到索引。线上效果表现的很明显，实名认证的接口直接超时，因为这个表大概有一亿多条数据。我心中有点疑惑，联合索引不应该每个字段都是命中因子吗？显然不是 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:1:0","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"mysql 读取速度 在了解索引之前，要明白是什么拖慢了整个接口的响应时长。 Work Latency L1 cache reference //一级缓存 0.5 ns Branch mispredict //分支预测错误 5 ns L2 cache reference //二级缓存 7 ns Mutex lock/unlock //互斥锁 25 ns Main memory reference //主存 100 ns Compress 1K bytes with Zippy //Zippy压缩1kB 3,000 ns Send 1K bytes over 1 Gbps network // 10,000 ns Read 4K randomly from SSD* //随机从ssd读4kb数据 150,000 ns Read 1 MB sequentially from memory //从主存顺读1MB数据 250,000 ns Round trip within same datacenter //同一个数据中心往返 500,000 ns Read 1 MB sequentially from SSD*//从ssd顺序读1MB数据 1,000,000 ns Disk seek //机械硬盘寻道 10,000,000 ns Read 1 MB sequentially from disk //从机械硬盘顺序读1MB数据 20,000,000 ns Send packet CA-\u003eNetherlands-\u003eCA //从美国加州到荷兰再到加州 150,000,000 ns 表 1- 2012 年延迟数字对比1 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:0","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"数据加载顺序 Title: mysql 加载数据顺序 查询请求-\u003edatabase buffer pool: 查询请求-\u003e内存: 如果database buffer pool没有则 查询请求-\u003e磁盘: 如果内存没有则 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:1","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"磁盘IO 上述关系描述了mysql加载数据的顺序 mysql 对数据的读取不是一行一行的读取，而是以页的形式。一般的来说mysql一页大小16KB，而linux系统包括Mac为4KB，可通过getconf PAGE_SIZE 获取。也就说读取的快慢和行数没有关系，和页数有关。如果database buffer pool 和 内存都没有命中，那么就会触发随机IO pie title 磁盘随机 I/O 占比（10ms） \"Queuing\" : 30.0 \"Seek\" : 40.0 \"Half Rotation\" : 20.0 \"Transfer\" : 10.0 上述统计描述io的总时长和占比，10ms对于计算机来说是个很大的时间消耗，这是查询请求最糟糕的一种情况 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:2","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"磁盘顺序I/O 磁盘的顺序I/O 和随机I/O在速度上是天差地别，如果一个页面有4KB，那么1s的时间可以读取10000页，而平均读取一页的内容所耗费0.1s，这个速度甚至比内存还快 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:3","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"内存读取 如果mysql在database buffer pool没有找到对应的数据页，那么会去内存寻找数据页，此过程花费大约1ms ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:4","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"database buffer pool 读取 毫无疑问，从database buffer pool 读取是几种方式最快的一种方式 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:5","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"总结 要想查询的快要尽量减少随机I/O的读取 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:2:6","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"索引 根据一个节点包含的列数不同，可分为宽索引(联合索引)和窄索引。列如下面这个是窄索引 unique index unqi_user_id(`user_id) 工作流程 这个就是宽索引 unique index unqi_user_id_name_id_card(`user_id`,`encrypt_id_card`,`encrypt_real_name`) 工作流程 如果宽索引设计的好的话，可以避免二次随机I/O，列如这个查询语句 select user_id,encrypt_id_card,encrypt_real_name from extend_new_user_profile where user_id=? 值得注意的是，一张表里面，有且只有一个聚集索引，一般来说这个字段就自增主键id。 对于窄索引来说，找到匹配的数据项后通过id再次找到剩余的数据项。 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:3:0","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"匹配因子 实际情况可能是窄索引带来的二次随机I/O不可避免，那么我们就应该考虑谁来做匹配因子最合适——最左原则。如下一张表 CREATE TABLE `game_user` ( `id` bigint(64) NOT NULL AUTO_INCREMENT COMMENT '主键ID', `user_name` varchar(128) NOT NULL DEFAULT '' COMMENT '名字', `age` int(4) NOT NULL DEFAULT 1 COMMENT '年龄', `gender` tinyint NOT NULL DEFAULT 0 COMMENT '性别', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT='user'; 有哪些字段适合做匹配因子。 工作流程 显然gender就不是一个好的匹配因子，因为会选中一半的数据，而user_name 就很适合做匹配因子，它可以过滤掉99.9%的数据 工作流程 另外同一个字段不同的值的过滤占比是不同的，往往是最差的输入换来的是最差的性能 工作流程 ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:4:0","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"如何设计一个完美索引 三星索引2对于一个查询来说是一个完美的索引，where id = 2 主键索引对于这个查询语句来说就是一个特殊的三星索引。 三星的定义 第一颗星，所有的等值谓词的列，例如 where name= 'joker' where age= 18 这种列，作为索引开头的最开始的列（最左原则） 如果有order by 的列，需要加入索引列中 查询字段覆盖所有的索引字段，以此避免再从主键索引查询剩余字段导致随机I/O https://gist.github.com/jboner/2841832 ↩︎ 三星索引的定义来自《数据库索引设计与优化》 ↩︎ ","date":"2021-10-19","objectID":"/2021/10/mysql/mysql_b_plus_tree/:5:0","tags":["mysql"],"title":"为什么mysql要用索引","uri":"/2021/10/mysql/mysql_b_plus_tree/"},{"categories":null,"content":"llvm 安装clang-format ","date":"2021-10-11","objectID":"/2021/10/c/clang/:0:0","tags":["C"],"title":"clang","uri":"/2021/10/c/clang/"},{"categories":null,"content":" 下载 iTerm2 zsh 系统自带，但不是最新，安装最新zsh ➜ ~ zsh --version zsh 5.8 (x86_64-apple-darwin18.7.0) ➜ brew install zsh 安装oh-my-zsh git clone git@github.com:toxicwebdev/robbyrussell-oh-my-zsh.git ~/.oh-my-zsh # or git clone git@github.com:genDopamine/ohmyzsh.git ~/.oh-my-zsh 配置oh-my-zsh cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc 兼容bash环境变量 echo \"source ~/.bash_profile\" \u003e\u003e ~/.zshrc #刷新bash的环境变量 ubuntu下不行可拷贝 修改shell chsh -s /bin/zsh 更换主题 主题图鉴 vim ~/.zshrc //修改ZSH_THEME=\"avit\" 挑几个自己喜欢的主题 avit robbyrussell 插件 oh-my-zsh 可以绑定插件，可以变的更加漂亮。 命令高亮 brew install zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting vim ~/.zshrc #在 plugins=(...) 增加zsh-syntax-highlighting 自动补全命令 git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions vim ~/.zshrc #在 plugins=(...) 增加zsh-autosuggestions 官方插件非常多 plugins 激活方式 vim ~/.zshrc #在 plugins=(...) 增加插件 注： 如果插件下不下来(墙的问题)可copy https://gitee.com/Joker_null/plugins/tree/master/zsh 下的文件夹到 cp -r ./plugins/zsh/zsh-syntax-highlighting ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting cp -r ./plugins/zsh/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions vim ~/.zshrc #在 plugins=(...) 增加zsh-syntax-highlighting vim ~/.zshrc #在 plugins=(...) 增加zsh-autosuggestions iterm2配置rz,sz 安装lrzsz brew install lrzsz 复制两个shell脚本到 /usr/local/bin 下 https://gitee.com/Joker_null/plugins/blob/master/zsh/iterm2-recv-zmodem.sh https://gitee.com/Joker_null/plugins/blob/master/zsh/iterm2-send-zmodem.sh 打开iterm2 Preferences -\u003e Profiles -\u003e Default -\u003e Advanced 的 tab 页 -\u003e Triggers - Edit Regular expression: rz waiting to receive.\\*\\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checked Regular expression: \\*\\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked ","date":"2021-10-10","objectID":"/2021/10/mac/iterm2%E5%92%8Czsh/:0:0","tags":["mac"],"title":"iterm2和zsh","uri":"/2021/10/mac/iterm2%E5%92%8Czsh/"},{"categories":null,"content":"官网 仓库 在线体验 命令参考 安装 make 后 src 下会出现 redis-server和 redis-cli $ wget https://download.redis.io/releases/redis-6.2.6.tar.gz $ tar xzf redis-6.2.6.tar.gz $ cd redis-6.2.6 $ make $ cd src $ ./redis-server ../redis.conf $ ./redis-cli ","date":"2021-10-08","objectID":"/2021/10/redis/redis/:0:0","tags":["redis"],"title":"redis","uri":"/2021/10/redis/redis/"},{"categories":null,"content":"配置 直接改配置文件 vim redis.conf 配置密码 requirepass 123456 配置守护进程 daemonize yes 配置外部访问 注释 bind 127.0.0.1 -::1 # bind 127.0.0.1 -::1 修改port port 6333 配置命令 获取配置 #获取配置信息 127.0.0.1:6333\u003e CONFIG GET CONFIG_SETTING_NAME #列如 127.0.0.1:6333\u003e config get loglevel 1) \"loglevel\" 2) \"notice\" 127.0.0.1:6333\u003e #获取全部 127.0.0.1:6333\u003e config get * 1) \"rdbchecksum\" 2) \"yes\" 3) \"daemonize\" 4) \"yes\" 5) \"io-threads-do-reads\" 6) \"no\" 7) \"lua-replicate-commands\" 8) \"yes\" 9) \"always-show-logo\" 10) \"no\" 11) \"protected-mode\" 12) \"no\" 13) \"rdbcompression\" 14) \"yes\" .................... 修改配置 127.0.0.1:6333\u003e config set loglevel \"notice\" OK 注册系统服务 创建文件，复制配置 $ mkdir /etc/redis $ cp ~/software/redis-6.2.1/redis.conf /etc/redis $ cp /etc/redis/redis.conf /etc/redis/6333.conf 复制redis-server redis-cli $ cp ~/software/redis-6.2.1/redis.conf/src/redis-server /usr/local/bin/redis-server $ cp ~/software/redis-6.2.1/redis.conf/src/redis-cli /usr/local/bin/redis-server 拷贝脚本 $ cat software/redis-6.2.1/utils/redis_init_script \u003e\u003e /etc/ini.d/redis #如果修改了port 需要更改脚本文件中的port $ chmod 755 /etc/ini.d/redis 注册开启服务 $ update-rc.d redis defaults 开启服务 $ service redis start ","date":"2021-10-08","objectID":"/2021/10/redis/redis/:0:1","tags":["redis"],"title":"redis","uri":"/2021/10/redis/redis/"},{"categories":null,"content":"归并排序，冒泡排序......","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":"归并排序 归并排序，时间复杂度(nlogn)，空间复杂度o(n),分而治之 4, 5, -1, -6, -9,-9,-10,-100 func TestMergeSort(t *testing.T) { array := []int{4, 5, -1, -6, -9,-9,-10,-100} fmt.Println(mergeSort(array, 0, len(array)-1)) } func mergeSort(array []int, low, high int) []int { mid := (high-low)/2 + low newArray := make([]int, 0) if low \u003e= high { newArray = append(newArray, array[low]) return newArray } left := mergeSort(array, low, mid) right := mergeSort(array, mid+1, high) var leftIndex, rightIndex int for leftIndex \u003c len(left) \u0026\u0026 rightIndex \u003c len(right) { if left[leftIndex] \u003c= right[rightIndex] { newArray = append(newArray, left[leftIndex]) leftIndex++ } else { newArray = append(newArray, right[rightIndex]) rightIndex++ } } for leftIndex \u003c len(left) { newArray = append(newArray, left[leftIndex]) leftIndex++ } for rightIndex \u003c len(right) { newArray = append(newArray, right[rightIndex]) rightIndex++ } return newArray } ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/:1:0","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":"冒泡排序 时间复杂度o(n*n) func TestBubbleSort(t *testing.T) { array := []int{1, 3, 9, -22, -9, 0, -1, -123} bubbleSort(array) fmt.Println(array) } func bubbleSort(array []int) { for i := 0; i \u003c len(array); i++ { for j := i + 1; j \u003c len(array); j++ { if array[i] \u003e array[j] { array[i], array[j] = array[j], array[i] } } } } ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/:2:0","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":"选择排序 ， 时间复杂度o(n*n),空间复杂度o(1)。每次选取最小(最大)的一个，与当前元素进行互换 func TestSelectSort(t *testing.T) { array := []int{8, -5, -3, -6, 1, 9, -90} selectSort(array) fmt.Println(array) } func selectSort(array []int) { for i := 0; i \u003c len(array); i++ { min := i for j := i + 1; j \u003c len(array); j++ { if array[j] \u003c array[min] { min = j } } if i != min { array[i],array[min]=array[min],array[i] } } } ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/:3:0","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":"插入排序 时间复杂度o(n*n),空间复杂度o(1)。流程类似于排序扑克牌。 func TestInsertSort(t *testing.T) { a := []int{-1, 8, -9, -10, 0, 1, 8, 7, -99} insertSort(a) fmt.Println(a) } func insertSort(array []int) { for i := 0; i \u003c len(array); i++ { for j := i + 1; j \u003e 0 \u0026\u0026 j \u003c len(array); j-- { if array[j] \u003c array[j-1] { array[j], array[j-1] = array[j-1], array[j] } else { break } } } } ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/:4:0","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":"快速排序 时间复杂度n*logn。基本思想是，随机选一个key。将所有小于key的放在左边，大于等于key的放在右边。 然后对左右两边的子数列进行第一个操作。直到子数组只有一个数字为止 func TestQuickSort(t *testing.T) { array := []int{-1, 2, 4, -6, 7, 8} quickSort(array, 0, len(array)-1) fmt.Println(array) } func quickSort(array []int, low, high int) { if low \u003e= high { return } i := low j := high key := array[i] for i \u003c j { for i \u003c j \u0026\u0026 array[j] \u003e= key { j-- } if i \u003c j { array[i] = array[j] i++ } for i \u003c j \u0026\u0026 array[i] \u003c key { i++ } if i \u003c j { array[j] = array[i] j-- } } array[i] = key quickSort(array, low, i-1) quickSort(array, i+1, high) } ⋊\u003e ~/g/s/j/go_learn go test --count=1 -v -run TestQuickSort sort/sort_test.go 10:36:54 === RUN TestQuickSort [-6 -1 2 4 7 8] --- PASS: TestQuickSort (0.00s) PASS ok command-line-arguments 0.006s ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/:5:0","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":"堆排序 利用堆的数据结构进行排序。分为大顶堆，小顶堆，如图 ​ 大顶堆 ​ 小顶堆 堆排序分为两部分 构建一个堆 移除顶部元素后重新维持一个堆 由于堆是一种特殊的二叉树，整体排序所消耗的时间是n*logn。具体过程如下 排序这个数组 [-9, 89, 2, 73, 88, 83, -34, -23, 8] 构建堆 移除堆顶元素，重新维护一个新的堆 //java public class BigHeap\u003cE extends Comparable\u003cE\u003e\u003e { private final ArrayList\u003cE\u003e list = new ArrayList\u003c\u003e(); public BigHeap() { } public BigHeap(ArrayList\u003cE\u003e list) { for (E e : list) { add(e); } } public void add(E e) { list.add(e); int currentIndex = list.size() - 1; while (currentIndex \u003e 0) { int parentIndex = (currentIndex - 1) / 2; if (list.get(currentIndex).compareTo(list.get(parentIndex)) \u003e 0) { E tmp = list.get(currentIndex); list.set(currentIndex, list.get(parentIndex)); list.set(parentIndex, tmp); } else { break; } currentIndex = parentIndex; } } public E remove() { if (list.isEmpty()) return null; E e = list.get(0); list.set(0, list.get(list.size() - 1)); list.remove(list.size() - 1); int len = list.size(); int currentIndex = 0; while (currentIndex \u003c len) { int leftIndex = currentIndex * 2 + 1; int rightIndex = currentIndex * 2 + 2; if (leftIndex \u003e= len) { break; } int maxIndex = leftIndex; if (rightIndex \u003c= len - 1) { if (list.get(rightIndex).compareTo(list.get(leftIndex)) \u003e 0) { maxIndex = rightIndex; } } if (list.get(currentIndex).compareTo(list.get(maxIndex)) \u003c 0) { E tmp = list.get(currentIndex); list.set(currentIndex, list.get(maxIndex)); list.set(maxIndex, tmp); currentIndex = maxIndex; } else { break; } } return e; } } public class HeapSort { public static \u003cE extends Comparable\u003cE\u003e\u003e void heapSort(ArrayList\u003cE\u003e list) { BigHeap\u003cE\u003e bigHeap = new BigHeap\u003c\u003e(list); for (int i = 0; i \u003c list.size(); i++) { list.set(i, bigHeap.remove()); } } public static void main(String[] args) { ArrayList\u003cInteger\u003e array = new ArrayList\u003c\u003e(); array.add(-9); array.add(89); array.add(2); array.add(73); array.add(88); array.add(83); array.add(-34); array.add(-23); array.add(8); System.out.println(array); heapSort(array); System.out.println(array); } } ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/:6:0","tags":["算法描述"],"title":"经典排序","uri":"/2021/10/arithmetic/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F/"},{"categories":null,"content":" //c #include \u003cstdio.h\u003e #define Row 21 int arry[Row][Row]; void init_arry() { for (int i = 0; i \u003c Row; i++) { for (int j = 0; j \u003c Row; j++) { arry[i][j] = 0; } } } void func() { int mid = Row / 2; int left = mid; int right = mid; int flag = 0; for (int i = 0; i \u003c Row; i++) { if (i == 0 || i == Row - 1) // { arry[i][mid] = 1; } else { if (flag) { left++; right--; } else { left--; right++; } arry[i][left] = 1; arry[i][right] = 1; if (left == 0 \u0026\u0026 right == Row - 1) { flag = 1; } } } } void print() { for (int i = 0; i \u003c Row; i++) { for (int j = 0; j \u003c Row; j++) { if (arry[i][j] == 0) { printf(\" \"); } else { printf(\"*\"); } } printf(\"\\n\"); } } int main() { init_arry(); func(); print(); } * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E7%BB%88%E7%AB%AF%E5%9B%BE%E5%BD%A2/:0:0","tags":["算法描述"],"title":"终端图形","uri":"/2021/10/arithmetic/%E7%BB%88%E7%AB%AF%E5%9B%BE%E5%BD%A2/"},{"categories":null,"content":"链表原地反转 type Node struct { Ele int Next *Node } func PrintNode(head *Node) { p := head for p != nil { if p.Next == nil { fmt.Printf(\"%d\", p.Ele) } else { fmt.Printf(\"%d,\", p.Ele) } p = p.Next } } func InitLinkList() *Node { head := \u0026Node{ Ele: 1, Next: \u0026Node{ Ele: 2, Next: \u0026Node{ Ele: 3, Next: \u0026Node{ Ele: 4, Next: \u0026Node{ Ele: 5, Next: \u0026Node{ Ele: 6, Next: \u0026Node{ Ele: 7, Next: \u0026Node{ Ele: 8, Next: \u0026Node{ Ele: 9, Next: nil, }, }, }, }, }, }, }, }, } return head } //反转链表 func Reverse(head *Node) *Node { if head == nil || head.Next == nil { return head } var newHead *Node p := head for p != nil { tmp := p.Next p.Next = newHead newHead = p p = tmp } return newHead } func TestB(t *testing.T) { head := InitLinkList() PrintNode(head) fmt.Println() PrintNode(Reverse(head)) } 1,2,3,4,5,6,7,8,9 9,8,7,6,5,4,3,2,1--- PASS: TestB (0.00s) PASS ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E9%93%BE%E8%A1%A8/:1:0","tags":["算法描述"],"title":"链表","uri":"/2021/10/arithmetic/%E9%93%BE%E8%A1%A8/"},{"categories":null,"content":"八皇问题(Eight queens),问题表述为：在8×8格的国际象棋上摆放8个皇后，使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上，问有多少种摆法。高斯认为有76种方案。1854年在柏林的象棋杂志上不同的作者发表了40种不同的解，后来有人用图论的方法解出92种结果。如果经过±90度、±180度旋转，和对角线对称变换的摆法看成一类，共有42类。计算机发明后，有多种计算机语言可以编程解决此问题(摘自百度)。 func TestEightQueen(t *testing.T) { queen := []int{-1, -1, -1, -1, -1, -1, -1, -1} row := 0 size := len(queen) for row \u003e= 0 \u0026\u0026 row \u003c size { pos := search(row, queen, size) if pos \u003c 0 { queen[row] = -1 row-- } else { queen[row] = pos row++ } } if row == -1 { fmt.Println(\"未找到方案\") return } draw(queen) } func draw(queen []int) { for i := 0; i \u003c len(queen); i++ { for j := 0; j \u003c len(queen); j++ { if j == queen[i] { fmt.Printf(\"* \") } else { fmt.Printf(\"- \") } } fmt.Println() } } func search(row int, queen []int, size int) int { start := queen[row] + 1 for column := start; column \u003c size; column++ { if getTarget(column, queen, row) { return column } } return -1 } func getTarget(column int, queen []int, row int) bool { for i := 1; i \u003c= row; i++ { if queen[row-i] == column || queen[row-i] == column-i || queen[row-i] == column+i { return false } } return true } === RUN TestEightQueen * - - - - - - - - - - - * - - - - - - - - - - * - - - - - * - - - - * - - - - - - - - - - - * - - * - - - - - - - - - * - - - - --- PASS: TestEightQueen (0.00s) PASS ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E5%85%AB%E7%9A%87%E9%97%AE%E9%A2%98/:0:0","tags":["算法描述"],"title":"八皇问题","uri":"/2021/10/arithmetic/%E5%85%AB%E7%9A%87%E9%97%AE%E9%A2%98/"},{"categories":null,"content":" a := []int{0, 1, 2, 3, 4, 5, 6} func binarySearch(array []int, target int) (bool, int) { min := 0 max := len(array) - 1 for min \u003c= max { mid := (max-min)/2 + min if target \u003e array[mid] { min = mid + 1 } else if target \u003c array[mid] { max = mid - 1 } else { return true,target } } return false,0 } public class BinarySearch { public static void main(String[] args) { ArrayList\u003cInteger\u003e list = new ArrayList\u003c\u003e(); list.add(0); list.add(1); list.add(2); list.add(3); list.add(4); list.add(5); list.add(6); System.out.println(binarySearch(list, 9)); } public static \u003cE extends Comparable\u003cE\u003e\u003e E binarySearch(ArrayList\u003cE\u003e list, E target) { if (list == null || target == null) return null; var min = 0; var max = list.size() - 1; while (min \u003c= max) { var mid = min + (max - min) / 2; if (target.compareTo(list.get(mid)) \u003e 0) { min = mid + 1; } else if (target.compareTo(list.get(mid)) == 0) { return target; } else { max = mid - 1; } } return null; } } ","date":"2021-10-07","objectID":"/2021/10/arithmetic/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/:0:0","tags":["算法描述"],"title":"二分查找","uri":"/2021/10/arithmetic/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"categories":null,"content":"官网 镜像文件 官网下载 windos启动盘 制作启动盘工具 官网 工具地址 一键开始将u盘制作成启动盘 mac制作启动盘 从官网下载ios镜像 .iso转换成.dmg hdiutil convert -format UDRW -o ubuntu.iso ubuntu-22.04.1-live-server-amd64.iso 插入U盘，并且umount掉 $ diskutil list /dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *31.0 GB disk2 1: Microsoft Basic Data 1.5 GB disk2s1 2: EFI ESP 4.3 MB disk2s2 3: Microsoft Basic Data 307.2 KB disk2s3 4: Linux Filesystem 29.5 GB disk2s4 diskutil unmountDisk /dev/disk2 dd 写入数据，在写入之前需要把后缀改回来 mv ubuntu.iso.dmg ubuntu.iso sudo dd if=./ubuntu.iso of=/dev/rdisk4 bs=1m ubuntu desktop安装 将启动盘插入电脑，联想电脑 fn+f12 进入bios，用启动盘启动安装系统 系统语言设置英文 ","date":"2021-09-30","objectID":"/2021/09/software/ubuntu-install/:0:0","tags":["软件"],"title":"ubuntu install","uri":"/2021/09/software/ubuntu-install/"},{"categories":null,"content":"语言与输入法 ","date":"2021-09-30","objectID":"/2021/09/software/ubuntu-install/:1:0","tags":["软件"],"title":"ubuntu install","uri":"/2021/09/software/ubuntu-install/"},{"categories":null,"content":"截图工具 $ sudo apt-get install flameshot $ flameshot -h Usage: flameshot [flameshot-options] [arguments] Per default runs Flameshot in the background and adds a tray icon for configuration. Options: -h, --help Displays this help -v, --version Displays version information Arguments: gui Start a manual capture in GUI mode. screen Capture a single screen. full Capture the entire desktop. launcher Open the capture launcher. config Configure flameshot. ","date":"2021-09-30","objectID":"/2021/09/software/ubuntu-install/:2:0","tags":["软件"],"title":"ubuntu install","uri":"/2021/09/software/ubuntu-install/"},{"categories":null,"content":"快捷键 软件包下载源 图形界面 关闭 $ sudo systemctl set-default multi-user $ sudo reboot 打开 $ sudo systemctl set-default graphical 设置合上盖子不休眠 $ sudo vim /etc/systemd/logind.conf #HandleLidSwitch=suspend 修改成 HandleLidSwitch=ignore 分区挂载 查看磁盘信息 df -h Filesystem Size Used Avail Use% Mounted on tmpfs 784M 2.0M 782M 1% /run /dev/sdb2 439G 7.7G 409G 2% / tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs 5.0M 4.0K 5.0M 1% /run/lock /dev/sda1 96M 31M 66M 32% /boot/efi /dev/sda3 223G 1.6G 222G 1% /root/minio tmpfs 784M 72K 784M 1% /run/user/127 tmpfs 784M 56K 784M 1% /run/user/0 格式化硬盘 mkfs.xfs /dev/sda3 挂载分区 mount /dev/sda3 /root/minio 查看磁盘分区的UUID blkid /dev/sda3: UUID=\"7b54b8d4-0c82-4459-8b37-c317b2c2cac7\" BLOCK_SIZE=\"512\" TYPE=\"xfs\" PARTLABEL=\"Basic data partition\" PARTUUID=\"c1d4bbc4-11cf-46f5-bac4-105153e22df3\" 修改fstab开机自动挂载 vim /etc/fstab # 加入下面内容 UUID=\"7b54b8d4-0c82-4459-8b37-c317b2c2cac7\" /root/minio xfs defaults 0 1 ","date":"2021-09-30","objectID":"/2021/09/software/ubuntu-install/:3:0","tags":["软件"],"title":"ubuntu install","uri":"/2021/09/software/ubuntu-install/"},{"categories":null,"content":"ufw (Uncomplicated FireWall) 简单防火墙，是ubunt系统下默认防火墙。比iptables简单 配置文件 cat /etc/default/ufw 查看规则 $ sudo ufw status #简略信息 $ sudo ufw status verbose#详细信息 $ sudo ufw status numbered #序号输出 开放某个端口 $ sudo ufw allow 80 #开放80 关闭某个端口 $ sudo ufw deny 22 阻止某个ip访问(针对所有的端口) $ sudo ufw deny from 208.176.0.50 to any 阻止某个ip访问(针对一个端口) $ sudo ufw deny from 202.54.1.5 to any port 80 ","date":"2021-09-30","objectID":"/2021/09/linux/ufw/:0:0","tags":["linux"],"title":"ufw","uri":"/2021/09/linux/ufw/"},{"categories":null,"content":"引入问题 为什么需要正则表达式。随着工作的深入，你会发现随时随地都有可能需要正则表达来提高工作效率。试想你需要过滤你想要的日志内容来排查某个bug，你会怎么做？命令grep 后面可以是一个正则表达，用于过滤匹配规则的日志类容。另外go的测试命令也有正则的影子 go test -v -run \\^Test1\\$ test_test.go //运行Test1 的测试用例 当要有规则地替换大量文本类容，使用正则表达是个很好的选择。比如下面这个，查出以下电话号码所对应的用户信息。最直接方法就是拼接裸sql，可是需要处理每个电话号码，如13575453505变成 “13575453505”, 每个电话号码需要引号，和一个逗号。最最最笨的方法就是一个个加，浪费时间。这时候用正则可以几秒搞定 ,替换时需要使用到捕获组，可以用()表示,并且$n进行引用，例如([0-9]),的引用是$1 (^[0-9]{11}$) “$1”, ","date":"2021-09-29","objectID":"/2021/09/reg/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/:1:0","tags":["正则表达"],"title":"正则表达式","uri":"/2021/09/reg/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"categories":null,"content":"语法 123+abc，可以匹配1233abc，123abc，123333abc，+号表示前面的字符至少出现有一个 func Test2(t *testing.T) { re, err := regexp.Compile(\"123+abc\") if err!=nil{ fmt.Println(err) return } fmt.Println(re.MatchString(\"1233abc\")) fmt.Println(re.MatchString(\"123abc\")) fmt.Println(re.MatchString(\"123333abc\")) fmt.Println(re.MatchString(\"12abc\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test2\\$ test_test.go true true true false 123*abc，可以匹配1233abc，123abc，123333abc，12abc, *号表示前面的字符出现有0个，1个，或多个 func Test3(t *testing.T) { re, err := regexp.Compile(\"123*abc\") if err!=nil{ fmt.Println(err) return } fmt.Println(re.MatchString(\"1233abc\")) fmt.Println(re.MatchString(\"123abc\")) fmt.Println(re.MatchString(\"123333abc\")) fmt.Println(re.MatchString(\"12abc\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test3\\$ test_test.go true true true true 123?abc，可以匹配123abc，12abc，?号代表前面的字符最多能出现一次(0次，1次) func Test4(t *testing.T) { re, err := regexp.Compile(\"123?abc\") if err!=nil{ fmt.Println(err) return } fmt.Println(re.MatchString(\"1233abc\")) fmt.Println(re.MatchString(\"123abc\")) fmt.Println(re.MatchString(\"123333abc\")) fmt.Println(re.MatchString(\"12abc\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test4\\$ test_test.go false true false true 匹配[…]中所有的字符,例如[hw]匹配 “hello,world” func Test5(t *testing.T) { re, err := regexp.Compile(\"[hw]\") if err!=nil{ fmt.Println(err) return } fmt.Println(re.MatchString(\"hello,world\")) fmt.Println(re.MatchString(\"huawei\")) fmt.Println(re.MatchString(\"uaei\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test5\\$ test_test.go 15:16:56 === RUN Test5 true true false --- PASS: Test5 (0.00s) PASS ok command-line-arguments 0.007s [^….]匹配除了….的字符 func Test6(t *testing.T) { re, err := regexp.Compile(\"[^a]\") if err != nil { fmt.Println(err) return } fmt.Println(re.MatchString(\"a\")) fmt.Println(re.MatchString(\"bc\")) fmt.Println(re.MatchString(\"abc\")) fmt.Println(re.MatchString(\"hw\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test6\\$ test_test.go 15:27:59 === RUN Test6 false true true true --- PASS: Test6 (0.00s) PASS ok command-line-arguments 0.006s ^$,^是开始匹配，$结束匹配,可用于准确匹配 func Test7(t *testing.T) { re, err := regexp.Compile(\"^[a$bc]\") if err != nil { fmt.Println(err) return } fmt.Println(re.MatchString(\"abc\")) fmt.Println(re.MatchString(\"abcssdda\")) fmt.Println(re.MatchString(\"afbgch\")) fmt.Println(re.MatchString(\"va\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test7\\$ test_test.go 16:10:32 === RUN Test7 true false false false --- PASS: Test7 (0.00s) PASS ok command-line-arguments 0.008s [A-Z]代表一个区间，匹配所有大写字母,[a-z]匹配所有的小写字母 func Test8(t *testing.T) { re, err := regexp.Compile(\"[a-z]\") if err != nil { fmt.Println(err) return } fmt.Println(re.MatchString(\"abc\")) fmt.Println(re.MatchString(\"abcssdda\")) fmt.Println(re.MatchString(\"afbgch\")) fmt.Println(re.MatchString(\"va\")) fmt.Println(re.MatchString(\"aA\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v -run \\^Test8\\$ test_test.go 16:14:30 === RUN Test8 true true true true false --- PASS: Test8 (0.00s) PASS ok command-line-arguments 0.006s 匹配所有。\\s 是匹配所有空白符，包括换行，\\S 非空白符，包括换行 func Test9(t *testing.T) { re, err := regexp.Compile(\"[\\\\s]\") if err != nil { fmt.Println(err) return } fmt.Println(re.MatchString(\"abc \")) fmt.Println(re.MatchString(\"abcssdda\\n\")) fmt.Println(re.MatchString(\"afbgch\")) fmt.Println(re.MatchString(\"va\")) fmt.Println(re.MatchString(\"A\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test -v -run \\^Test9\\$ test_test.go 16:17:54 === RUN Test9 true true false false false --- PASS: Test9 (0.00s) PASS ok command-line-arguments 0.012s func Test9(t *testing.T) { re, err := regexp.Compile(\"[\\\\S]\") if err != nil { fmt.Println(err) return } fmt.Println(re.MatchString(\"\\n\")) fmt.Println(re.MatchString(\" \")) fmt.Println(re.MatchString(\"abcssdda\\n\")) fmt.Println(re.MatchString(\"afbgch\")) fmt.Println(re.MatchString(\"va\")) fmt.Println(re.MatchString(\"A\")) } ⋊\u003e ~/g/s/j/g/regular_expression go test --count=1 -v","date":"2021-09-29","objectID":"/2021/09/reg/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/:2:0","tags":["正则表达"],"title":"正则表达式","uri":"/2021/09/reg/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"categories":null,"content":"常用正则表示 邮箱 支持英文，多级域名 只允许英文字母、数字、下划线、英文句号、以及中划线组成 ^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(\\.[a-zA-Z0-9_-]+)+$ 支持中文、字母、数字，域名只允许英文域名 ^[A-Za-z0-9\\u4e00-\\u9fa5]+@[a-zA-Z0-9_-]+(\\.[a-zA-Z0-9_-]+)+$ 电话号码 ^1(?:3\\d|4[4-9]|5[0-35-9]|6[67]|7[013-8]|8\\d|9\\d)\\d{8}$ 匹配一行字符串 (^\\S{1,}$) 需求是 下面随机字符串，需要替换成 \"Ericwang100\", Ericwang100 FSRunner VK李康 尚文欢shangwh lowkeyd stonezou 匹配每一行并且加上引号 \\b(\\w+)\\b \"$1\", aaaaa asdasdsad asdasdsa ","date":"2021-09-29","objectID":"/2021/09/reg/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/:3:0","tags":["正则表达"],"title":"正则表达式","uri":"/2021/09/reg/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]